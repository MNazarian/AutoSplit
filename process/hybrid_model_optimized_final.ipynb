{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fb13bc74-e90d-497f-be31-c0d4b6376045",
   "metadata": {},
   
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models, Input, mixed_precision\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import confusion_matrix, classification_report, f1_score, roc_auc_score\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Konstanten\n",
    "IMG_HEIGHT, IMG_WIDTH = 224, 224\n",
    "BATCH_SIZE = 32\n",
    "EPOCHS = 100\n",
    "\n",
    "# GPU Konfiguration\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    try:\n",
    "        for gpu in gpus:\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)\n",
    "    except RuntimeError as e:\n",
    "        print(e)\n",
    "\n",
    "# Mixed Precision\n",
    "if tf.test.is_built_with_cuda():\n",
    "    policy = mixed_precision.Policy('mixed_float16')\n",
    "    mixed_precision.set_global_policy(policy)\n",
    "\n",
    "def create_dataset(dataframe, numeric_columns, is_training=True):\n",
    "    def parse_function(filename, label, numeric_data):\n",
    "        # Bild verarbeiten\n",
    "        img = tf.io.read_file(filename)\n",
    "        img = tf.image.decode_jpeg(img, channels=3)\n",
    "        img = tf.image.resize(img, [IMG_HEIGHT, IMG_WIDTH])\n",
    "        img = tf.cast(img, tf.float32) / 255.0\n",
    "        \n",
    "        if is_training:\n",
    "            img = tf.image.random_flip_left_right(img)\n",
    "            img = tf.image.random_brightness(img, max_delta=0.2)\n",
    "            img = tf.image.random_contrast(img, lower=0.8, upper=1.2)\n",
    "        \n",
    "        return (img, numeric_data), label\n",
    "\n",
    "    # Dataset erstellen\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((\n",
    "        dataframe['image_path'].values,\n",
    "        dataframe['label'].values,\n",
    "        dataframe[numeric_columns].values.astype(np.float32)\n",
    "    ))\n",
    "    \n",
    "    dataset = dataset.map(parse_function, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "    if is_training:\n",
    "        dataset = dataset.shuffle(buffer_size=len(dataframe))\n",
    "    \n",
    "    dataset = dataset.batch(BATCH_SIZE)\n",
    "    dataset = dataset.prefetch(tf.data.AUTOTUNE)\n",
    "    return dataset\n",
    "\n",
    "def create_model(num_numeric_features):\n",
    "    # Image processing branch\n",
    "    img_input = Input(shape=(IMG_HEIGHT, IMG_WIDTH, 3))\n",
    "    \n",
    "    x = layers.Conv2D(64, 3, activation='relu', padding='same')(img_input)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.MaxPooling2D()(x)\n",
    "    \n",
    "    x = layers.Conv2D(128, 3, activation='relu', padding='same')(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.MaxPooling2D()(x)\n",
    "    \n",
    "    x = layers.Conv2D(256, 3, activation='relu', padding='same')(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.MaxPooling2D()(x)\n",
    "    \n",
    "    x = layers.GlobalAveragePooling2D()(x)\n",
    "    x = layers.Dropout(0.5)(x)\n",
    "    \n",
    "    # Numeric data branch\n",
    "    num_input = Input(shape=(num_numeric_features,))\n",
    "    y = layers.Dense(64, activation='relu')(num_input)\n",
    "    y = layers.BatchNormalization()(y)\n",
    "    y = layers.Dropout(0.3)(y)\n",
    "    \n",
    "    # Combine branches\n",
    "    combined = layers.concatenate([x, y])\n",
    "    \n",
    "    z = layers.Dense(128, activation='relu')(combined)\n",
    "    z = layers.BatchNormalization()(z)\n",
    "    z = layers.Dropout(0.5)(z)\n",
    "    \n",
    "    output = layers.Dense(1, activation='sigmoid')(z)\n",
    "    \n",
    "    return models.Model(inputs=[img_input, num_input], outputs=output)\n",
    "\n",
    "def main():\n",
    "    # Daten laden\n",
    "    file_path = r\"C:\\Users\\meh91075\\Downloads\\AutoSplit_dataset_09092024.csv\"\n",
    "    df = pd.read_csv(file_path)\n",
    "    \n",
    "    # Bilder laden\n",
    "    filtered_dir = os.path.join(\"C:/Users/meh91075/Desktop/02_AutoSplit/02_AutoSplit/10_Datasets/01_Autosplit_dataset\", 'New_filtered_data_label4')\n",
    "    non_filtered_dir = os.path.join(\"C:/Users/meh91075/Desktop/02_AutoSplit/02_AutoSplit/10_Datasets/01_Autosplit_dataset\", 'New_non_filtered_data')\n",
    "    \n",
    "    def get_image_paths(directory):\n",
    "        return [os.path.join(root, file) for root, _, files in os.walk(directory) \n",
    "                for file in files if file.lower().endswith(('.png', '.jpg', '.jpeg'))]\n",
    "\n",
    "    filtered_images = get_image_paths(filtered_dir)\n",
    "    non_filtered_images = get_image_paths(non_filtered_dir)\n",
    "    \n",
    "    def extract_filename(path):\n",
    "        return os.path.basename(path).replace('_ISO', '').replace('_ISOAlt', '').split('.')[0]\n",
    "\n",
    "    image_df = pd.DataFrame({\n",
    "        'image_path': filtered_images + non_filtered_images,\n",
    "        'label': [1] * len(filtered_images) + [0] * len(non_filtered_images),\n",
    "        'filename': [extract_filename(path) for path in filtered_images + non_filtered_images]\n",
    "    })\n",
    "    \n",
    "    # Daten vorbereiten\n",
    "    merged_df = pd.merge(df, image_df[['filename', 'image_path']], on='filename', how='left')\n",
    "    merged_df['label'] = merged_df['label_majority'].map({1: 0, 2: 0, 3: 0, 4: 1})\n",
    "    merged_df = merged_df[merged_df['image_path'].notna()]\n",
    "    \n",
    "    # Daten balancieren\n",
    "    minority_class = merged_df[merged_df['label'] == 1]\n",
    "    majority_class = merged_df[merged_df['label'] == 0].sample(n=len(minority_class), random_state=42)\n",
    "    balanced_df = pd.concat([majority_class, minority_class])\n",
    "    \n",
    "    # Train-Test-Split\n",
    "    train_df, val_df = train_test_split(balanced_df, test_size=0.2, stratify=balanced_df['label'], random_state=42)\n",
    "    \n",
    "    # Numerische Features\n",
    "    numeric_columns = df.select_dtypes(include=[np.number]).columns.tolist()\n",
    "    numeric_columns = [col for col in numeric_columns if col not in ['label_majority', 'label']]\n",
    "    \n",
    "    # Normalisierung\n",
    "    scaler = StandardScaler()\n",
    "    train_df[numeric_columns] = scaler.fit_transform(train_df[numeric_columns])\n",
    "    val_df[numeric_columns] = scaler.transform(val_df[numeric_columns])\n",
    "    \n",
    "    # Datasets erstellen\n",
    "    train_dataset = create_dataset(train_df, numeric_columns)\n",
    "    val_dataset = create_dataset(val_df, numeric_columns, is_training=False)\n",
    "    \n",
    "    # Model erstellen\n",
    "    strategy = tf.distribute.MirroredStrategy()\n",
    "    with strategy.scope():\n",
    "        model = create_model(len(numeric_columns))\n",
    "        \n",
    "        optimizer = tf.keras.optimizers.Adam(learning_rate=1e-3)\n",
    "        \n",
    "        model.compile(\n",
    "            optimizer=optimizer,\n",
    "            loss='binary_crossentropy',\n",
    "            metrics=['accuracy', \n",
    "                    tf.keras.metrics.AUC(name='auc'), \n",
    "                    tf.keras.metrics.Precision(name='precision'),\n",
    "                    tf.keras.metrics.Recall(name='recall')]\n",
    "        )\n",
    "    \n",
    "    # Callbacks\n",
    "    callbacks = [\n",
    "        tf.keras.callbacks.EarlyStopping(\n",
    "            monitor='val_auc',\n",
    "            patience=10,\n",
    "            mode='max',\n",
    "            restore_best_weights=True,\n",
    "            verbose=1\n",
    "        ),\n",
    "        tf.keras.callbacks.ReduceLROnPlateau(\n",
    "            monitor='val_auc',\n",
    "            factor=0.2,\n",
    "            patience=5,\n",
    "            mode='max',\n",
    "            min_lr=1e-6,\n",
    "            verbose=1\n",
    "        ),\n",
    "        tf.keras.callbacks.ModelCheckpoint(\n",
    "            'best_model.h5',\n",
    "            monitor='val_auc',\n",
    "            mode='max',\n",
    "            save_best_only=True,\n",
    "            verbose=1\n",
    "        )\n",
    "    ]\n",
    "    \n",
    "    # Training\n",
    "    print(\"\\nStarting training...\")\n",
    "    history = model.fit(\n",
    "        train_dataset,\n",
    "        epochs=EPOCHS,\n",
    "        validation_data=val_dataset,\n",
    "        callbacks=callbacks,\n",
    "        verbose=1\n",
    "    )\n",
    "    \n",
    "    # Evaluierung\n",
    "    val_metrics = model.evaluate(val_dataset)\n",
    "    print(\"\\nValidation Metrics:\")\n",
    "    for name, value in zip(model.metrics_names, val_metrics):\n",
    "        print(f\"{name}: {value:.4f}\")\n",
    "    \n",
    "    # Vorhersagen\n",
    "    y_pred = model.predict(val_dataset)\n",
    "    y_pred_classes = (y_pred > 0.5).astype(\"int32\")\n",
    "    y_true = val_df['label'].values\n",
    "    \n",
    "    # Metriken\n",
    "    print(\"\\nClassification Report:\")\n",
    "    print(classification_report(y_true, y_pred_classes))\n",
    "    \n",
    "    # ROC AUC\n",
    "    roc_auc = roc_auc_score(y_true, y_pred)\n",
    "    print(f\"ROC AUC: {roc_auc:.4f}\")\n",
    "    \n",
    "    # Confusion Matrix\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.heatmap(confusion_matrix(y_true, y_pred_classes), \n",
    "                annot=True, fmt='d', cmap='Blues')\n",
    "    plt.title('Confusion Matrix')\n",
    "    plt.xlabel('Predicted Label')\n",
    "    plt.ylabel('True Label')\n",
    "    plt.savefig('confusion_matrix.png')\n",
    "    plt.close()\n",
    "    \n",
    "    # Training History\n",
    "    plt.figure(figsize=(15, 5))\n",
    "    \n",
    "    plt.subplot(1, 3, 1)\n",
    "    plt.plot(history.history['accuracy'], label='Training')\n",
    "    plt.plot(history.history['val_accuracy'], label='Validation')\n",
    "    plt.title('Model Accuracy')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.legend()\n",
    "    \n",
    "    plt.subplot(1, 3, 2)\n",
    "    plt.plot(history.history['loss'], label='Training')\n",
    "    plt.plot(history.history['val_loss'], label='Validation')\n",
    "    plt.title('Model Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    \n",
    "    plt.subplot(1, 3, 3)\n",
    "    plt.plot(history.history['auc'], label='Training')\n",
    "    plt.plot(history.history['val_auc'], label='Validation')\n",
    "    plt.title('Model AUC')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('AUC')\n",
    "    plt.legend()\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('training_history.png')\n",
    "    plt.close()\n",
    "    \n",
    "    # Model speichern\n",
    "    model.save('final_model.h5')\n",
    "    print(\"\\nTraining completed. Model and visualizations saved.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d27f44d-ed92-4a9f-84ae-8f265adc5472",
   "metadata": {},
   "source": [
    "## hybrid_model_optimized_final_new.h5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ede66ba9-0d18-4a74-9571-44454310aa28",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original CSV data loaded. Number of rows: 19274\n",
      "Original distribution of labels:\n",
      "label_majority\n",
      "2    10850\n",
      "1     3485\n",
      "4     2745\n",
      "3     2194\n",
      "Name: count, dtype: int64\n",
      "1 Physical GPUs, 1 Logical GPUs\n",
      "WARNING:tensorflow:Mixed precision compatibility check (mixed_float16): WARNING\n",
      "Your GPU may run slowly with dtype policy mixed_float16 because it does not have compute capability of at least 7.0. Your GPU:\n",
      "  Quadro P5000, compute capability 6.1\n",
      "See https://developer.nvidia.com/cuda-gpus for a list of GPUs and their compute capabilities.\n",
      "If you will use compatible GPU(s) not attached to this host, e.g. by running a multi-worker model, you can ignore this warning. This message will only be logged once\n",
      "Mixed precision policy set to mixed_float16\n",
      "Found images: Filtered: 5664, Not filtered: 27994\n",
      "Sample of extracted filenames:\n",
      "0       00042065\n",
      "1    00042065Alt\n",
      "2       00043749\n",
      "3    00043749Alt\n",
      "4       00080967\n",
      "Name: filename, dtype: object\n",
      "After undersampling: Number of rows: 4499\n",
      "Distribution of labels after undersampling:\n",
      "label\n",
      "1    2499\n",
      "0    2000\n",
      "Name: count, dtype: int64\n",
      "Training data: 3599, Validation data: 900\n",
      "INFO:tensorflow:Using MirroredStrategy with devices ('/job:localhost/replica:0/task:0/device:GPU:0',)\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "Epoch 1/100\n",
      "112/112 [==============================] - 33s 130ms/step - loss: 0.6487 - accuracy: 0.7246 - auc: 0.7877 - precision: 0.7556 - recall: 0.7461 - val_loss: 0.3985 - val_accuracy: 0.8270 - val_auc: 0.9012 - val_precision: 0.8101 - val_recall: 0.8996 - lr: 0.0010\n",
      "Epoch 2/100\n",
      "112/112 [==============================] - 12s 110ms/step - loss: 0.4695 - accuracy: 0.7905 - auc: 0.8731 - precision: 0.8082 - recall: 0.8159 - val_loss: 0.3641 - val_accuracy: 0.8371 - val_auc: 0.9199 - val_precision: 0.8200 - val_recall: 0.9056 - lr: 0.0010\n",
      "Epoch 3/100\n",
      "112/112 [==============================] - 11s 102ms/step - loss: 0.4147 - accuracy: 0.8181 - auc: 0.8962 - precision: 0.8225 - recall: 0.8587 - val_loss: 0.3730 - val_accuracy: 0.8415 - val_auc: 0.9045 - val_precision: 0.8309 - val_recall: 0.8976 - lr: 0.0010\n",
      "Epoch 4/100\n",
      "112/112 [==============================] - 12s 110ms/step - loss: 0.3894 - accuracy: 0.8287 - auc: 0.9061 - precision: 0.8330 - recall: 0.8633 - val_loss: 0.3496 - val_accuracy: 0.8560 - val_auc: 0.9210 - val_precision: 0.8596 - val_recall: 0.8855 - lr: 0.0010\n",
      "Epoch 5/100\n",
      "112/112 [==============================] - 11s 102ms/step - loss: 0.3483 - accuracy: 0.8502 - auc: 0.9226 - precision: 0.8524 - recall: 0.8837 - val_loss: 0.3503 - val_accuracy: 0.8549 - val_auc: 0.9196 - val_precision: 0.8695 - val_recall: 0.8695 - lr: 0.0010\n",
      "Epoch 6/100\n",
      "112/112 [==============================] - 12s 110ms/step - loss: 0.3379 - accuracy: 0.8552 - auc: 0.9267 - precision: 0.8534 - recall: 0.8917 - val_loss: 0.3259 - val_accuracy: 0.8683 - val_auc: 0.9307 - val_precision: 0.8571 - val_recall: 0.9157 - lr: 0.0010\n",
      "Epoch 7/100\n",
      "112/112 [==============================] - 12s 109ms/step - loss: 0.3217 - accuracy: 0.8697 - auc: 0.9333 - precision: 0.8700 - recall: 0.9001 - val_loss: 0.3178 - val_accuracy: 0.8661 - val_auc: 0.9360 - val_precision: 0.8663 - val_recall: 0.8976 - lr: 0.0010\n",
      "Epoch 8/100\n",
      "112/112 [==============================] - 11s 101ms/step - loss: 0.3059 - accuracy: 0.8677 - auc: 0.9412 - precision: 0.8717 - recall: 0.8928 - val_loss: 0.3452 - val_accuracy: 0.8460 - val_auc: 0.9256 - val_precision: 0.8719 - val_recall: 0.8474 - lr: 0.0010\n",
      "Epoch 9/100\n",
      "112/112 [==============================] - 11s 101ms/step - loss: 0.2986 - accuracy: 0.8756 - auc: 0.9425 - precision: 0.8683 - recall: 0.9160 - val_loss: 0.3438 - val_accuracy: 0.8248 - val_auc: 0.9310 - val_precision: 0.8797 - val_recall: 0.7932 - lr: 9.0000e-04\n",
      "Epoch 10/100\n",
      "112/112 [==============================] - 11s 101ms/step - loss: 0.2945 - accuracy: 0.8736 - auc: 0.9450 - precision: 0.8763 - recall: 0.8980 - val_loss: 0.3203 - val_accuracy: 0.8638 - val_auc: 0.9343 - val_precision: 0.8715 - val_recall: 0.8855 - lr: 9.0000e-04\n",
      "Epoch 11/100\n",
      "112/112 [==============================] - 12s 109ms/step - loss: 0.2883 - accuracy: 0.8783 - auc: 0.9463 - precision: 0.8782 - recall: 0.9088 - val_loss: 0.3149 - val_accuracy: 0.8627 - val_auc: 0.9387 - val_precision: 0.8758 - val_recall: 0.8775 - lr: 9.0000e-04\n",
      "Epoch 12/100\n",
      "112/112 [==============================] - 12s 109ms/step - loss: 0.2721 - accuracy: 0.8912 - auc: 0.9519 - precision: 0.8889 - recall: 0.9185 - val_loss: 0.3087 - val_accuracy: 0.8650 - val_auc: 0.9425 - val_precision: 0.8793 - val_recall: 0.8775 - lr: 9.0000e-04\n",
      "Epoch 13/100\n",
      "112/112 [==============================] - 11s 101ms/step - loss: 0.2616 - accuracy: 0.8929 - auc: 0.9558 - precision: 0.8865 - recall: 0.9249 - val_loss: 0.3098 - val_accuracy: 0.8627 - val_auc: 0.9406 - val_precision: 0.8378 - val_recall: 0.9337 - lr: 9.0000e-04\n",
      "Epoch 14/100\n",
      "112/112 [==============================] - 11s 101ms/step - loss: 0.2622 - accuracy: 0.8898 - auc: 0.9558 - precision: 0.8874 - recall: 0.9173 - val_loss: 0.3074 - val_accuracy: 0.8650 - val_auc: 0.9409 - val_precision: 0.8733 - val_recall: 0.8855 - lr: 9.0000e-04\n",
      "Epoch 15/100\n",
      "112/112 [==============================] - 12s 109ms/step - loss: 0.2611 - accuracy: 0.8937 - auc: 0.9550 - precision: 0.8904 - recall: 0.9237 - val_loss: 0.2995 - val_accuracy: 0.8728 - val_auc: 0.9462 - val_precision: 0.8466 - val_recall: 0.9418 - lr: 9.0000e-04\n",
      "Epoch 16/100\n",
      "112/112 [==============================] - 11s 101ms/step - loss: 0.2467 - accuracy: 0.8990 - auc: 0.9607 - precision: 0.8949 - recall: 0.9249 - val_loss: 0.2941 - val_accuracy: 0.8839 - val_auc: 0.9445 - val_precision: 0.8833 - val_recall: 0.9116 - lr: 9.0000e-04\n",
      "Epoch 17/100\n",
      "112/112 [==============================] - 12s 109ms/step - loss: 0.2623 - accuracy: 0.8895 - auc: 0.9558 - precision: 0.8908 - recall: 0.9148 - val_loss: 0.2872 - val_accuracy: 0.8806 - val_auc: 0.9473 - val_precision: 0.8826 - val_recall: 0.9056 - lr: 9.0000e-04\n",
      "Epoch 18/100\n",
      "112/112 [==============================] - 12s 108ms/step - loss: 0.2412 - accuracy: 0.9004 - auc: 0.9623 - precision: 0.8968 - recall: 0.9270 - val_loss: 0.2784 - val_accuracy: 0.8873 - val_auc: 0.9512 - val_precision: 0.8738 - val_recall: 0.9317 - lr: 8.1000e-04\n",
      "Epoch 19/100\n",
      "112/112 [==============================] - 11s 100ms/step - loss: 0.2385 - accuracy: 0.9065 - auc: 0.9634 - precision: 0.9090 - recall: 0.9240 - val_loss: 0.2834 - val_accuracy: 0.8828 - val_auc: 0.9510 - val_precision: 0.8816 - val_recall: 0.9116 - lr: 8.1000e-04\n",
      "Epoch 20/100\n",
      "112/112 [==============================] - 11s 100ms/step - loss: 0.2368 - accuracy: 0.9065 - auc: 0.9632 - precision: 0.9034 - recall: 0.9311 - val_loss: 0.2812 - val_accuracy: 0.8839 - val_auc: 0.9507 - val_precision: 0.8788 - val_recall: 0.9177 - lr: 8.1000e-04\n",
      "Epoch 21/100\n",
      "112/112 [==============================] - 11s 100ms/step - loss: 0.2405 - accuracy: 0.9035 - auc: 0.9625 - precision: 0.9023 - recall: 0.9268 - val_loss: 0.2973 - val_accuracy: 0.8750 - val_auc: 0.9460 - val_precision: 0.8814 - val_recall: 0.8956 - lr: 8.1000e-04\n",
      "Epoch 22/100\n",
      "112/112 [==============================] - 11s 100ms/step - loss: 0.2266 - accuracy: 0.9085 - auc: 0.9670 - precision: 0.9069 - recall: 0.9330 - val_loss: 0.3040 - val_accuracy: 0.8616 - val_auc: 0.9438 - val_precision: 0.8740 - val_recall: 0.8775 - lr: 8.1000e-04\n",
      "Epoch 23/100\n",
      "112/112 [==============================] - ETA: 0s - loss: 0.2243 - accuracy: 0.9110 - auc: 0.9672 - precision: 0.9125 - recall: 0.9277\n",
      "Epoch 23: ReduceLROnPlateau reducing learning rate to 0.0001619999995455146.\n",
      "112/112 [==============================] - 11s 102ms/step - loss: 0.2243 - accuracy: 0.9110 - auc: 0.9672 - precision: 0.9125 - recall: 0.9277 - val_loss: 0.3127 - val_accuracy: 0.8672 - val_auc: 0.9440 - val_precision: 0.8366 - val_recall: 0.9458 - lr: 1.3122e-04\n",
      "Epoch 24/100\n",
      "112/112 [==============================] - 11s 100ms/step - loss: 0.2306 - accuracy: 0.9074 - auc: 0.9658 - precision: 0.9049 - recall: 0.9296 - val_loss: 0.2879 - val_accuracy: 0.8828 - val_auc: 0.9483 - val_precision: 0.8772 - val_recall: 0.9177 - lr: 1.3122e-04\n",
      "Epoch 25/100\n",
      "112/112 [==============================] - 11s 99ms/step - loss: 0.2340 - accuracy: 0.9035 - auc: 0.9639 - precision: 0.9027 - recall: 0.9278 - val_loss: 0.2855 - val_accuracy: 0.8806 - val_auc: 0.9500 - val_precision: 0.8668 - val_recall: 0.9277 - lr: 1.3122e-04\n",
      "Epoch 26/100\n",
      "112/112 [==============================] - 12s 107ms/step - loss: 0.2444 - accuracy: 0.9029 - auc: 0.9608 - precision: 0.8992 - recall: 0.9275 - val_loss: 0.2799 - val_accuracy: 0.8839 - val_auc: 0.9517 - val_precision: 0.8848 - val_recall: 0.9096 - lr: 1.3122e-04\n",
      "Epoch 27/100\n",
      "112/112 [==============================] - 11s 99ms/step - loss: 0.2334 - accuracy: 0.9093 - auc: 0.9650 - precision: 0.9095 - recall: 0.9291 - val_loss: 0.2839 - val_accuracy: 0.8862 - val_auc: 0.9509 - val_precision: 0.8708 - val_recall: 0.9337 - lr: 1.1810e-04\n",
      "Epoch 28/100\n",
      "112/112 [==============================] - 12s 108ms/step - loss: 0.2181 - accuracy: 0.9124 - auc: 0.9679 - precision: 0.9113 - recall: 0.9349 - val_loss: 0.2755 - val_accuracy: 0.8895 - val_auc: 0.9527 - val_precision: 0.8844 - val_recall: 0.9217 - lr: 1.1810e-04\n",
      "Epoch 29/100\n",
      "112/112 [==============================] - 11s 100ms/step - loss: 0.2393 - accuracy: 0.9021 - auc: 0.9628 - precision: 0.8977 - recall: 0.9281 - val_loss: 0.2913 - val_accuracy: 0.8817 - val_auc: 0.9494 - val_precision: 0.8828 - val_recall: 0.9076 - lr: 1.1810e-04\n",
      "Epoch 30/100\n",
      "112/112 [==============================] - 12s 108ms/step - loss: 0.2068 - accuracy: 0.9146 - auc: 0.9717 - precision: 0.9061 - recall: 0.9443 - val_loss: 0.2676 - val_accuracy: 0.8862 - val_auc: 0.9562 - val_precision: 0.8944 - val_recall: 0.9016 - lr: 1.1810e-04\n",
      "Epoch 31/100\n",
      "112/112 [==============================] - 11s 100ms/step - loss: 0.2110 - accuracy: 0.9093 - auc: 0.9719 - precision: 0.9165 - recall: 0.9215 - val_loss: 0.2844 - val_accuracy: 0.8850 - val_auc: 0.9498 - val_precision: 0.8776 - val_recall: 0.9217 - lr: 1.1810e-04\n",
      "Epoch 32/100\n",
      "112/112 [==============================] - 12s 108ms/step - loss: 0.2125 - accuracy: 0.9141 - auc: 0.9700 - precision: 0.9067 - recall: 0.9407 - val_loss: 0.2709 - val_accuracy: 0.8917 - val_auc: 0.9568 - val_precision: 0.8748 - val_recall: 0.9398 - lr: 1.1810e-04\n",
      "Epoch 33/100\n",
      "112/112 [==============================] - 12s 108ms/step - loss: 0.2060 - accuracy: 0.9230 - auc: 0.9716 - precision: 0.9232 - recall: 0.9415 - val_loss: 0.2657 - val_accuracy: 0.8895 - val_auc: 0.9573 - val_precision: 0.8919 - val_recall: 0.9116 - lr: 1.1810e-04\n",
      "Epoch 34/100\n",
      "112/112 [==============================] - 11s 100ms/step - loss: 0.2080 - accuracy: 0.9104 - auc: 0.9728 - precision: 0.9079 - recall: 0.9314 - val_loss: 0.2869 - val_accuracy: 0.8761 - val_auc: 0.9530 - val_precision: 0.8564 - val_recall: 0.9337 - lr: 1.1810e-04\n",
      "Epoch 35/100\n",
      "112/112 [==============================] - 11s 100ms/step - loss: 0.2073 - accuracy: 0.9202 - auc: 0.9710 - precision: 0.9202 - recall: 0.9394 - val_loss: 0.2765 - val_accuracy: 0.8795 - val_auc: 0.9544 - val_precision: 0.8963 - val_recall: 0.8855 - lr: 1.1810e-04\n",
      "Epoch 36/100\n",
      "112/112 [==============================] - 11s 100ms/step - loss: 0.1948 - accuracy: 0.9222 - auc: 0.9754 - precision: 0.9241 - recall: 0.9367 - val_loss: 0.2728 - val_accuracy: 0.8906 - val_auc: 0.9538 - val_precision: 0.8774 - val_recall: 0.9337 - lr: 1.0629e-04\n",
      "Epoch 37/100\n",
      "112/112 [==============================] - 11s 100ms/step - loss: 0.1936 - accuracy: 0.9241 - auc: 0.9757 - precision: 0.9221 - recall: 0.9425 - val_loss: 0.2949 - val_accuracy: 0.8817 - val_auc: 0.9491 - val_precision: 0.8784 - val_recall: 0.9137 - lr: 1.0629e-04\n",
      "Epoch 38/100\n",
      "112/112 [==============================] - ETA: 0s - loss: 0.1807 - accuracy: 0.9294 - auc: 0.9775 - precision: 0.9263 - recall: 0.9482\n",
      "Epoch 38: ReduceLROnPlateau reducing learning rate to 2.125763421645388e-05.\n",
      "112/112 [==============================] - 11s 102ms/step - loss: 0.1807 - accuracy: 0.9294 - auc: 0.9775 - precision: 0.9263 - recall: 0.9482 - val_loss: 0.2829 - val_accuracy: 0.8839 - val_auc: 0.9542 - val_precision: 0.8788 - val_recall: 0.9177 - lr: 1.3947e-05\n",
      "Epoch 39/100\n",
      "112/112 [==============================] - 11s 100ms/step - loss: 0.2127 - accuracy: 0.9149 - auc: 0.9707 - precision: 0.9148 - recall: 0.9320 - val_loss: 0.2831 - val_accuracy: 0.8795 - val_auc: 0.9534 - val_precision: 0.8585 - val_recall: 0.9378 - lr: 1.3947e-05\n",
      "Epoch 40/100\n",
      "112/112 [==============================] - 11s 99ms/step - loss: 0.1939 - accuracy: 0.9252 - auc: 0.9749 - precision: 0.9204 - recall: 0.9480 - val_loss: 0.2791 - val_accuracy: 0.8839 - val_auc: 0.9549 - val_precision: 0.8689 - val_recall: 0.9317 - lr: 1.3947e-05\n",
      "Epoch 41/100\n",
      "112/112 [==============================] - 11s 100ms/step - loss: 0.1901 - accuracy: 0.9244 - auc: 0.9761 - precision: 0.9209 - recall: 0.9454 - val_loss: 0.3081 - val_accuracy: 0.8739 - val_auc: 0.9491 - val_precision: 0.9053 - val_recall: 0.8635 - lr: 1.3947e-05\n",
      "Epoch 42/100\n",
      "112/112 [==============================] - 11s 100ms/step - loss: 0.1981 - accuracy: 0.9227 - auc: 0.9739 - precision: 0.9241 - recall: 0.9376 - val_loss: 0.2800 - val_accuracy: 0.8817 - val_auc: 0.9542 - val_precision: 0.8657 - val_recall: 0.9317 - lr: 1.3947e-05\n",
      "Epoch 43/100\n",
      "112/112 [==============================] - ETA: 0s - loss: 0.1953 - accuracy: 0.9205 - auc: 0.9755 - precision: 0.9168 - recall: 0.9414\n",
      "Epoch 43: ReduceLROnPlateau reducing learning rate to 2.7894260711036626e-06.\n",
      "112/112 [==============================] - 11s 102ms/step - loss: 0.1953 - accuracy: 0.9205 - auc: 0.9755 - precision: 0.9168 - recall: 0.9414 - val_loss: 0.2784 - val_accuracy: 0.8783 - val_auc: 0.9551 - val_precision: 0.8691 - val_recall: 0.9197 - lr: 1.8301e-06\n",
      "Epoch 44/100\n",
      "112/112 [==============================] - 11s 99ms/step - loss: 0.1963 - accuracy: 0.9227 - auc: 0.9746 - precision: 0.9203 - recall: 0.9445 - val_loss: 0.2780 - val_accuracy: 0.8817 - val_auc: 0.9528 - val_precision: 0.8798 - val_recall: 0.9116 - lr: 1.8301e-06\n",
      "Epoch 45/100\n",
      "112/112 [==============================] - 11s 100ms/step - loss: 0.1931 - accuracy: 0.9272 - auc: 0.9748 - precision: 0.9196 - recall: 0.9501 - val_loss: 0.2722 - val_accuracy: 0.8973 - val_auc: 0.9557 - val_precision: 0.9044 - val_recall: 0.9116 - lr: 1.6471e-06\n",
      "Epoch 46/100\n",
      "112/112 [==============================] - 11s 100ms/step - loss: 0.1829 - accuracy: 0.9302 - auc: 0.9782 - precision: 0.9315 - recall: 0.9454 - val_loss: 0.2965 - val_accuracy: 0.8873 - val_auc: 0.9496 - val_precision: 0.8825 - val_recall: 0.9197 - lr: 1.6471e-06\n",
      "Epoch 47/100\n",
      "112/112 [==============================] - 11s 100ms/step - loss: 0.1882 - accuracy: 0.9300 - auc: 0.9770 - precision: 0.9305 - recall: 0.9441 - val_loss: 0.2966 - val_accuracy: 0.8906 - val_auc: 0.9492 - val_precision: 0.8984 - val_recall: 0.9056 - lr: 1.6471e-06\n",
      "Epoch 48/100\n",
      "112/112 [==============================] - ETA: 0s - loss: 0.1891 - accuracy: 0.9255 - auc: 0.9769 - precision: 0.9256 - recall: 0.9410\n",
      "Epoch 48: ReduceLROnPlateau reducing learning rate to 1e-06.\n",
      "112/112 [==============================] - 11s 102ms/step - loss: 0.1891 - accuracy: 0.9255 - auc: 0.9769 - precision: 0.9256 - recall: 0.9410 - val_loss: 0.2879 - val_accuracy: 0.8817 - val_auc: 0.9512 - val_precision: 0.8769 - val_recall: 0.9157 - lr: 5.9049e-07\n",
      "Epoch 49/100\n",
      "112/112 [==============================] - 11s 100ms/step - loss: 0.1971 - accuracy: 0.9205 - auc: 0.9745 - precision: 0.9171 - recall: 0.9424 - val_loss: 0.2910 - val_accuracy: 0.8795 - val_auc: 0.9490 - val_precision: 0.8869 - val_recall: 0.8976 - lr: 5.9049e-07\n",
      "Epoch 50/100\n",
      "112/112 [==============================] - 11s 100ms/step - loss: 0.1982 - accuracy: 0.9258 - auc: 0.9741 - precision: 0.9269 - recall: 0.9416 - val_loss: 0.2764 - val_accuracy: 0.8839 - val_auc: 0.9533 - val_precision: 0.8803 - val_recall: 0.9157 - lr: 5.9049e-07\n",
      "Epoch 51/100\n",
      "112/112 [==============================] - 11s 100ms/step - loss: 0.2032 - accuracy: 0.9219 - auc: 0.9724 - precision: 0.9153 - recall: 0.9466 - val_loss: 0.2860 - val_accuracy: 0.8828 - val_auc: 0.9522 - val_precision: 0.8786 - val_recall: 0.9157 - lr: 5.9049e-07\n",
      "Epoch 52/100\n",
      "112/112 [==============================] - 11s 100ms/step - loss: 0.1837 - accuracy: 0.9277 - auc: 0.9778 - precision: 0.9233 - recall: 0.9464 - val_loss: 0.3009 - val_accuracy: 0.8795 - val_auc: 0.9471 - val_precision: 0.8611 - val_recall: 0.9337 - lr: 5.9049e-07\n",
      "Epoch 53/100\n",
      "112/112 [==============================] - 11s 100ms/step - loss: 0.1809 - accuracy: 0.9266 - auc: 0.9783 - precision: 0.9238 - recall: 0.9464 - val_loss: 0.2942 - val_accuracy: 0.8806 - val_auc: 0.9532 - val_precision: 0.8587 - val_recall: 0.9398 - lr: 5.9049e-07\n",
      "Epoch 54/100\n",
      "112/112 [==============================] - 11s 100ms/step - loss: 0.1807 - accuracy: 0.9319 - auc: 0.9782 - precision: 0.9308 - recall: 0.9476 - val_loss: 0.2922 - val_accuracy: 0.8850 - val_auc: 0.9524 - val_precision: 0.8911 - val_recall: 0.9036 - lr: 5.3144e-07\n",
      "Epoch 55/100\n",
      "112/112 [==============================] - 11s 100ms/step - loss: 0.1794 - accuracy: 0.9297 - auc: 0.9786 - precision: 0.9319 - recall: 0.9447 - val_loss: 0.3108 - val_accuracy: 0.8817 - val_auc: 0.9476 - val_precision: 0.8643 - val_recall: 0.9337 - lr: 5.3144e-07\n",
      "Epoch 56/100\n",
      "112/112 [==============================] - 11s 100ms/step - loss: 0.1852 - accuracy: 0.9222 - auc: 0.9773 - precision: 0.9161 - recall: 0.9453 - val_loss: 0.2891 - val_accuracy: 0.8862 - val_auc: 0.9498 - val_precision: 0.8852 - val_recall: 0.9137 - lr: 5.3144e-07\n",
      "Epoch 57/100\n",
      "112/112 [==============================] - 11s 100ms/step - loss: 0.1838 - accuracy: 0.9272 - auc: 0.9781 - precision: 0.9297 - recall: 0.9413 - val_loss: 0.2971 - val_accuracy: 0.8783 - val_auc: 0.9516 - val_precision: 0.8636 - val_recall: 0.9277 - lr: 5.3144e-07\n",
      "Epoch 58/100\n",
      "112/112 [==============================] - 11s 100ms/step - loss: 0.1702 - accuracy: 0.9350 - auc: 0.9805 - precision: 0.9274 - recall: 0.9554 - val_loss: 0.2954 - val_accuracy: 0.8884 - val_auc: 0.9544 - val_precision: 0.8798 - val_recall: 0.9257 - lr: 5.3144e-07\n",
      "Epoch 59/100\n",
      "112/112 [==============================] - 11s 100ms/step - loss: 0.1643 - accuracy: 0.9375 - auc: 0.9823 - precision: 0.9349 - recall: 0.9553 - val_loss: 0.2877 - val_accuracy: 0.8828 - val_auc: 0.9539 - val_precision: 0.8743 - val_recall: 0.9217 - lr: 5.3144e-07\n",
      "Epoch 60/100\n",
      "112/112 [==============================] - 11s 100ms/step - loss: 0.1678 - accuracy: 0.9358 - auc: 0.9816 - precision: 0.9346 - recall: 0.9506 - val_loss: 0.3043 - val_accuracy: 0.8839 - val_auc: 0.9490 - val_precision: 0.8818 - val_recall: 0.9137 - lr: 5.3144e-07\n",
      "Epoch 61/100\n",
      "112/112 [==============================] - 11s 100ms/step - loss: 0.1725 - accuracy: 0.9314 - auc: 0.9808 - precision: 0.9304 - recall: 0.9473 - val_loss: 0.2988 - val_accuracy: 0.8862 - val_auc: 0.9508 - val_precision: 0.8822 - val_recall: 0.9177 - lr: 5.3144e-07\n",
      "Epoch 62/100\n",
      "112/112 [==============================] - 11s 100ms/step - loss: 0.1734 - accuracy: 0.9372 - auc: 0.9797 - precision: 0.9363 - recall: 0.9518 - val_loss: 0.2870 - val_accuracy: 0.8951 - val_auc: 0.9518 - val_precision: 0.9024 - val_recall: 0.9096 - lr: 5.3144e-07\n",
      "Epoch 63/100\n",
      "112/112 [==============================] - 11s 101ms/step - loss: 0.1665 - accuracy: 0.9369 - auc: 0.9801 - precision: 0.9300 - recall: 0.9582 - val_loss: 0.3050 - val_accuracy: 0.8828 - val_auc: 0.9485 - val_precision: 0.8876 - val_recall: 0.9036 - lr: 4.7830e-07\n",
      "Epoch 64/100\n",
      "112/112 [==============================] - 11s 101ms/step - loss: 0.1588 - accuracy: 0.9411 - auc: 0.9831 - precision: 0.9415 - recall: 0.9533 - val_loss: 0.2895 - val_accuracy: 0.8917 - val_auc: 0.9518 - val_precision: 0.8908 - val_recall: 0.9177 - lr: 4.7830e-07\n",
      "Epoch 65/100\n",
      "112/112 [==============================] - 11s 101ms/step - loss: 0.1716 - accuracy: 0.9347 - auc: 0.9801 - precision: 0.9332 - recall: 0.9501 - val_loss: 0.3010 - val_accuracy: 0.8850 - val_auc: 0.9484 - val_precision: 0.8958 - val_recall: 0.8976 - lr: 4.7830e-07\n",
      "Epoch 66/100\n",
      "112/112 [==============================] - 11s 101ms/step - loss: 0.1597 - accuracy: 0.9381 - auc: 0.9819 - precision: 0.9385 - recall: 0.9525 - val_loss: 0.2959 - val_accuracy: 0.8739 - val_auc: 0.9532 - val_precision: 0.8545 - val_recall: 0.9317 - lr: 4.7830e-07\n",
      "Epoch 67/100\n",
      "112/112 [==============================] - 11s 101ms/step - loss: 0.1461 - accuracy: 0.9408 - auc: 0.9867 - precision: 0.9355 - recall: 0.9575 - val_loss: 0.3063 - val_accuracy: 0.8828 - val_auc: 0.9508 - val_precision: 0.8816 - val_recall: 0.9116 - lr: 4.7830e-07\n",
      "Epoch 68/100\n",
      "112/112 [==============================] - 11s 101ms/step - loss: 0.1582 - accuracy: 0.9422 - auc: 0.9829 - precision: 0.9467 - recall: 0.9500 - val_loss: 0.3047 - val_accuracy: 0.8761 - val_auc: 0.9538 - val_precision: 0.8550 - val_recall: 0.9357 - lr: 4.7830e-07\n",
      "Epoch 69/100\n",
      "112/112 [==============================] - 11s 101ms/step - loss: 0.1489 - accuracy: 0.9425 - auc: 0.9852 - precision: 0.9392 - recall: 0.9590 - val_loss: 0.3172 - val_accuracy: 0.8772 - val_auc: 0.9501 - val_precision: 0.8619 - val_recall: 0.9277 - lr: 4.7830e-07\n",
      "Epoch 70/100\n",
      "112/112 [==============================] - 11s 101ms/step - loss: 0.1571 - accuracy: 0.9381 - auc: 0.9836 - precision: 0.9342 - recall: 0.9558 - val_loss: 0.2895 - val_accuracy: 0.8884 - val_auc: 0.9550 - val_precision: 0.8827 - val_recall: 0.9217 - lr: 4.7830e-07\n",
      "Epoch 71/100\n",
      "112/112 [==============================] - 11s 100ms/step - loss: 0.1436 - accuracy: 0.9434 - auc: 0.9863 - precision: 0.9473 - recall: 0.9516 - val_loss: 0.3101 - val_accuracy: 0.8873 - val_auc: 0.9510 - val_precision: 0.8738 - val_recall: 0.9317 - lr: 4.7830e-07\n",
      "Epoch 72/100\n",
      "112/112 [==============================] - 11s 101ms/step - loss: 0.1465 - accuracy: 0.9442 - auc: 0.9849 - precision: 0.9348 - recall: 0.9661 - val_loss: 0.3076 - val_accuracy: 0.8873 - val_auc: 0.9513 - val_precision: 0.8869 - val_recall: 0.9137 - lr: 4.3047e-07\n",
      "Epoch 73/100\n",
      "112/112 [==============================] - 11s 101ms/step - loss: 0.1463 - accuracy: 0.9467 - auc: 0.9851 - precision: 0.9478 - recall: 0.9568 - val_loss: 0.3182 - val_accuracy: 0.8828 - val_auc: 0.9511 - val_precision: 0.8786 - val_recall: 0.9157 - lr: 4.3047e-07\n",
      "Epoch 74/100\n",
      "112/112 [==============================] - 11s 101ms/step - loss: 0.1504 - accuracy: 0.9445 - auc: 0.9848 - precision: 0.9463 - recall: 0.9552 - val_loss: 0.3327 - val_accuracy: 0.8806 - val_auc: 0.9453 - val_precision: 0.8738 - val_recall: 0.9177 - lr: 4.3047e-07\n",
      "Epoch 75/100\n",
      "112/112 [==============================] - 11s 100ms/step - loss: 0.1398 - accuracy: 0.9467 - auc: 0.9861 - precision: 0.9364 - recall: 0.9684 - val_loss: 0.3432 - val_accuracy: 0.8728 - val_auc: 0.9459 - val_precision: 0.8794 - val_recall: 0.8936 - lr: 4.3047e-07\n",
      "Epoch 76/100\n",
      "112/112 [==============================] - 11s 100ms/step - loss: 0.1579 - accuracy: 0.9434 - auc: 0.9825 - precision: 0.9360 - recall: 0.9646 - val_loss: 0.3271 - val_accuracy: 0.8694 - val_auc: 0.9488 - val_precision: 0.8629 - val_recall: 0.9096 - lr: 4.3047e-07\n",
      "Epoch 77/100\n",
      "112/112 [==============================] - 11s 100ms/step - loss: 0.1491 - accuracy: 0.9473 - auc: 0.9843 - precision: 0.9455 - recall: 0.9603 - val_loss: 0.3364 - val_accuracy: 0.8683 - val_auc: 0.9454 - val_precision: 0.8831 - val_recall: 0.8795 - lr: 4.3047e-07\n",
      "Epoch 78/100\n",
      "112/112 [==============================] - 11s 100ms/step - loss: 0.1474 - accuracy: 0.9461 - auc: 0.9850 - precision: 0.9445 - recall: 0.9601 - val_loss: 0.3202 - val_accuracy: 0.8728 - val_auc: 0.9497 - val_precision: 0.8664 - val_recall: 0.9116 - lr: 4.3047e-07\n",
      "Epoch 79/100\n",
      "112/112 [==============================] - 11s 100ms/step - loss: 0.1385 - accuracy: 0.9481 - auc: 0.9872 - precision: 0.9502 - recall: 0.9568 - val_loss: 0.3379 - val_accuracy: 0.8761 - val_auc: 0.9434 - val_precision: 0.8817 - val_recall: 0.8976 - lr: 4.3047e-07\n",
      "Epoch 80/100\n",
      "112/112 [==============================] - 11s 100ms/step - loss: 0.1596 - accuracy: 0.9386 - auc: 0.9830 - precision: 0.9346 - recall: 0.9554 - val_loss: 0.3138 - val_accuracy: 0.8850 - val_auc: 0.9508 - val_precision: 0.8776 - val_recall: 0.9217 - lr: 4.3047e-07\n",
      "Epoch 81/100\n",
      "112/112 [==============================] - 11s 100ms/step - loss: 0.1214 - accuracy: 0.9540 - auc: 0.9900 - precision: 0.9496 - recall: 0.9683 - val_loss: 0.3216 - val_accuracy: 0.8717 - val_auc: 0.9504 - val_precision: 0.8593 - val_recall: 0.9197 - lr: 3.8742e-07\n",
      "Epoch 82/100\n",
      "112/112 [==============================] - 11s 101ms/step - loss: 0.1369 - accuracy: 0.9484 - auc: 0.9871 - precision: 0.9461 - recall: 0.9626 - val_loss: 0.3347 - val_accuracy: 0.8783 - val_auc: 0.9459 - val_precision: 0.8929 - val_recall: 0.8876 - lr: 3.8742e-07\n",
      "Epoch 83/100\n",
      "112/112 [==============================] - 11s 101ms/step - loss: 0.1324 - accuracy: 0.9520 - auc: 0.9876 - precision: 0.9505 - recall: 0.9639 - val_loss: 0.3348 - val_accuracy: 0.8672 - val_auc: 0.9461 - val_precision: 0.8694 - val_recall: 0.8956 - lr: 3.8742e-07\n",
      "Epoch 84/100\n",
      "112/112 [==============================] - 11s 100ms/step - loss: 0.1363 - accuracy: 0.9492 - auc: 0.9871 - precision: 0.9463 - recall: 0.9617 - val_loss: 0.3250 - val_accuracy: 0.8862 - val_auc: 0.9495 - val_precision: 0.8867 - val_recall: 0.9116 - lr: 3.8742e-07\n",
      "Epoch 85/100\n",
      "112/112 [==============================] - 11s 100ms/step - loss: 0.1412 - accuracy: 0.9467 - auc: 0.9865 - precision: 0.9458 - recall: 0.9603 - val_loss: 0.3302 - val_accuracy: 0.8750 - val_auc: 0.9472 - val_precision: 0.8876 - val_recall: 0.8876 - lr: 3.8742e-07\n",
      "Epoch 86/100\n",
      "112/112 [==============================] - 11s 100ms/step - loss: 0.1368 - accuracy: 0.9506 - auc: 0.9873 - precision: 0.9483 - recall: 0.9631 - val_loss: 0.3348 - val_accuracy: 0.8806 - val_auc: 0.9458 - val_precision: 0.8811 - val_recall: 0.9076 - lr: 3.8742e-07\n",
      "Epoch 87/100\n",
      "112/112 [==============================] - 11s 100ms/step - loss: 0.1445 - accuracy: 0.9445 - auc: 0.9855 - precision: 0.9430 - recall: 0.9585 - val_loss: 0.3345 - val_accuracy: 0.8839 - val_auc: 0.9479 - val_precision: 0.8878 - val_recall: 0.9056 - lr: 3.8742e-07\n",
      "Epoch 88/100\n",
      "112/112 [==============================] - 11s 100ms/step - loss: 0.1196 - accuracy: 0.9545 - auc: 0.9905 - precision: 0.9475 - recall: 0.9711 - val_loss: 0.3270 - val_accuracy: 0.8873 - val_auc: 0.9472 - val_precision: 0.8946 - val_recall: 0.9036 - lr: 3.8742e-07\n",
      "Epoch 89/100\n",
      "112/112 [==============================] - 11s 100ms/step - loss: 0.1211 - accuracy: 0.9542 - auc: 0.9904 - precision: 0.9528 - recall: 0.9653 - val_loss: 0.3500 - val_accuracy: 0.8873 - val_auc: 0.9460 - val_precision: 0.8946 - val_recall: 0.9036 - lr: 3.8742e-07\n",
      "Epoch 90/100\n",
      "112/112 [==============================] - 11s 100ms/step - loss: 0.1289 - accuracy: 0.9517 - auc: 0.9883 - precision: 0.9529 - recall: 0.9627 - val_loss: 0.3281 - val_accuracy: 0.8862 - val_auc: 0.9491 - val_precision: 0.8852 - val_recall: 0.9137 - lr: 3.4868e-07\n",
      "Epoch 91/100\n",
      "112/112 [==============================] - 11s 100ms/step - loss: 0.1095 - accuracy: 0.9568 - auc: 0.9917 - precision: 0.9525 - recall: 0.9695 - val_loss: 0.3510 - val_accuracy: 0.8850 - val_auc: 0.9443 - val_precision: 0.8748 - val_recall: 0.9257 - lr: 3.4868e-07\n",
      "Epoch 92/100\n",
      "112/112 [==============================] - 11s 101ms/step - loss: 0.1313 - accuracy: 0.9487 - auc: 0.9886 - precision: 0.9521 - recall: 0.9560 - val_loss: 0.3317 - val_accuracy: 0.8806 - val_auc: 0.9478 - val_precision: 0.8752 - val_recall: 0.9157 - lr: 3.4868e-07\n",
      "Epoch 93/100\n",
      "112/112 [==============================] - 11s 100ms/step - loss: 0.1165 - accuracy: 0.9556 - auc: 0.9909 - precision: 0.9535 - recall: 0.9674 - val_loss: 0.3341 - val_accuracy: 0.8862 - val_auc: 0.9507 - val_precision: 0.8867 - val_recall: 0.9116 - lr: 3.4868e-07\n",
      "Epoch 94/100\n",
      "112/112 [==============================] - 11s 100ms/step - loss: 0.1183 - accuracy: 0.9598 - auc: 0.9893 - precision: 0.9577 - recall: 0.9703 - val_loss: 0.3397 - val_accuracy: 0.8806 - val_auc: 0.9473 - val_precision: 0.8752 - val_recall: 0.9157 - lr: 3.4868e-07\n",
      "Epoch 95/100\n",
      "112/112 [==============================] - 11s 100ms/step - loss: 0.1228 - accuracy: 0.9551 - auc: 0.9897 - precision: 0.9562 - recall: 0.9643 - val_loss: 0.3335 - val_accuracy: 0.8839 - val_auc: 0.9494 - val_precision: 0.8833 - val_recall: 0.9116 - lr: 3.4868e-07\n",
      "Epoch 96/100\n",
      "112/112 [==============================] - 11s 100ms/step - loss: 0.1293 - accuracy: 0.9495 - auc: 0.9892 - precision: 0.9437 - recall: 0.9653 - val_loss: 0.3366 - val_accuracy: 0.8839 - val_auc: 0.9475 - val_precision: 0.8924 - val_recall: 0.8996 - lr: 3.4868e-07\n",
      "Epoch 97/100\n",
      "112/112 [==============================] - 11s 100ms/step - loss: 0.1249 - accuracy: 0.9551 - auc: 0.9890 - precision: 0.9502 - recall: 0.9682 - val_loss: 0.3474 - val_accuracy: 0.8817 - val_auc: 0.9471 - val_precision: 0.8858 - val_recall: 0.9036 - lr: 3.4868e-07\n",
      "Epoch 98/100\n",
      "112/112 [==============================] - 11s 100ms/step - loss: 0.1190 - accuracy: 0.9531 - auc: 0.9909 - precision: 0.9604 - recall: 0.9576 - val_loss: 0.3353 - val_accuracy: 0.8873 - val_auc: 0.9483 - val_precision: 0.8795 - val_recall: 0.9237 - lr: 3.4868e-07\n",
      "Epoch 99/100\n",
      "112/112 [==============================] - 11s 100ms/step - loss: 0.1091 - accuracy: 0.9579 - auc: 0.9918 - precision: 0.9492 - recall: 0.9749 - val_loss: 0.3379 - val_accuracy: 0.8929 - val_auc: 0.9498 - val_precision: 0.8957 - val_recall: 0.9137 - lr: 3.1381e-07\n",
      "Epoch 100/100\n",
      "112/112 [==============================] - 11s 100ms/step - loss: 0.1095 - accuracy: 0.9568 - auc: 0.9918 - precision: 0.9574 - recall: 0.9656 - val_loss: 0.3589 - val_accuracy: 0.8884 - val_auc: 0.9417 - val_precision: 0.8902 - val_recall: 0.9116 - lr: 3.1381e-07\n",
      "28/28 [==============================] - 2s 72ms/step - loss: 0.3589 - accuracy: 0.8884 - auc: 0.9417 - precision: 0.8902 - recall: 0.9116\n",
      "Validation Loss: 0.3589\n",
      "Validation Accuracy: 0.8884\n",
      "Validation AUC: 0.9417\n",
      "Validation Precision: 0.8902\n",
      "Validation Recall: 0.9116\n",
      "28/28 [==============================] - 4s 64ms/step\n",
      "Confusion Matrix:\n",
      "[[342  56]\n",
      " [ 44 454]]\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.89      0.86      0.87       398\n",
      "           1       0.89      0.91      0.90       498\n",
      "\n",
      "    accuracy                           0.89       896\n",
      "   macro avg       0.89      0.89      0.89       896\n",
      "weighted avg       0.89      0.89      0.89       896\n",
      "\n",
      "Visualizations have been saved as PNG files.\n",
      "Predictions have been saved in 'predictions.csv'.\n",
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_1 (InputLayer)           [(None, 224, 224, 3  0           []                               \n",
      "                                )]                                                                \n",
      "                                                                                                  \n",
      " conv1_pad (ZeroPadding2D)      (None, 230, 230, 3)  0           ['input_1[0][0]']                \n",
      "                                                                                                  \n",
      " conv1_conv (Conv2D)            (None, 112, 112, 64  9472        ['conv1_pad[0][0]']              \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " pool1_pad (ZeroPadding2D)      (None, 114, 114, 64  0           ['conv1_conv[0][0]']             \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " pool1_pool (MaxPooling2D)      (None, 56, 56, 64)   0           ['pool1_pad[0][0]']              \n",
      "                                                                                                  \n",
      " conv2_block1_preact_bn (BatchN  (None, 56, 56, 64)  256         ['pool1_pool[0][0]']             \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " conv2_block1_preact_relu (Acti  (None, 56, 56, 64)  0           ['conv2_block1_preact_bn[0][0]'] \n",
      " vation)                                                                                          \n",
      "                                                                                                  \n",
      " conv2_block1_1_conv (Conv2D)   (None, 56, 56, 64)   4096        ['conv2_block1_preact_relu[0][0]'\n",
      "                                                                 ]                                \n",
      "                                                                                                  \n",
      " conv2_block1_1_bn (BatchNormal  (None, 56, 56, 64)  256         ['conv2_block1_1_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv2_block1_1_relu (Activatio  (None, 56, 56, 64)  0           ['conv2_block1_1_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv2_block1_2_pad (ZeroPaddin  (None, 58, 58, 64)  0           ['conv2_block1_1_relu[0][0]']    \n",
      " g2D)                                                                                             \n",
      "                                                                                                  \n",
      " conv2_block1_2_conv (Conv2D)   (None, 56, 56, 64)   36864       ['conv2_block1_2_pad[0][0]']     \n",
      "                                                                                                  \n",
      " conv2_block1_2_bn (BatchNormal  (None, 56, 56, 64)  256         ['conv2_block1_2_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv2_block1_2_relu (Activatio  (None, 56, 56, 64)  0           ['conv2_block1_2_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv2_block1_0_conv (Conv2D)   (None, 56, 56, 256)  16640       ['conv2_block1_preact_relu[0][0]'\n",
      "                                                                 ]                                \n",
      "                                                                                                  \n",
      " conv2_block1_3_conv (Conv2D)   (None, 56, 56, 256)  16640       ['conv2_block1_2_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv2_block1_out (Add)         (None, 56, 56, 256)  0           ['conv2_block1_0_conv[0][0]',    \n",
      "                                                                  'conv2_block1_3_conv[0][0]']    \n",
      "                                                                                                  \n",
      " conv2_block2_preact_bn (BatchN  (None, 56, 56, 256)  1024       ['conv2_block1_out[0][0]']       \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " conv2_block2_preact_relu (Acti  (None, 56, 56, 256)  0          ['conv2_block2_preact_bn[0][0]'] \n",
      " vation)                                                                                          \n",
      "                                                                                                  \n",
      " conv2_block2_1_conv (Conv2D)   (None, 56, 56, 64)   16384       ['conv2_block2_preact_relu[0][0]'\n",
      "                                                                 ]                                \n",
      "                                                                                                  \n",
      " conv2_block2_1_bn (BatchNormal  (None, 56, 56, 64)  256         ['conv2_block2_1_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv2_block2_1_relu (Activatio  (None, 56, 56, 64)  0           ['conv2_block2_1_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv2_block2_2_pad (ZeroPaddin  (None, 58, 58, 64)  0           ['conv2_block2_1_relu[0][0]']    \n",
      " g2D)                                                                                             \n",
      "                                                                                                  \n",
      " conv2_block2_2_conv (Conv2D)   (None, 56, 56, 64)   36864       ['conv2_block2_2_pad[0][0]']     \n",
      "                                                                                                  \n",
      " conv2_block2_2_bn (BatchNormal  (None, 56, 56, 64)  256         ['conv2_block2_2_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv2_block2_2_relu (Activatio  (None, 56, 56, 64)  0           ['conv2_block2_2_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv2_block2_3_conv (Conv2D)   (None, 56, 56, 256)  16640       ['conv2_block2_2_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv2_block2_out (Add)         (None, 56, 56, 256)  0           ['conv2_block1_out[0][0]',       \n",
      "                                                                  'conv2_block2_3_conv[0][0]']    \n",
      "                                                                                                  \n",
      " conv2_block3_preact_bn (BatchN  (None, 56, 56, 256)  1024       ['conv2_block2_out[0][0]']       \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " conv2_block3_preact_relu (Acti  (None, 56, 56, 256)  0          ['conv2_block3_preact_bn[0][0]'] \n",
      " vation)                                                                                          \n",
      "                                                                                                  \n",
      " conv2_block3_1_conv (Conv2D)   (None, 56, 56, 64)   16384       ['conv2_block3_preact_relu[0][0]'\n",
      "                                                                 ]                                \n",
      "                                                                                                  \n",
      " conv2_block3_1_bn (BatchNormal  (None, 56, 56, 64)  256         ['conv2_block3_1_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv2_block3_1_relu (Activatio  (None, 56, 56, 64)  0           ['conv2_block3_1_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv2_block3_2_pad (ZeroPaddin  (None, 58, 58, 64)  0           ['conv2_block3_1_relu[0][0]']    \n",
      " g2D)                                                                                             \n",
      "                                                                                                  \n",
      " conv2_block3_2_conv (Conv2D)   (None, 28, 28, 64)   36864       ['conv2_block3_2_pad[0][0]']     \n",
      "                                                                                                  \n",
      " conv2_block3_2_bn (BatchNormal  (None, 28, 28, 64)  256         ['conv2_block3_2_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv2_block3_2_relu (Activatio  (None, 28, 28, 64)  0           ['conv2_block3_2_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " max_pooling2d (MaxPooling2D)   (None, 28, 28, 256)  0           ['conv2_block2_out[0][0]']       \n",
      "                                                                                                  \n",
      " conv2_block3_3_conv (Conv2D)   (None, 28, 28, 256)  16640       ['conv2_block3_2_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv2_block3_out (Add)         (None, 28, 28, 256)  0           ['max_pooling2d[0][0]',          \n",
      "                                                                  'conv2_block3_3_conv[0][0]']    \n",
      "                                                                                                  \n",
      " conv3_block1_preact_bn (BatchN  (None, 28, 28, 256)  1024       ['conv2_block3_out[0][0]']       \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " conv3_block1_preact_relu (Acti  (None, 28, 28, 256)  0          ['conv3_block1_preact_bn[0][0]'] \n",
      " vation)                                                                                          \n",
      "                                                                                                  \n",
      " conv3_block1_1_conv (Conv2D)   (None, 28, 28, 128)  32768       ['conv3_block1_preact_relu[0][0]'\n",
      "                                                                 ]                                \n",
      "                                                                                                  \n",
      " conv3_block1_1_bn (BatchNormal  (None, 28, 28, 128)  512        ['conv3_block1_1_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv3_block1_1_relu (Activatio  (None, 28, 28, 128)  0          ['conv3_block1_1_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv3_block1_2_pad (ZeroPaddin  (None, 30, 30, 128)  0          ['conv3_block1_1_relu[0][0]']    \n",
      " g2D)                                                                                             \n",
      "                                                                                                  \n",
      " conv3_block1_2_conv (Conv2D)   (None, 28, 28, 128)  147456      ['conv3_block1_2_pad[0][0]']     \n",
      "                                                                                                  \n",
      " conv3_block1_2_bn (BatchNormal  (None, 28, 28, 128)  512        ['conv3_block1_2_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv3_block1_2_relu (Activatio  (None, 28, 28, 128)  0          ['conv3_block1_2_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv3_block1_0_conv (Conv2D)   (None, 28, 28, 512)  131584      ['conv3_block1_preact_relu[0][0]'\n",
      "                                                                 ]                                \n",
      "                                                                                                  \n",
      " conv3_block1_3_conv (Conv2D)   (None, 28, 28, 512)  66048       ['conv3_block1_2_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv3_block1_out (Add)         (None, 28, 28, 512)  0           ['conv3_block1_0_conv[0][0]',    \n",
      "                                                                  'conv3_block1_3_conv[0][0]']    \n",
      "                                                                                                  \n",
      " conv3_block2_preact_bn (BatchN  (None, 28, 28, 512)  2048       ['conv3_block1_out[0][0]']       \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " conv3_block2_preact_relu (Acti  (None, 28, 28, 512)  0          ['conv3_block2_preact_bn[0][0]'] \n",
      " vation)                                                                                          \n",
      "                                                                                                  \n",
      " conv3_block2_1_conv (Conv2D)   (None, 28, 28, 128)  65536       ['conv3_block2_preact_relu[0][0]'\n",
      "                                                                 ]                                \n",
      "                                                                                                  \n",
      " conv3_block2_1_bn (BatchNormal  (None, 28, 28, 128)  512        ['conv3_block2_1_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv3_block2_1_relu (Activatio  (None, 28, 28, 128)  0          ['conv3_block2_1_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv3_block2_2_pad (ZeroPaddin  (None, 30, 30, 128)  0          ['conv3_block2_1_relu[0][0]']    \n",
      " g2D)                                                                                             \n",
      "                                                                                                  \n",
      " conv3_block2_2_conv (Conv2D)   (None, 28, 28, 128)  147456      ['conv3_block2_2_pad[0][0]']     \n",
      "                                                                                                  \n",
      " conv3_block2_2_bn (BatchNormal  (None, 28, 28, 128)  512        ['conv3_block2_2_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv3_block2_2_relu (Activatio  (None, 28, 28, 128)  0          ['conv3_block2_2_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv3_block2_3_conv (Conv2D)   (None, 28, 28, 512)  66048       ['conv3_block2_2_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv3_block2_out (Add)         (None, 28, 28, 512)  0           ['conv3_block1_out[0][0]',       \n",
      "                                                                  'conv3_block2_3_conv[0][0]']    \n",
      "                                                                                                  \n",
      " conv3_block3_preact_bn (BatchN  (None, 28, 28, 512)  2048       ['conv3_block2_out[0][0]']       \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " conv3_block3_preact_relu (Acti  (None, 28, 28, 512)  0          ['conv3_block3_preact_bn[0][0]'] \n",
      " vation)                                                                                          \n",
      "                                                                                                  \n",
      " conv3_block3_1_conv (Conv2D)   (None, 28, 28, 128)  65536       ['conv3_block3_preact_relu[0][0]'\n",
      "                                                                 ]                                \n",
      "                                                                                                  \n",
      " conv3_block3_1_bn (BatchNormal  (None, 28, 28, 128)  512        ['conv3_block3_1_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv3_block3_1_relu (Activatio  (None, 28, 28, 128)  0          ['conv3_block3_1_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv3_block3_2_pad (ZeroPaddin  (None, 30, 30, 128)  0          ['conv3_block3_1_relu[0][0]']    \n",
      " g2D)                                                                                             \n",
      "                                                                                                  \n",
      " conv3_block3_2_conv (Conv2D)   (None, 28, 28, 128)  147456      ['conv3_block3_2_pad[0][0]']     \n",
      "                                                                                                  \n",
      " conv3_block3_2_bn (BatchNormal  (None, 28, 28, 128)  512        ['conv3_block3_2_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv3_block3_2_relu (Activatio  (None, 28, 28, 128)  0          ['conv3_block3_2_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv3_block3_3_conv (Conv2D)   (None, 28, 28, 512)  66048       ['conv3_block3_2_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv3_block3_out (Add)         (None, 28, 28, 512)  0           ['conv3_block2_out[0][0]',       \n",
      "                                                                  'conv3_block3_3_conv[0][0]']    \n",
      "                                                                                                  \n",
      " conv3_block4_preact_bn (BatchN  (None, 28, 28, 512)  2048       ['conv3_block3_out[0][0]']       \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " conv3_block4_preact_relu (Acti  (None, 28, 28, 512)  0          ['conv3_block4_preact_bn[0][0]'] \n",
      " vation)                                                                                          \n",
      "                                                                                                  \n",
      " conv3_block4_1_conv (Conv2D)   (None, 28, 28, 128)  65536       ['conv3_block4_preact_relu[0][0]'\n",
      "                                                                 ]                                \n",
      "                                                                                                  \n",
      " conv3_block4_1_bn (BatchNormal  (None, 28, 28, 128)  512        ['conv3_block4_1_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv3_block4_1_relu (Activatio  (None, 28, 28, 128)  0          ['conv3_block4_1_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv3_block4_2_pad (ZeroPaddin  (None, 30, 30, 128)  0          ['conv3_block4_1_relu[0][0]']    \n",
      " g2D)                                                                                             \n",
      "                                                                                                  \n",
      " conv3_block4_2_conv (Conv2D)   (None, 14, 14, 128)  147456      ['conv3_block4_2_pad[0][0]']     \n",
      "                                                                                                  \n",
      " conv3_block4_2_bn (BatchNormal  (None, 14, 14, 128)  512        ['conv3_block4_2_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv3_block4_2_relu (Activatio  (None, 14, 14, 128)  0          ['conv3_block4_2_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " max_pooling2d_1 (MaxPooling2D)  (None, 14, 14, 512)  0          ['conv3_block3_out[0][0]']       \n",
      "                                                                                                  \n",
      " conv3_block4_3_conv (Conv2D)   (None, 14, 14, 512)  66048       ['conv3_block4_2_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv3_block4_out (Add)         (None, 14, 14, 512)  0           ['max_pooling2d_1[0][0]',        \n",
      "                                                                  'conv3_block4_3_conv[0][0]']    \n",
      "                                                                                                  \n",
      " conv4_block1_preact_bn (BatchN  (None, 14, 14, 512)  2048       ['conv3_block4_out[0][0]']       \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " conv4_block1_preact_relu (Acti  (None, 14, 14, 512)  0          ['conv4_block1_preact_bn[0][0]'] \n",
      " vation)                                                                                          \n",
      "                                                                                                  \n",
      " conv4_block1_1_conv (Conv2D)   (None, 14, 14, 256)  131072      ['conv4_block1_preact_relu[0][0]'\n",
      "                                                                 ]                                \n",
      "                                                                                                  \n",
      " conv4_block1_1_bn (BatchNormal  (None, 14, 14, 256)  1024       ['conv4_block1_1_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv4_block1_1_relu (Activatio  (None, 14, 14, 256)  0          ['conv4_block1_1_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv4_block1_2_pad (ZeroPaddin  (None, 16, 16, 256)  0          ['conv4_block1_1_relu[0][0]']    \n",
      " g2D)                                                                                             \n",
      "                                                                                                  \n",
      " conv4_block1_2_conv (Conv2D)   (None, 14, 14, 256)  589824      ['conv4_block1_2_pad[0][0]']     \n",
      "                                                                                                  \n",
      " conv4_block1_2_bn (BatchNormal  (None, 14, 14, 256)  1024       ['conv4_block1_2_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv4_block1_2_relu (Activatio  (None, 14, 14, 256)  0          ['conv4_block1_2_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv4_block1_0_conv (Conv2D)   (None, 14, 14, 1024  525312      ['conv4_block1_preact_relu[0][0]'\n",
      "                                )                                ]                                \n",
      "                                                                                                  \n",
      " conv4_block1_3_conv (Conv2D)   (None, 14, 14, 1024  263168      ['conv4_block1_2_relu[0][0]']    \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " conv4_block1_out (Add)         (None, 14, 14, 1024  0           ['conv4_block1_0_conv[0][0]',    \n",
      "                                )                                 'conv4_block1_3_conv[0][0]']    \n",
      "                                                                                                  \n",
      " conv4_block2_preact_bn (BatchN  (None, 14, 14, 1024  4096       ['conv4_block1_out[0][0]']       \n",
      " ormalization)                  )                                                                 \n",
      "                                                                                                  \n",
      " conv4_block2_preact_relu (Acti  (None, 14, 14, 1024  0          ['conv4_block2_preact_bn[0][0]'] \n",
      " vation)                        )                                                                 \n",
      "                                                                                                  \n",
      " conv4_block2_1_conv (Conv2D)   (None, 14, 14, 256)  262144      ['conv4_block2_preact_relu[0][0]'\n",
      "                                                                 ]                                \n",
      "                                                                                                  \n",
      " conv4_block2_1_bn (BatchNormal  (None, 14, 14, 256)  1024       ['conv4_block2_1_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv4_block2_1_relu (Activatio  (None, 14, 14, 256)  0          ['conv4_block2_1_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv4_block2_2_pad (ZeroPaddin  (None, 16, 16, 256)  0          ['conv4_block2_1_relu[0][0]']    \n",
      " g2D)                                                                                             \n",
      "                                                                                                  \n",
      " conv4_block2_2_conv (Conv2D)   (None, 14, 14, 256)  589824      ['conv4_block2_2_pad[0][0]']     \n",
      "                                                                                                  \n",
      " conv4_block2_2_bn (BatchNormal  (None, 14, 14, 256)  1024       ['conv4_block2_2_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv4_block2_2_relu (Activatio  (None, 14, 14, 256)  0          ['conv4_block2_2_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv4_block2_3_conv (Conv2D)   (None, 14, 14, 1024  263168      ['conv4_block2_2_relu[0][0]']    \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " conv4_block2_out (Add)         (None, 14, 14, 1024  0           ['conv4_block1_out[0][0]',       \n",
      "                                )                                 'conv4_block2_3_conv[0][0]']    \n",
      "                                                                                                  \n",
      " conv4_block3_preact_bn (BatchN  (None, 14, 14, 1024  4096       ['conv4_block2_out[0][0]']       \n",
      " ormalization)                  )                                                                 \n",
      "                                                                                                  \n",
      " conv4_block3_preact_relu (Acti  (None, 14, 14, 1024  0          ['conv4_block3_preact_bn[0][0]'] \n",
      " vation)                        )                                                                 \n",
      "                                                                                                  \n",
      " conv4_block3_1_conv (Conv2D)   (None, 14, 14, 256)  262144      ['conv4_block3_preact_relu[0][0]'\n",
      "                                                                 ]                                \n",
      "                                                                                                  \n",
      " conv4_block3_1_bn (BatchNormal  (None, 14, 14, 256)  1024       ['conv4_block3_1_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv4_block3_1_relu (Activatio  (None, 14, 14, 256)  0          ['conv4_block3_1_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv4_block3_2_pad (ZeroPaddin  (None, 16, 16, 256)  0          ['conv4_block3_1_relu[0][0]']    \n",
      " g2D)                                                                                             \n",
      "                                                                                                  \n",
      " conv4_block3_2_conv (Conv2D)   (None, 14, 14, 256)  589824      ['conv4_block3_2_pad[0][0]']     \n",
      "                                                                                                  \n",
      " conv4_block3_2_bn (BatchNormal  (None, 14, 14, 256)  1024       ['conv4_block3_2_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv4_block3_2_relu (Activatio  (None, 14, 14, 256)  0          ['conv4_block3_2_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv4_block3_3_conv (Conv2D)   (None, 14, 14, 1024  263168      ['conv4_block3_2_relu[0][0]']    \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " conv4_block3_out (Add)         (None, 14, 14, 1024  0           ['conv4_block2_out[0][0]',       \n",
      "                                )                                 'conv4_block3_3_conv[0][0]']    \n",
      "                                                                                                  \n",
      " conv4_block4_preact_bn (BatchN  (None, 14, 14, 1024  4096       ['conv4_block3_out[0][0]']       \n",
      " ormalization)                  )                                                                 \n",
      "                                                                                                  \n",
      " conv4_block4_preact_relu (Acti  (None, 14, 14, 1024  0          ['conv4_block4_preact_bn[0][0]'] \n",
      " vation)                        )                                                                 \n",
      "                                                                                                  \n",
      " conv4_block4_1_conv (Conv2D)   (None, 14, 14, 256)  262144      ['conv4_block4_preact_relu[0][0]'\n",
      "                                                                 ]                                \n",
      "                                                                                                  \n",
      " conv4_block4_1_bn (BatchNormal  (None, 14, 14, 256)  1024       ['conv4_block4_1_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv4_block4_1_relu (Activatio  (None, 14, 14, 256)  0          ['conv4_block4_1_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv4_block4_2_pad (ZeroPaddin  (None, 16, 16, 256)  0          ['conv4_block4_1_relu[0][0]']    \n",
      " g2D)                                                                                             \n",
      "                                                                                                  \n",
      " conv4_block4_2_conv (Conv2D)   (None, 14, 14, 256)  589824      ['conv4_block4_2_pad[0][0]']     \n",
      "                                                                                                  \n",
      " conv4_block4_2_bn (BatchNormal  (None, 14, 14, 256)  1024       ['conv4_block4_2_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv4_block4_2_relu (Activatio  (None, 14, 14, 256)  0          ['conv4_block4_2_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv4_block4_3_conv (Conv2D)   (None, 14, 14, 1024  263168      ['conv4_block4_2_relu[0][0]']    \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " conv4_block4_out (Add)         (None, 14, 14, 1024  0           ['conv4_block3_out[0][0]',       \n",
      "                                )                                 'conv4_block4_3_conv[0][0]']    \n",
      "                                                                                                  \n",
      " conv4_block5_preact_bn (BatchN  (None, 14, 14, 1024  4096       ['conv4_block4_out[0][0]']       \n",
      " ormalization)                  )                                                                 \n",
      "                                                                                                  \n",
      " conv4_block5_preact_relu (Acti  (None, 14, 14, 1024  0          ['conv4_block5_preact_bn[0][0]'] \n",
      " vation)                        )                                                                 \n",
      "                                                                                                  \n",
      " conv4_block5_1_conv (Conv2D)   (None, 14, 14, 256)  262144      ['conv4_block5_preact_relu[0][0]'\n",
      "                                                                 ]                                \n",
      "                                                                                                  \n",
      " conv4_block5_1_bn (BatchNormal  (None, 14, 14, 256)  1024       ['conv4_block5_1_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv4_block5_1_relu (Activatio  (None, 14, 14, 256)  0          ['conv4_block5_1_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv4_block5_2_pad (ZeroPaddin  (None, 16, 16, 256)  0          ['conv4_block5_1_relu[0][0]']    \n",
      " g2D)                                                                                             \n",
      "                                                                                                  \n",
      " conv4_block5_2_conv (Conv2D)   (None, 14, 14, 256)  589824      ['conv4_block5_2_pad[0][0]']     \n",
      "                                                                                                  \n",
      " conv4_block5_2_bn (BatchNormal  (None, 14, 14, 256)  1024       ['conv4_block5_2_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv4_block5_2_relu (Activatio  (None, 14, 14, 256)  0          ['conv4_block5_2_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv4_block5_3_conv (Conv2D)   (None, 14, 14, 1024  263168      ['conv4_block5_2_relu[0][0]']    \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " conv4_block5_out (Add)         (None, 14, 14, 1024  0           ['conv4_block4_out[0][0]',       \n",
      "                                )                                 'conv4_block5_3_conv[0][0]']    \n",
      "                                                                                                  \n",
      " conv4_block6_preact_bn (BatchN  (None, 14, 14, 1024  4096       ['conv4_block5_out[0][0]']       \n",
      " ormalization)                  )                                                                 \n",
      "                                                                                                  \n",
      " conv4_block6_preact_relu (Acti  (None, 14, 14, 1024  0          ['conv4_block6_preact_bn[0][0]'] \n",
      " vation)                        )                                                                 \n",
      "                                                                                                  \n",
      " conv4_block6_1_conv (Conv2D)   (None, 14, 14, 256)  262144      ['conv4_block6_preact_relu[0][0]'\n",
      "                                                                 ]                                \n",
      "                                                                                                  \n",
      " conv4_block6_1_bn (BatchNormal  (None, 14, 14, 256)  1024       ['conv4_block6_1_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv4_block6_1_relu (Activatio  (None, 14, 14, 256)  0          ['conv4_block6_1_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv4_block6_2_pad (ZeroPaddin  (None, 16, 16, 256)  0          ['conv4_block6_1_relu[0][0]']    \n",
      " g2D)                                                                                             \n",
      "                                                                                                  \n",
      " conv4_block6_2_conv (Conv2D)   (None, 7, 7, 256)    589824      ['conv4_block6_2_pad[0][0]']     \n",
      "                                                                                                  \n",
      " conv4_block6_2_bn (BatchNormal  (None, 7, 7, 256)   1024        ['conv4_block6_2_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv4_block6_2_relu (Activatio  (None, 7, 7, 256)   0           ['conv4_block6_2_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " max_pooling2d_2 (MaxPooling2D)  (None, 7, 7, 1024)  0           ['conv4_block5_out[0][0]']       \n",
      "                                                                                                  \n",
      " conv4_block6_3_conv (Conv2D)   (None, 7, 7, 1024)   263168      ['conv4_block6_2_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv4_block6_out (Add)         (None, 7, 7, 1024)   0           ['max_pooling2d_2[0][0]',        \n",
      "                                                                  'conv4_block6_3_conv[0][0]']    \n",
      "                                                                                                  \n",
      " conv5_block1_preact_bn (BatchN  (None, 7, 7, 1024)  4096        ['conv4_block6_out[0][0]']       \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " conv5_block1_preact_relu (Acti  (None, 7, 7, 1024)  0           ['conv5_block1_preact_bn[0][0]'] \n",
      " vation)                                                                                          \n",
      "                                                                                                  \n",
      " conv5_block1_1_conv (Conv2D)   (None, 7, 7, 512)    524288      ['conv5_block1_preact_relu[0][0]'\n",
      "                                                                 ]                                \n",
      "                                                                                                  \n",
      " conv5_block1_1_bn (BatchNormal  (None, 7, 7, 512)   2048        ['conv5_block1_1_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv5_block1_1_relu (Activatio  (None, 7, 7, 512)   0           ['conv5_block1_1_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv5_block1_2_pad (ZeroPaddin  (None, 9, 9, 512)   0           ['conv5_block1_1_relu[0][0]']    \n",
      " g2D)                                                                                             \n",
      "                                                                                                  \n",
      " conv5_block1_2_conv (Conv2D)   (None, 7, 7, 512)    2359296     ['conv5_block1_2_pad[0][0]']     \n",
      "                                                                                                  \n",
      " conv5_block1_2_bn (BatchNormal  (None, 7, 7, 512)   2048        ['conv5_block1_2_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv5_block1_2_relu (Activatio  (None, 7, 7, 512)   0           ['conv5_block1_2_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv5_block1_0_conv (Conv2D)   (None, 7, 7, 2048)   2099200     ['conv5_block1_preact_relu[0][0]'\n",
      "                                                                 ]                                \n",
      "                                                                                                  \n",
      " conv5_block1_3_conv (Conv2D)   (None, 7, 7, 2048)   1050624     ['conv5_block1_2_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv5_block1_out (Add)         (None, 7, 7, 2048)   0           ['conv5_block1_0_conv[0][0]',    \n",
      "                                                                  'conv5_block1_3_conv[0][0]']    \n",
      "                                                                                                  \n",
      " conv5_block2_preact_bn (BatchN  (None, 7, 7, 2048)  8192        ['conv5_block1_out[0][0]']       \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " conv5_block2_preact_relu (Acti  (None, 7, 7, 2048)  0           ['conv5_block2_preact_bn[0][0]'] \n",
      " vation)                                                                                          \n",
      "                                                                                                  \n",
      " conv5_block2_1_conv (Conv2D)   (None, 7, 7, 512)    1048576     ['conv5_block2_preact_relu[0][0]'\n",
      "                                                                 ]                                \n",
      "                                                                                                  \n",
      " conv5_block2_1_bn (BatchNormal  (None, 7, 7, 512)   2048        ['conv5_block2_1_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv5_block2_1_relu (Activatio  (None, 7, 7, 512)   0           ['conv5_block2_1_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv5_block2_2_pad (ZeroPaddin  (None, 9, 9, 512)   0           ['conv5_block2_1_relu[0][0]']    \n",
      " g2D)                                                                                             \n",
      "                                                                                                  \n",
      " conv5_block2_2_conv (Conv2D)   (None, 7, 7, 512)    2359296     ['conv5_block2_2_pad[0][0]']     \n",
      "                                                                                                  \n",
      " conv5_block2_2_bn (BatchNormal  (None, 7, 7, 512)   2048        ['conv5_block2_2_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv5_block2_2_relu (Activatio  (None, 7, 7, 512)   0           ['conv5_block2_2_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv5_block2_3_conv (Conv2D)   (None, 7, 7, 2048)   1050624     ['conv5_block2_2_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv5_block2_out (Add)         (None, 7, 7, 2048)   0           ['conv5_block1_out[0][0]',       \n",
      "                                                                  'conv5_block2_3_conv[0][0]']    \n",
      "                                                                                                  \n",
      " conv5_block3_preact_bn (BatchN  (None, 7, 7, 2048)  8192        ['conv5_block2_out[0][0]']       \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " conv5_block3_preact_relu (Acti  (None, 7, 7, 2048)  0           ['conv5_block3_preact_bn[0][0]'] \n",
      " vation)                                                                                          \n",
      "                                                                                                  \n",
      " conv5_block3_1_conv (Conv2D)   (None, 7, 7, 512)    1048576     ['conv5_block3_preact_relu[0][0]'\n",
      "                                                                 ]                                \n",
      "                                                                                                  \n",
      " conv5_block3_1_bn (BatchNormal  (None, 7, 7, 512)   2048        ['conv5_block3_1_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv5_block3_1_relu (Activatio  (None, 7, 7, 512)   0           ['conv5_block3_1_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv5_block3_2_pad (ZeroPaddin  (None, 9, 9, 512)   0           ['conv5_block3_1_relu[0][0]']    \n",
      " g2D)                                                                                             \n",
      "                                                                                                  \n",
      " conv5_block3_2_conv (Conv2D)   (None, 7, 7, 512)    2359296     ['conv5_block3_2_pad[0][0]']     \n",
      "                                                                                                  \n",
      " conv5_block3_2_bn (BatchNormal  (None, 7, 7, 512)   2048        ['conv5_block3_2_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " input_2 (InputLayer)           [(None, 31)]         0           []                               \n",
      "                                                                                                  \n",
      " conv5_block3_2_relu (Activatio  (None, 7, 7, 512)   0           ['conv5_block3_2_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " dense_1 (Dense)                (None, 128)          4096        ['input_2[0][0]']                \n",
      "                                                                                                  \n",
      " conv5_block3_3_conv (Conv2D)   (None, 7, 7, 2048)   1050624     ['conv5_block3_2_relu[0][0]']    \n",
      "                                                                                                  \n",
      " batch_normalization_1 (BatchNo  (None, 128)         512         ['dense_1[0][0]']                \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " conv5_block3_out (Add)         (None, 7, 7, 2048)   0           ['conv5_block2_out[0][0]',       \n",
      "                                                                  'conv5_block3_3_conv[0][0]']    \n",
      "                                                                                                  \n",
      " dropout_1 (Dropout)            (None, 128)          0           ['batch_normalization_1[0][0]']  \n",
      "                                                                                                  \n",
      " post_bn (BatchNormalization)   (None, 7, 7, 2048)   8192        ['conv5_block3_out[0][0]']       \n",
      "                                                                                                  \n",
      " dense_2 (Dense)                (None, 256)          33024       ['dropout_1[0][0]']              \n",
      "                                                                                                  \n",
      " post_relu (Activation)         (None, 7, 7, 2048)   0           ['post_bn[0][0]']                \n",
      "                                                                                                  \n",
      " batch_normalization_2 (BatchNo  (None, 256)         1024        ['dense_2[0][0]']                \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " global_average_pooling2d (Glob  (None, 2048)        0           ['post_relu[0][0]']              \n",
      " alAveragePooling2D)                                                                              \n",
      "                                                                                                  \n",
      " dropout_2 (Dropout)            (None, 256)          0           ['batch_normalization_2[0][0]']  \n",
      "                                                                                                  \n",
      " dense (Dense)                  (None, 256)          524544      ['global_average_pooling2d[0][0]'\n",
      "                                                                 ]                                \n",
      "                                                                                                  \n",
      " dense_3 (Dense)                (None, 128)          32896       ['dropout_2[0][0]']              \n",
      "                                                                                                  \n",
      " batch_normalization (BatchNorm  (None, 256)         1024        ['dense[0][0]']                  \n",
      " alization)                                                                                       \n",
      "                                                                                                  \n",
      " batch_normalization_3 (BatchNo  (None, 128)         512         ['dense_3[0][0]']                \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " dropout (Dropout)              (None, 256)          0           ['batch_normalization[0][0]']    \n",
      "                                                                                                  \n",
      " dropout_3 (Dropout)            (None, 128)          0           ['batch_normalization_3[0][0]']  \n",
      "                                                                                                  \n",
      " concatenate (Concatenate)      (None, 384)          0           ['dropout[0][0]',                \n",
      "                                                                  'dropout_3[0][0]']              \n",
      "                                                                                                  \n",
      " dense_4 (Dense)                (None, 256)          98560       ['concatenate[0][0]']            \n",
      "                                                                                                  \n",
      " batch_normalization_4 (BatchNo  (None, 256)         1024        ['dense_4[0][0]']                \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " dropout_4 (Dropout)            (None, 256)          0           ['batch_normalization_4[0][0]']  \n",
      "                                                                                                  \n",
      " dense_5 (Dense)                (None, 128)          32896       ['dropout_4[0][0]']              \n",
      "                                                                                                  \n",
      " batch_normalization_5 (BatchNo  (None, 128)         512         ['dense_5[0][0]']                \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " dropout_5 (Dropout)            (None, 128)          0           ['batch_normalization_5[0][0]']  \n",
      "                                                                                                  \n",
      " dense_6 (Dense)                (None, 1)            129         ['dropout_5[0][0]']              \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 24,295,553\n",
      "Trainable params: 728,449\n",
      "Non-trainable params: 23,567,104\n",
      "__________________________________________________________________________________________________\n"
     ]
    }

{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fb13bc74-e90d-497f-be31-c0d4b6376045",
   "metadata": {},
   "source": [
    "# hybrid_model_optimized_final_fake"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "735d6e21-187c-4dd5-8943-439bec7f67d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original CSV data loaded. Number of rows: 15435\n",
      "Original distribution of labels:\n",
      "label_majority\n",
      "2    9758\n",
      "1    3241\n",
      "3    2113\n",
      "4     323\n",
      "Name: count, dtype: int64\n",
      "1 Physical GPUs, 1 Logical GPUs\n",
      "Mixed precision policy set to mixed_float16\n",
      "Found images: Filtered: 646, Not filtered: 27994\n",
      "Sample of extracted filenames:\n",
      "0       00042065\n",
      "1    00042065Alt\n",
      "2       00043749\n",
      "3    00043749Alt\n",
      "4       00080967\n",
      "Name: filename, dtype: object\n",
      "After undersampling: Number of rows: 2323\n",
      "Distribution of labels after undersampling:\n",
      "label\n",
      "0    2000\n",
      "1     323\n",
      "Name: count, dtype: int64\n",
      "Training data: 1858, Validation data: 465\n",
      "INFO:tensorflow:Using MirroredStrategy with devices ('/job:localhost/replica:0/task:0/device:GPU:0',)\n",
      "Epoch 1/100\n",
      "58/58 [==============================] - 21s 163ms/step - loss: 0.7933 - accuracy: 0.6067 - auc: 0.6359 - precision_4: 0.1887 - recall_4: 0.5543 - val_loss: 0.6030 - val_accuracy: 0.7232 - val_auc: 0.7177 - val_precision_4: 0.2701 - val_recall_4: 0.6066 - lr: 0.0010\n",
      "Epoch 2/100\n",
      "58/58 [==============================] - 7s 120ms/step - loss: 0.5886 - accuracy: 0.7198 - auc: 0.7003 - precision_4: 0.2519 - recall_4: 0.5155 - val_loss: 0.3558 - val_accuracy: 0.8906 - val_auc: 0.8223 - val_precision_4: 0.6875 - val_recall_4: 0.3607 - lr: 0.0010\n",
      "Epoch 3/100\n",
      "58/58 [==============================] - 7s 121ms/step - loss: 0.4639 - accuracy: 0.8039 - auc: 0.7164 - precision_4: 0.3384 - recall_4: 0.4302 - val_loss: 0.3339 - val_accuracy: 0.8884 - val_auc: 0.8367 - val_precision_4: 0.6410 - val_recall_4: 0.4098 - lr: 0.0010\n",
      "Epoch 4/100\n",
      "58/58 [==============================] - 6s 105ms/step - loss: 0.3887 - accuracy: 0.8411 - auc: 0.7899 - precision_4: 0.4339 - recall_4: 0.5000 - val_loss: 0.3046 - val_accuracy: 0.8973 - val_auc: 0.8262 - val_precision_4: 0.7143 - val_recall_4: 0.4098 - lr: 0.0010\n",
      "Epoch 5/100\n",
      "58/58 [==============================] - 7s 121ms/step - loss: 0.3529 - accuracy: 0.8551 - auc: 0.8232 - precision_4: 0.4821 - recall_4: 0.4654 - val_loss: 0.2479 - val_accuracy: 0.9018 - val_auc: 0.9186 - val_precision_4: 0.8148 - val_recall_4: 0.3607 - lr: 0.0010\n",
      "Epoch 6/100\n",
      "58/58 [==============================] - 7s 121ms/step - loss: 0.2904 - accuracy: 0.8831 - auc: 0.8747 - precision_4: 0.5738 - recall_4: 0.5534 - val_loss: 0.2230 - val_accuracy: 0.9196 - val_auc: 0.9304 - val_precision_4: 0.8378 - val_recall_4: 0.5082 - lr: 0.0010\n",
      "Epoch 7/100\n",
      "58/58 [==============================] - 7s 121ms/step - loss: 0.3043 - accuracy: 0.8825 - auc: 0.8484 - precision_4: 0.5929 - recall_4: 0.5154 - val_loss: 0.2055 - val_accuracy: 0.9107 - val_auc: 0.9452 - val_precision_4: 0.7692 - val_recall_4: 0.4918 - lr: 0.0010\n",
      "Epoch 8/100\n",
      "58/58 [==============================] - 6s 105ms/step - loss: 0.2629 - accuracy: 0.8939 - auc: 0.8980 - precision_4: 0.6255 - recall_4: 0.6038 - val_loss: 0.2132 - val_accuracy: 0.9129 - val_auc: 0.9377 - val_precision_4: 0.7895 - val_recall_4: 0.4918 - lr: 0.0010\n",
      "Epoch 9/100\n",
      "58/58 [==============================] - 7s 121ms/step - loss: 0.2566 - accuracy: 0.9057 - auc: 0.8994 - precision_4: 0.6784 - recall_4: 0.6016 - val_loss: 0.1991 - val_accuracy: 0.9196 - val_auc: 0.9477 - val_precision_4: 0.8378 - val_recall_4: 0.5082 - lr: 0.0010\n",
      "Epoch 10/100\n",
      "58/58 [==============================] - 7s 122ms/step - loss: 0.2490 - accuracy: 0.8992 - auc: 0.9033 - precision_4: 0.6550 - recall_4: 0.5814 - val_loss: 0.1952 - val_accuracy: 0.9152 - val_auc: 0.9569 - val_precision_4: 0.8108 - val_recall_4: 0.4918 - lr: 0.0010\n",
      "Epoch 11/100\n",
      "58/58 [==============================] - 7s 122ms/step - loss: 0.2094 - accuracy: 0.9219 - auc: 0.9276 - precision_4: 0.7446 - recall_4: 0.6667 - val_loss: 0.1795 - val_accuracy: 0.9219 - val_auc: 0.9656 - val_precision_4: 0.8095 - val_recall_4: 0.5574 - lr: 0.0010\n",
      "Epoch 12/100\n",
      "58/58 [==============================] - 6s 105ms/step - loss: 0.2195 - accuracy: 0.9192 - auc: 0.9272 - precision_4: 0.7424 - recall_4: 0.6513 - val_loss: 0.1770 - val_accuracy: 0.9308 - val_auc: 0.9615 - val_precision_4: 0.8125 - val_recall_4: 0.6393 - lr: 0.0010\n",
      "Epoch 13/100\n",
      "58/58 [==============================] - 6s 106ms/step - loss: 0.2001 - accuracy: 0.9235 - auc: 0.9332 - precision_4: 0.7384 - recall_4: 0.6863 - val_loss: 0.1756 - val_accuracy: 0.9241 - val_auc: 0.9653 - val_precision_4: 0.8000 - val_recall_4: 0.5902 - lr: 0.0010\n",
      "Epoch 14/100\n",
      "58/58 [==============================] - 7s 119ms/step - loss: 0.1958 - accuracy: 0.9246 - auc: 0.9417 - precision_4: 0.7794 - recall_4: 0.6260 - val_loss: 0.1657 - val_accuracy: 0.9263 - val_auc: 0.9675 - val_precision_4: 0.7593 - val_recall_4: 0.6721 - lr: 0.0010\n",
      "Epoch 15/100\n",
      "58/58 [==============================] - 7s 121ms/step - loss: 0.1799 - accuracy: 0.9332 - auc: 0.9523 - precision_4: 0.7897 - recall_4: 0.7104 - val_loss: 0.1703 - val_accuracy: 0.9263 - val_auc: 0.9699 - val_precision_4: 0.7917 - val_recall_4: 0.6230 - lr: 0.0010\n",
      "Epoch 16/100\n",
      "58/58 [==============================] - 6s 105ms/step - loss: 0.1771 - accuracy: 0.9267 - auc: 0.9576 - precision_4: 0.7625 - recall_4: 0.6985 - val_loss: 0.1680 - val_accuracy: 0.9397 - val_auc: 0.9696 - val_precision_4: 0.8148 - val_recall_4: 0.7213 - lr: 0.0010\n",
      "Epoch 17/100\n",
      "58/58 [==============================] - 6s 105ms/step - loss: 0.1874 - accuracy: 0.9278 - auc: 0.9446 - precision_4: 0.7742 - recall_4: 0.6640 - val_loss: 0.1710 - val_accuracy: 0.9286 - val_auc: 0.9698 - val_precision_4: 0.7636 - val_recall_4: 0.6885 - lr: 0.0010\n",
      "Epoch 18/100\n",
      "58/58 [==============================] - 7s 121ms/step - loss: 0.1612 - accuracy: 0.9407 - auc: 0.9586 - precision_4: 0.8213 - recall_4: 0.7395 - val_loss: 0.1571 - val_accuracy: 0.9375 - val_auc: 0.9722 - val_precision_4: 0.8000 - val_recall_4: 0.7213 - lr: 9.0000e-04\n",
      "Epoch 19/100\n",
      "58/58 [==============================] - 7s 121ms/step - loss: 0.1565 - accuracy: 0.9407 - auc: 0.9641 - precision_4: 0.8205 - recall_4: 0.7385 - val_loss: 0.1644 - val_accuracy: 0.9353 - val_auc: 0.9726 - val_precision_4: 0.7963 - val_recall_4: 0.7049 - lr: 9.0000e-04\n",
      "Epoch 20/100\n",
      "58/58 [==============================] - 6s 105ms/step - loss: 0.1590 - accuracy: 0.9450 - auc: 0.9617 - precision_4: 0.8451 - recall_4: 0.7403 - val_loss: 0.1625 - val_accuracy: 0.9375 - val_auc: 0.9717 - val_precision_4: 0.8000 - val_recall_4: 0.7213 - lr: 9.0000e-04\n",
      "Epoch 21/100\n",
      "58/58 [==============================] - 6s 105ms/step - loss: 0.1708 - accuracy: 0.9332 - auc: 0.9547 - precision_4: 0.7911 - recall_4: 0.6980 - val_loss: 0.1775 - val_accuracy: 0.9353 - val_auc: 0.9618 - val_precision_4: 0.8333 - val_recall_4: 0.6557 - lr: 9.0000e-04\n",
      "Epoch 22/100\n",
      "58/58 [==============================] - 7s 121ms/step - loss: 0.1566 - accuracy: 0.9375 - auc: 0.9625 - precision_4: 0.8114 - recall_4: 0.7171 - val_loss: 0.1579 - val_accuracy: 0.9353 - val_auc: 0.9729 - val_precision_4: 0.7667 - val_recall_4: 0.7541 - lr: 9.0000e-04\n",
      "Epoch 23/100\n",
      "58/58 [==============================] - 7s 121ms/step - loss: 0.1449 - accuracy: 0.9456 - auc: 0.9667 - precision_4: 0.8319 - recall_4: 0.7645 - val_loss: 0.1480 - val_accuracy: 0.9397 - val_auc: 0.9769 - val_precision_4: 0.7833 - val_recall_4: 0.7705 - lr: 9.0000e-04\n",
      "Epoch 24/100\n",
      "58/58 [==============================] - 7s 121ms/step - loss: 0.1405 - accuracy: 0.9510 - auc: 0.9710 - precision_4: 0.8590 - recall_4: 0.7761 - val_loss: 0.1383 - val_accuracy: 0.9464 - val_auc: 0.9787 - val_precision_4: 0.8136 - val_recall_4: 0.7869 - lr: 9.0000e-04\n",
      "Epoch 25/100\n",
      "58/58 [==============================] - 6s 105ms/step - loss: 0.1423 - accuracy: 0.9456 - auc: 0.9658 - precision_4: 0.8311 - recall_4: 0.7480 - val_loss: 0.1405 - val_accuracy: 0.9397 - val_auc: 0.9784 - val_precision_4: 0.7833 - val_recall_4: 0.7705 - lr: 9.0000e-04\n",
      "Epoch 26/100\n",
      "58/58 [==============================] - 6s 105ms/step - loss: 0.1402 - accuracy: 0.9429 - auc: 0.9715 - precision_4: 0.8107 - recall_4: 0.7665 - val_loss: 0.1524 - val_accuracy: 0.9464 - val_auc: 0.9757 - val_precision_4: 0.7681 - val_recall_4: 0.8689 - lr: 9.0000e-04\n",
      "Epoch 27/100\n",
      "58/58 [==============================] - 6s 105ms/step - loss: 0.1350 - accuracy: 0.9488 - auc: 0.9744 - precision_4: 0.8652 - recall_4: 0.7567 - val_loss: 0.1501 - val_accuracy: 0.9375 - val_auc: 0.9744 - val_precision_4: 0.7705 - val_recall_4: 0.7705 - lr: 9.0000e-04\n",
      "Epoch 28/100\n",
      "58/58 [==============================] - 6s 107ms/step - loss: 0.1164 - accuracy: 0.9553 - auc: 0.9821 - precision_4: 0.8745 - recall_4: 0.7977 - val_loss: 0.1449 - val_accuracy: 0.9375 - val_auc: 0.9775 - val_precision_4: 0.7895 - val_recall_4: 0.7377 - lr: 9.0000e-04\n",
      "Epoch 29/100\n",
      "58/58 [==============================] - ETA: 0s - loss: 0.1205 - accuracy: 0.9574 - auc: 0.9779 - precision_4: 0.8607 - recall_4: 0.8235\n",
      "Epoch 29: ReduceLROnPlateau reducing learning rate to 0.00018000000854954124.\n",
      "58/58 [==============================] - 6s 109ms/step - loss: 0.1205 - accuracy: 0.9574 - auc: 0.9779 - precision_4: 0.8607 - recall_4: 0.8235 - val_loss: 0.1691 - val_accuracy: 0.9353 - val_auc: 0.9763 - val_precision_4: 0.8333 - val_recall_4: 0.6557 - lr: 1.6200e-04\n",
      "Epoch 30/100\n",
      "58/58 [==============================] - 6s 105ms/step - loss: 0.1260 - accuracy: 0.9520 - auc: 0.9751 - precision_4: 0.8571 - recall_4: 0.7876 - val_loss: 0.1490 - val_accuracy: 0.9353 - val_auc: 0.9757 - val_precision_4: 0.7963 - val_recall_4: 0.7049 - lr: 1.6200e-04\n",
      "Epoch 31/100\n",
      "58/58 [==============================] - 6s 105ms/step - loss: 0.1227 - accuracy: 0.9467 - auc: 0.9806 - precision_4: 0.8153 - recall_4: 0.7930 - val_loss: 0.1794 - val_accuracy: 0.9375 - val_auc: 0.9720 - val_precision_4: 0.8000 - val_recall_4: 0.7213 - lr: 1.6200e-04\n",
      "Epoch 32/100\n",
      "58/58 [==============================] - 6s 105ms/step - loss: 0.1201 - accuracy: 0.9504 - auc: 0.9791 - precision_4: 0.8452 - recall_4: 0.7860 - val_loss: 0.1650 - val_accuracy: 0.9308 - val_auc: 0.9724 - val_precision_4: 0.7419 - val_recall_4: 0.7541 - lr: 1.6200e-04\n",
      "Epoch 33/100\n",
      "58/58 [==============================] - 6s 105ms/step - loss: 0.1196 - accuracy: 0.9553 - auc: 0.9743 - precision_4: 0.8625 - recall_4: 0.8054 - val_loss: 0.2004 - val_accuracy: 0.9219 - val_auc: 0.9598 - val_precision_4: 0.7708 - val_recall_4: 0.6066 - lr: 1.6200e-04\n",
      "Epoch 34/100\n",
      "58/58 [==============================] - ETA: 0s - loss: 0.1288 - accuracy: 0.9504 - auc: 0.9790 - precision_4: 0.8300 - recall_4: 0.8039\n",
      "Epoch 34: ReduceLROnPlateau reducing learning rate to 3.239999932702631e-05.\n",
      "58/58 [==============================] - 6s 109ms/step - loss: 0.1288 - accuracy: 0.9504 - auc: 0.9790 - precision_4: 0.8300 - recall_4: 0.8039 - val_loss: 0.1980 - val_accuracy: 0.9286 - val_auc: 0.9593 - val_precision_4: 0.8085 - val_recall_4: 0.6230 - lr: 2.9160e-05\n",
      "Epoch 35/100\n",
      "58/58 [==============================] - 6s 105ms/step - loss: 0.1283 - accuracy: 0.9488 - auc: 0.9782 - precision_4: 0.8565 - recall_4: 0.7689 - val_loss: 0.1475 - val_accuracy: 0.9397 - val_auc: 0.9766 - val_precision_4: 0.7656 - val_recall_4: 0.8033 - lr: 2.6244e-05\n",
      "Epoch 36/100\n",
      "58/58 [==============================] - 6s 104ms/step - loss: 0.1381 - accuracy: 0.9504 - auc: 0.9743 - precision_4: 0.8423 - recall_4: 0.7899 - val_loss: 0.1413 - val_accuracy: 0.9420 - val_auc: 0.9777 - val_precision_4: 0.7778 - val_recall_4: 0.8033 - lr: 2.6244e-05\n",
      "Epoch 37/100\n",
      "58/58 [==============================] - 6s 105ms/step - loss: 0.1237 - accuracy: 0.9483 - auc: 0.9789 - precision_4: 0.8211 - recall_4: 0.7953 - val_loss: 0.1499 - val_accuracy: 0.9487 - val_auc: 0.9764 - val_precision_4: 0.8167 - val_recall_4: 0.8033 - lr: 2.6244e-05\n",
      "Epoch 38/100\n",
      "58/58 [==============================] - 7s 121ms/step - loss: 0.1263 - accuracy: 0.9526 - auc: 0.9765 - precision_4: 0.8484 - recall_4: 0.8023 - val_loss: 0.1346 - val_accuracy: 0.9487 - val_auc: 0.9802 - val_precision_4: 0.7794 - val_recall_4: 0.8689 - lr: 2.6244e-05\n",
      "Epoch 39/100\n",
      "58/58 [==============================] - 6s 105ms/step - loss: 0.1184 - accuracy: 0.9531 - auc: 0.9804 - precision_4: 0.8536 - recall_4: 0.7969 - val_loss: 0.1563 - val_accuracy: 0.9397 - val_auc: 0.9724 - val_precision_4: 0.7833 - val_recall_4: 0.7705 - lr: 2.6244e-05\n",
      "Epoch 40/100\n",
      "58/58 [==============================] - 6s 105ms/step - loss: 0.1097 - accuracy: 0.9558 - auc: 0.9833 - precision_4: 0.8577 - recall_4: 0.8251 - val_loss: 0.1426 - val_accuracy: 0.9397 - val_auc: 0.9777 - val_precision_4: 0.7500 - val_recall_4: 0.8361 - lr: 2.6244e-05\n",
      "Epoch 41/100\n",
      "58/58 [==============================] - 6s 104ms/step - loss: 0.1039 - accuracy: 0.9585 - auc: 0.9844 - precision_4: 0.8548 - recall_4: 0.8306 - val_loss: 0.1410 - val_accuracy: 0.9420 - val_auc: 0.9781 - val_precision_4: 0.8070 - val_recall_4: 0.7541 - lr: 2.6244e-05\n",
      "Epoch 42/100\n",
      "58/58 [==============================] - 6s 105ms/step - loss: 0.1007 - accuracy: 0.9612 - auc: 0.9868 - precision_4: 0.8769 - recall_4: 0.8507 - val_loss: 0.1499 - val_accuracy: 0.9464 - val_auc: 0.9763 - val_precision_4: 0.7937 - val_recall_4: 0.8197 - lr: 2.6244e-05\n",
      "Epoch 43/100\n",
      "58/58 [==============================] - 7s 121ms/step - loss: 0.1115 - accuracy: 0.9591 - auc: 0.9821 - precision_4: 0.8667 - recall_4: 0.8254 - val_loss: 0.1335 - val_accuracy: 0.9420 - val_auc: 0.9807 - val_precision_4: 0.8182 - val_recall_4: 0.7377 - lr: 2.6244e-05\n",
      "Epoch 44/100\n",
      "58/58 [==============================] - 6s 105ms/step - loss: 0.1002 - accuracy: 0.9596 - auc: 0.9842 - precision_4: 0.8922 - recall_4: 0.8054 - val_loss: 0.1346 - val_accuracy: 0.9531 - val_auc: 0.9805 - val_precision_4: 0.8030 - val_recall_4: 0.8689 - lr: 2.6244e-05\n",
      "Epoch 45/100\n",
      "58/58 [==============================] - 6s 104ms/step - loss: 0.0940 - accuracy: 0.9607 - auc: 0.9884 - precision_4: 0.8667 - recall_4: 0.8500 - val_loss: 0.1433 - val_accuracy: 0.9487 - val_auc: 0.9788 - val_precision_4: 0.7794 - val_recall_4: 0.8689 - lr: 2.6244e-05\n",
      "Epoch 46/100\n",
      "58/58 [==============================] - 6s 105ms/step - loss: 0.1069 - accuracy: 0.9607 - auc: 0.9828 - precision_4: 0.8725 - recall_4: 0.8423 - val_loss: 0.1622 - val_accuracy: 0.9420 - val_auc: 0.9726 - val_precision_4: 0.7778 - val_recall_4: 0.8033 - lr: 2.6244e-05\n",
      "Epoch 47/100\n",
      "58/58 [==============================] - 6s 105ms/step - loss: 0.0932 - accuracy: 0.9671 - auc: 0.9867 - precision_4: 0.9060 - recall_4: 0.8446 - val_loss: 0.1601 - val_accuracy: 0.9353 - val_auc: 0.9741 - val_precision_4: 0.7581 - val_recall_4: 0.7705 - lr: 2.6244e-05\n",
      "Epoch 48/100\n",
      "58/58 [==============================] - ETA: 0s - loss: 0.0898 - accuracy: 0.9644 - auc: 0.9900 - precision_4: 0.8812 - recall_4: 0.8679\n",
      "Epoch 48: ReduceLROnPlateau reducing learning rate to 5.2487994253169745e-06.\n",
      "58/58 [==============================] - 6s 109ms/step - loss: 0.0898 - accuracy: 0.9644 - auc: 0.9900 - precision_4: 0.8812 - recall_4: 0.8679 - val_loss: 0.1575 - val_accuracy: 0.9397 - val_auc: 0.9675 - val_precision_4: 0.8400 - val_recall_4: 0.6885 - lr: 4.2515e-06\n",
      "Epoch 49/100\n",
      "58/58 [==============================] - 6s 105ms/step - loss: 0.1075 - accuracy: 0.9591 - auc: 0.9853 - precision_4: 0.8807 - recall_4: 0.8199 - val_loss: 0.1361 - val_accuracy: 0.9464 - val_auc: 0.9792 - val_precision_4: 0.8033 - val_recall_4: 0.8033 - lr: 4.2515e-06\n",
      "Epoch 50/100\n",
      "58/58 [==============================] - 6s 105ms/step - loss: 0.1054 - accuracy: 0.9580 - auc: 0.9828 - precision_4: 0.8589 - recall_4: 0.8247 - val_loss: 0.1394 - val_accuracy: 0.9464 - val_auc: 0.9779 - val_precision_4: 0.7937 - val_recall_4: 0.8197 - lr: 4.2515e-06\n",
      "Epoch 51/100\n",
      "58/58 [==============================] - 6s 105ms/step - loss: 0.0948 - accuracy: 0.9596 - auc: 0.9894 - precision_4: 0.8701 - recall_4: 0.8403 - val_loss: 0.1549 - val_accuracy: 0.9375 - val_auc: 0.9752 - val_precision_4: 0.7538 - val_recall_4: 0.8033 - lr: 4.2515e-06\n",
      "Epoch 52/100\n",
      "58/58 [==============================] - 6s 105ms/step - loss: 0.1114 - accuracy: 0.9553 - auc: 0.9839 - precision_4: 0.8347 - recall_4: 0.8313 - val_loss: 0.1320 - val_accuracy: 0.9531 - val_auc: 0.9792 - val_precision_4: 0.8030 - val_recall_4: 0.8689 - lr: 3.8264e-06\n",
      "Epoch 53/100\n",
      "58/58 [==============================] - ETA: 0s - loss: 0.0904 - accuracy: 0.9671 - auc: 0.9904 - precision_4: 0.9195 - recall_4: 0.8378\n",
      "Epoch 53: ReduceLROnPlateau reducing learning rate to 1e-06.\n",
      "58/58 [==============================] - 6s 109ms/step - loss: 0.0904 - accuracy: 0.9671 - auc: 0.9904 - precision_4: 0.9195 - recall_4: 0.8378 - val_loss: 0.1459 - val_accuracy: 0.9464 - val_auc: 0.9777 - val_precision_4: 0.7846 - val_recall_4: 0.8361 - lr: 7.2900e-07\n",
      "Epoch 54/100\n",
      "58/58 [==============================] - 6s 105ms/step - loss: 0.0996 - accuracy: 0.9591 - auc: 0.9854 - precision_4: 0.8725 - recall_4: 0.8327 - val_loss: 0.1437 - val_accuracy: 0.9420 - val_auc: 0.9780 - val_precision_4: 0.7869 - val_recall_4: 0.7869 - lr: 7.2900e-07\n",
      "Epoch 55/100\n",
      "58/58 [==============================] - 6s 104ms/step - loss: 0.1143 - accuracy: 0.9553 - auc: 0.9812 - precision_4: 0.8425 - recall_4: 0.8327 - val_loss: 0.1557 - val_accuracy: 0.9330 - val_auc: 0.9750 - val_precision_4: 0.7719 - val_recall_4: 0.7213 - lr: 7.2900e-07\n",
      "Epoch 56/100\n",
      "58/58 [==============================] - 6s 104ms/step - loss: 0.1105 - accuracy: 0.9547 - auc: 0.9834 - precision_4: 0.8739 - recall_4: 0.7852 - val_loss: 0.1430 - val_accuracy: 0.9442 - val_auc: 0.9754 - val_precision_4: 0.8333 - val_recall_4: 0.7377 - lr: 7.2900e-07\n",
      "Epoch 57/100\n",
      "58/58 [==============================] - 6s 104ms/step - loss: 0.1077 - accuracy: 0.9617 - auc: 0.9824 - precision_4: 0.8802 - recall_4: 0.8353 - val_loss: 0.1488 - val_accuracy: 0.9397 - val_auc: 0.9754 - val_precision_4: 0.7742 - val_recall_4: 0.7869 - lr: 7.2900e-07\n",
      "Epoch 58/100\n",
      "58/58 [==============================] - 6s 104ms/step - loss: 0.0850 - accuracy: 0.9725 - auc: 0.9879 - precision_4: 0.9375 - recall_4: 0.8621 - val_loss: 0.1451 - val_accuracy: 0.9509 - val_auc: 0.9754 - val_precision_4: 0.7826 - val_recall_4: 0.8852 - lr: 7.2900e-07\n",
      "Epoch 59/100\n",
      "58/58 [==============================] - 6s 105ms/step - loss: 0.0789 - accuracy: 0.9682 - auc: 0.9925 - precision_4: 0.8984 - recall_4: 0.8667 - val_loss: 0.1653 - val_accuracy: 0.9420 - val_auc: 0.9748 - val_precision_4: 0.8182 - val_recall_4: 0.7377 - lr: 7.2900e-07\n",
      "Epoch 60/100\n",
      "58/58 [==============================] - 7s 121ms/step - loss: 0.1040 - accuracy: 0.9617 - auc: 0.9839 - precision_4: 0.8672 - recall_4: 0.8571 - val_loss: 0.1246 - val_accuracy: 0.9487 - val_auc: 0.9810 - val_precision_4: 0.8065 - val_recall_4: 0.8197 - lr: 7.2900e-07\n",
      "Epoch 61/100\n",
      "58/58 [==============================] - 6s 104ms/step - loss: 0.1098 - accuracy: 0.9526 - auc: 0.9819 - precision_4: 0.8502 - recall_4: 0.8046 - val_loss: 0.1557 - val_accuracy: 0.9464 - val_auc: 0.9742 - val_precision_4: 0.8033 - val_recall_4: 0.8033 - lr: 7.2900e-07\n",
      "Epoch 62/100\n",
      "58/58 [==============================] - 7s 120ms/step - loss: 0.0892 - accuracy: 0.9628 - auc: 0.9885 - precision_4: 0.8871 - recall_4: 0.8429 - val_loss: 0.1237 - val_accuracy: 0.9464 - val_auc: 0.9820 - val_precision_4: 0.7606 - val_recall_4: 0.8852 - lr: 7.2900e-07\n",
      "Epoch 63/100\n",
      "58/58 [==============================] - 6s 104ms/step - loss: 0.0849 - accuracy: 0.9688 - auc: 0.9894 - precision_4: 0.8934 - recall_4: 0.8720 - val_loss: 0.1489 - val_accuracy: 0.9464 - val_auc: 0.9783 - val_precision_4: 0.8136 - val_recall_4: 0.7869 - lr: 7.2900e-07\n",
      "Epoch 64/100\n",
      "58/58 [==============================] - 6s 104ms/step - loss: 0.0868 - accuracy: 0.9677 - auc: 0.9894 - precision_4: 0.8849 - recall_4: 0.8780 - val_loss: 0.1481 - val_accuracy: 0.9353 - val_auc: 0.9691 - val_precision_4: 0.7963 - val_recall_4: 0.7049 - lr: 7.2900e-07\n",
      "Epoch 65/100\n",
      "58/58 [==============================] - 6s 104ms/step - loss: 0.0819 - accuracy: 0.9704 - auc: 0.9905 - precision_4: 0.8974 - recall_4: 0.9007 - val_loss: 0.1473 - val_accuracy: 0.9487 - val_auc: 0.9764 - val_precision_4: 0.7879 - val_recall_4: 0.8525 - lr: 7.2900e-07\n",
      "Epoch 66/100\n",
      "58/58 [==============================] - 6s 105ms/step - loss: 0.0794 - accuracy: 0.9709 - auc: 0.9896 - precision_4: 0.8952 - recall_4: 0.8880 - val_loss: 0.1457 - val_accuracy: 0.9509 - val_auc: 0.9778 - val_precision_4: 0.8197 - val_recall_4: 0.8197 - lr: 7.2900e-07\n",
      "Epoch 67/100\n",
      "58/58 [==============================] - 6s 104ms/step - loss: 0.0750 - accuracy: 0.9720 - auc: 0.9931 - precision_4: 0.9177 - recall_4: 0.8745 - val_loss: 0.1463 - val_accuracy: 0.9487 - val_auc: 0.9802 - val_precision_4: 0.7639 - val_recall_4: 0.9016 - lr: 7.2900e-07\n",
      "Epoch 68/100\n",
      "58/58 [==============================] - 6s 104ms/step - loss: 0.0887 - accuracy: 0.9655 - auc: 0.9881 - precision_4: 0.8831 - recall_4: 0.8622 - val_loss: 0.1509 - val_accuracy: 0.9464 - val_auc: 0.9689 - val_precision_4: 0.7846 - val_recall_4: 0.8361 - lr: 7.2900e-07\n",
      "Epoch 69/100\n",
      "58/58 [==============================] - 6s 105ms/step - loss: 0.0693 - accuracy: 0.9768 - auc: 0.9915 - precision_4: 0.9286 - recall_4: 0.9035 - val_loss: 0.1486 - val_accuracy: 0.9442 - val_auc: 0.9770 - val_precision_4: 0.7571 - val_recall_4: 0.8689 - lr: 6.5610e-07\n",
      "Epoch 70/100\n",
      "58/58 [==============================] - 6s 105ms/step - loss: 0.0757 - accuracy: 0.9731 - auc: 0.9914 - precision_4: 0.9225 - recall_4: 0.8881 - val_loss: 0.1317 - val_accuracy: 0.9487 - val_auc: 0.9807 - val_precision_4: 0.8276 - val_recall_4: 0.7869 - lr: 6.5610e-07\n",
      "Epoch 71/100\n",
      "58/58 [==============================] - 6s 104ms/step - loss: 0.0601 - accuracy: 0.9822 - auc: 0.9948 - precision_4: 0.9344 - recall_4: 0.9306 - val_loss: 0.1482 - val_accuracy: 0.9420 - val_auc: 0.9797 - val_precision_4: 0.8182 - val_recall_4: 0.7377 - lr: 6.5610e-07\n",
      "Epoch 72/100\n",
      "58/58 [==============================] - 6s 104ms/step - loss: 0.0711 - accuracy: 0.9714 - auc: 0.9935 - precision_4: 0.9023 - recall_4: 0.8989 - val_loss: 0.1712 - val_accuracy: 0.9397 - val_auc: 0.9683 - val_precision_4: 0.8269 - val_recall_4: 0.7049 - lr: 6.5610e-07\n",
      "Epoch 73/100\n",
      "58/58 [==============================] - 6s 104ms/step - loss: 0.0527 - accuracy: 0.9833 - auc: 0.9962 - precision_4: 0.9419 - recall_4: 0.9382 - val_loss: 0.1457 - val_accuracy: 0.9397 - val_auc: 0.9782 - val_precision_4: 0.7742 - val_recall_4: 0.7869 - lr: 6.5610e-07\n",
      "Epoch 74/100\n",
      "58/58 [==============================] - 6s 105ms/step - loss: 0.0588 - accuracy: 0.9795 - auc: 0.9935 - precision_4: 0.9317 - recall_4: 0.9170 - val_loss: 0.1908 - val_accuracy: 0.9420 - val_auc: 0.9582 - val_precision_4: 0.8182 - val_recall_4: 0.7377 - lr: 6.5610e-07\n",
      "Epoch 75/100\n",
      "58/58 [==============================] - 6s 104ms/step - loss: 0.0612 - accuracy: 0.9768 - auc: 0.9934 - precision_4: 0.9385 - recall_4: 0.8911 - val_loss: 0.1406 - val_accuracy: 0.9442 - val_auc: 0.9813 - val_precision_4: 0.7647 - val_recall_4: 0.8525 - lr: 6.5610e-07\n",
      "Epoch 76/100\n",
      "58/58 [==============================] - 6s 105ms/step - loss: 0.0602 - accuracy: 0.9741 - auc: 0.9957 - precision_4: 0.9206 - recall_4: 0.8923 - val_loss: 0.1450 - val_accuracy: 0.9464 - val_auc: 0.9804 - val_precision_4: 0.7846 - val_recall_4: 0.8361 - lr: 6.5610e-07\n",
      "Epoch 77/100\n",
      "58/58 [==============================] - 6s 104ms/step - loss: 0.0580 - accuracy: 0.9774 - auc: 0.9937 - precision_4: 0.9144 - recall_4: 0.9216 - val_loss: 0.1428 - val_accuracy: 0.9442 - val_auc: 0.9797 - val_precision_4: 0.7727 - val_recall_4: 0.8361 - lr: 6.5610e-07\n",
      "Epoch 78/100\n",
      "58/58 [==============================] - 6s 105ms/step - loss: 0.0678 - accuracy: 0.9741 - auc: 0.9939 - precision_4: 0.9249 - recall_4: 0.8897 - val_loss: 0.1641 - val_accuracy: 0.9353 - val_auc: 0.9772 - val_precision_4: 0.7286 - val_recall_4: 0.8361 - lr: 6.5610e-07\n",
      "Epoch 79/100\n",
      "58/58 [==============================] - 6s 104ms/step - loss: 0.0561 - accuracy: 0.9790 - auc: 0.9961 - precision_4: 0.9029 - recall_4: 0.9544 - val_loss: 0.1589 - val_accuracy: 0.9576 - val_auc: 0.9714 - val_precision_4: 0.8182 - val_recall_4: 0.8852 - lr: 6.5610e-07\n",
      "Epoch 80/100\n",
      "58/58 [==============================] - 6s 104ms/step - loss: 0.0652 - accuracy: 0.9720 - auc: 0.9929 - precision_4: 0.9167 - recall_4: 0.8636 - val_loss: 0.1793 - val_accuracy: 0.9464 - val_auc: 0.9789 - val_precision_4: 0.7403 - val_recall_4: 0.9344 - lr: 6.5610e-07\n",
      "Epoch 81/100\n",
      "58/58 [==============================] - 6s 106ms/step - loss: 0.0573 - accuracy: 0.9811 - auc: 0.9958 - precision_4: 0.9288 - recall_4: 0.9394 - val_loss: 0.1780 - val_accuracy: 0.9487 - val_auc: 0.9696 - val_precision_4: 0.8276 - val_recall_4: 0.7869 - lr: 6.5610e-07\n",
      "Epoch 82/100\n",
      "58/58 [==============================] - 6s 105ms/step - loss: 0.0559 - accuracy: 0.9828 - auc: 0.9956 - precision_4: 0.9526 - recall_4: 0.9234 - val_loss: 0.1740 - val_accuracy: 0.9420 - val_auc: 0.9618 - val_precision_4: 0.7778 - val_recall_4: 0.8033 - lr: 6.5610e-07\n",
      "Epoch 83/100\n",
      "58/58 [==============================] - 6s 105ms/step - loss: 0.0619 - accuracy: 0.9774 - auc: 0.9953 - precision_4: 0.9255 - recall_4: 0.9112 - val_loss: 0.1781 - val_accuracy: 0.9442 - val_auc: 0.9607 - val_precision_4: 0.7812 - val_recall_4: 0.8197 - lr: 6.5610e-07\n",
      "Epoch 84/100\n",
      "58/58 [==============================] - 6s 104ms/step - loss: 0.0690 - accuracy: 0.9747 - auc: 0.9902 - precision_4: 0.9055 - recall_4: 0.9091 - val_loss: 0.1601 - val_accuracy: 0.9397 - val_auc: 0.9689 - val_precision_4: 0.7576 - val_recall_4: 0.8197 - lr: 6.5610e-07\n",
      "Epoch 85/100\n",
      "58/58 [==============================] - 6s 105ms/step - loss: 0.0493 - accuracy: 0.9806 - auc: 0.9970 - precision_4: 0.9294 - recall_4: 0.9294 - val_loss: 0.1785 - val_accuracy: 0.9442 - val_auc: 0.9671 - val_precision_4: 0.7903 - val_recall_4: 0.8033 - lr: 6.5610e-07\n",
      "Epoch 86/100\n",
      "58/58 [==============================] - 6s 105ms/step - loss: 0.0630 - accuracy: 0.9768 - auc: 0.9930 - precision_4: 0.9202 - recall_4: 0.9167 - val_loss: 0.1535 - val_accuracy: 0.9487 - val_auc: 0.9703 - val_precision_4: 0.7714 - val_recall_4: 0.8852 - lr: 6.5610e-07\n",
      "Epoch 87/100\n",
      "58/58 [==============================] - 6s 105ms/step - loss: 0.0626 - accuracy: 0.9784 - auc: 0.9947 - precision_4: 0.9336 - recall_4: 0.9122 - val_loss: 0.1542 - val_accuracy: 0.9464 - val_auc: 0.9788 - val_precision_4: 0.7467 - val_recall_4: 0.9180 - lr: 5.9049e-07\n",
      "Epoch 88/100\n",
      "58/58 [==============================] - 6s 104ms/step - loss: 0.0534 - accuracy: 0.9811 - auc: 0.9920 - precision_4: 0.9388 - recall_4: 0.9200 - val_loss: 0.1645 - val_accuracy: 0.9464 - val_auc: 0.9631 - val_precision_4: 0.7846 - val_recall_4: 0.8361 - lr: 5.9049e-07\n",
      "Epoch 89/100\n",
      "58/58 [==============================] - 6s 104ms/step - loss: 0.0542 - accuracy: 0.9811 - auc: 0.9946 - precision_4: 0.9252 - recall_4: 0.9363 - val_loss: 0.1692 - val_accuracy: 0.9420 - val_auc: 0.9628 - val_precision_4: 0.8070 - val_recall_4: 0.7541 - lr: 5.9049e-07\n",
      "Epoch 90/100\n",
      "58/58 [==============================] - 6s 105ms/step - loss: 0.0615 - accuracy: 0.9747 - auc: 0.9951 - precision_4: 0.9160 - recall_4: 0.9057 - val_loss: 0.1740 - val_accuracy: 0.9442 - val_auc: 0.9698 - val_precision_4: 0.7903 - val_recall_4: 0.8033 - lr: 5.9049e-07\n",
      "Epoch 91/100\n",
      "58/58 [==============================] - 6s 105ms/step - loss: 0.0473 - accuracy: 0.9806 - auc: 0.9973 - precision_4: 0.9423 - recall_4: 0.9211 - val_loss: 0.1686 - val_accuracy: 0.9397 - val_auc: 0.9763 - val_precision_4: 0.7656 - val_recall_4: 0.8033 - lr: 5.9049e-07\n",
      "Epoch 92/100\n",
      "58/58 [==============================] - 6s 105ms/step - loss: 0.0521 - accuracy: 0.9817 - auc: 0.9940 - precision_4: 0.9289 - recall_4: 0.9363 - val_loss: 0.1573 - val_accuracy: 0.9420 - val_auc: 0.9719 - val_precision_4: 0.7778 - val_recall_4: 0.8033 - lr: 5.9049e-07\n",
      "Epoch 93/100\n",
      "58/58 [==============================] - 6s 104ms/step - loss: 0.0508 - accuracy: 0.9833 - auc: 0.9966 - precision_4: 0.9518 - recall_4: 0.9258 - val_loss: 0.1598 - val_accuracy: 0.9464 - val_auc: 0.9720 - val_precision_4: 0.7937 - val_recall_4: 0.8197 - lr: 5.9049e-07\n",
      "Epoch 94/100\n",
      "58/58 [==============================] - 6s 105ms/step - loss: 0.0517 - accuracy: 0.9811 - auc: 0.9946 - precision_4: 0.9325 - recall_4: 0.9289 - val_loss: 0.1587 - val_accuracy: 0.9397 - val_auc: 0.9698 - val_precision_4: 0.7656 - val_recall_4: 0.8033 - lr: 5.9049e-07\n",
      "Epoch 95/100\n",
      "58/58 [==============================] - 6s 104ms/step - loss: 0.0618 - accuracy: 0.9758 - auc: 0.9936 - precision_4: 0.9101 - recall_4: 0.9205 - val_loss: 0.1640 - val_accuracy: 0.9464 - val_auc: 0.9704 - val_precision_4: 0.7846 - val_recall_4: 0.8361 - lr: 5.9049e-07\n",
      "Epoch 96/100\n",
      "58/58 [==============================] - 6s 105ms/step - loss: 0.0494 - accuracy: 0.9806 - auc: 0.9964 - precision_4: 0.9300 - recall_4: 0.9300 - val_loss: 0.1651 - val_accuracy: 0.9375 - val_auc: 0.9771 - val_precision_4: 0.7463 - val_recall_4: 0.8197 - lr: 5.9049e-07\n",
      "Epoch 97/100\n",
      "58/58 [==============================] - 6s 105ms/step - loss: 0.0489 - accuracy: 0.9849 - auc: 0.9962 - precision_4: 0.9459 - recall_4: 0.9459 - val_loss: 0.1680 - val_accuracy: 0.9509 - val_auc: 0.9773 - val_precision_4: 0.7746 - val_recall_4: 0.9016 - lr: 5.9049e-07\n",
      "Epoch 98/100\n",
      "58/58 [==============================] - 6s 105ms/step - loss: 0.0427 - accuracy: 0.9844 - auc: 0.9959 - precision_4: 0.9449 - recall_4: 0.9412 - val_loss: 0.1526 - val_accuracy: 0.9531 - val_auc: 0.9712 - val_precision_4: 0.8030 - val_recall_4: 0.8689 - lr: 5.9049e-07\n",
      "Epoch 99/100\n",
      "58/58 [==============================] - 6s 105ms/step - loss: 0.0384 - accuracy: 0.9860 - auc: 0.9963 - precision_4: 0.9567 - recall_4: 0.9419 - val_loss: 0.1751 - val_accuracy: 0.9420 - val_auc: 0.9698 - val_precision_4: 0.7397 - val_recall_4: 0.8852 - lr: 5.9049e-07\n",
      "Epoch 100/100\n",
      "58/58 [==============================] - 6s 105ms/step - loss: 0.0421 - accuracy: 0.9860 - auc: 0.9977 - precision_4: 0.9370 - recall_4: 0.9656 - val_loss: 0.1739 - val_accuracy: 0.9420 - val_auc: 0.9679 - val_precision_4: 0.7692 - val_recall_4: 0.8197 - lr: 5.9049e-07\n",
      "14/14 [==============================] - 1s 74ms/step - loss: 0.1739 - accuracy: 0.9420 - auc: 0.9679 - precision_4: 0.7692 - recall_4: 0.8197\n",
      "Validation Loss: 0.1739\n",
      "Validation Accuracy: 0.9420\n",
      "Validation AUC: 0.9679\n",
      "Validation Precision: 0.7692\n",
      "Validation Recall: 0.8197\n",
      "14/14 [==============================] - 3s 68ms/step\n",
      "Confusion Matrix:\n",
      "[[372  15]\n",
      " [ 11  50]]\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.96      0.97       387\n",
      "           1       0.77      0.82      0.79        61\n",
      "\n",
      "    accuracy                           0.94       448\n",
      "   macro avg       0.87      0.89      0.88       448\n",
      "weighted avg       0.94      0.94      0.94       448\n",
      "\n",
      "Visualizations have been saved as PNG files.\n",
      "Predictions have been saved in 'predictions.csv'.\n",
      "Model: \"model_4\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_9 (InputLayer)           [(None, 224, 224, 3  0           []                               \n",
      "                                )]                                                                \n",
      "                                                                                                  \n",
      " conv1_pad (ZeroPadding2D)      (None, 230, 230, 3)  0           ['input_9[0][0]']                \n",
      "                                                                                                  \n",
      " conv1_conv (Conv2D)            (None, 112, 112, 64  9472        ['conv1_pad[0][0]']              \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " pool1_pad (ZeroPadding2D)      (None, 114, 114, 64  0           ['conv1_conv[0][0]']             \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " pool1_pool (MaxPooling2D)      (None, 56, 56, 64)   0           ['pool1_pad[0][0]']              \n",
      "                                                                                                  \n",
      " conv2_block1_preact_bn (BatchN  (None, 56, 56, 64)  256         ['pool1_pool[0][0]']             \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " conv2_block1_preact_relu (Acti  (None, 56, 56, 64)  0           ['conv2_block1_preact_bn[0][0]'] \n",
      " vation)                                                                                          \n",
      "                                                                                                  \n",
      " conv2_block1_1_conv (Conv2D)   (None, 56, 56, 64)   4096        ['conv2_block1_preact_relu[0][0]'\n",
      "                                                                 ]                                \n",
      "                                                                                                  \n",
      " conv2_block1_1_bn (BatchNormal  (None, 56, 56, 64)  256         ['conv2_block1_1_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv2_block1_1_relu (Activatio  (None, 56, 56, 64)  0           ['conv2_block1_1_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv2_block1_2_pad (ZeroPaddin  (None, 58, 58, 64)  0           ['conv2_block1_1_relu[0][0]']    \n",
      " g2D)                                                                                             \n",
      "                                                                                                  \n",
      " conv2_block1_2_conv (Conv2D)   (None, 56, 56, 64)   36864       ['conv2_block1_2_pad[0][0]']     \n",
      "                                                                                                  \n",
      " conv2_block1_2_bn (BatchNormal  (None, 56, 56, 64)  256         ['conv2_block1_2_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv2_block1_2_relu (Activatio  (None, 56, 56, 64)  0           ['conv2_block1_2_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv2_block1_0_conv (Conv2D)   (None, 56, 56, 256)  16640       ['conv2_block1_preact_relu[0][0]'\n",
      "                                                                 ]                                \n",
      "                                                                                                  \n",
      " conv2_block1_3_conv (Conv2D)   (None, 56, 56, 256)  16640       ['conv2_block1_2_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv2_block1_out (Add)         (None, 56, 56, 256)  0           ['conv2_block1_0_conv[0][0]',    \n",
      "                                                                  'conv2_block1_3_conv[0][0]']    \n",
      "                                                                                                  \n",
      " conv2_block2_preact_bn (BatchN  (None, 56, 56, 256)  1024       ['conv2_block1_out[0][0]']       \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " conv2_block2_preact_relu (Acti  (None, 56, 56, 256)  0          ['conv2_block2_preact_bn[0][0]'] \n",
      " vation)                                                                                          \n",
      "                                                                                                  \n",
      " conv2_block2_1_conv (Conv2D)   (None, 56, 56, 64)   16384       ['conv2_block2_preact_relu[0][0]'\n",
      "                                                                 ]                                \n",
      "                                                                                                  \n",
      " conv2_block2_1_bn (BatchNormal  (None, 56, 56, 64)  256         ['conv2_block2_1_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv2_block2_1_relu (Activatio  (None, 56, 56, 64)  0           ['conv2_block2_1_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv2_block2_2_pad (ZeroPaddin  (None, 58, 58, 64)  0           ['conv2_block2_1_relu[0][0]']    \n",
      " g2D)                                                                                             \n",
      "                                                                                                  \n",
      " conv2_block2_2_conv (Conv2D)   (None, 56, 56, 64)   36864       ['conv2_block2_2_pad[0][0]']     \n",
      "                                                                                                  \n",
      " conv2_block2_2_bn (BatchNormal  (None, 56, 56, 64)  256         ['conv2_block2_2_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv2_block2_2_relu (Activatio  (None, 56, 56, 64)  0           ['conv2_block2_2_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv2_block2_3_conv (Conv2D)   (None, 56, 56, 256)  16640       ['conv2_block2_2_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv2_block2_out (Add)         (None, 56, 56, 256)  0           ['conv2_block1_out[0][0]',       \n",
      "                                                                  'conv2_block2_3_conv[0][0]']    \n",
      "                                                                                                  \n",
      " conv2_block3_preact_bn (BatchN  (None, 56, 56, 256)  1024       ['conv2_block2_out[0][0]']       \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " conv2_block3_preact_relu (Acti  (None, 56, 56, 256)  0          ['conv2_block3_preact_bn[0][0]'] \n",
      " vation)                                                                                          \n",
      "                                                                                                  \n",
      " conv2_block3_1_conv (Conv2D)   (None, 56, 56, 64)   16384       ['conv2_block3_preact_relu[0][0]'\n",
      "                                                                 ]                                \n",
      "                                                                                                  \n",
      " conv2_block3_1_bn (BatchNormal  (None, 56, 56, 64)  256         ['conv2_block3_1_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv2_block3_1_relu (Activatio  (None, 56, 56, 64)  0           ['conv2_block3_1_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv2_block3_2_pad (ZeroPaddin  (None, 58, 58, 64)  0           ['conv2_block3_1_relu[0][0]']    \n",
      " g2D)                                                                                             \n",
      "                                                                                                  \n",
      " conv2_block3_2_conv (Conv2D)   (None, 28, 28, 64)   36864       ['conv2_block3_2_pad[0][0]']     \n",
      "                                                                                                  \n",
      " conv2_block3_2_bn (BatchNormal  (None, 28, 28, 64)  256         ['conv2_block3_2_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv2_block3_2_relu (Activatio  (None, 28, 28, 64)  0           ['conv2_block3_2_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " max_pooling2d_15 (MaxPooling2D  (None, 28, 28, 256)  0          ['conv2_block2_out[0][0]']       \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " conv2_block3_3_conv (Conv2D)   (None, 28, 28, 256)  16640       ['conv2_block3_2_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv2_block3_out (Add)         (None, 28, 28, 256)  0           ['max_pooling2d_15[0][0]',       \n",
      "                                                                  'conv2_block3_3_conv[0][0]']    \n",
      "                                                                                                  \n",
      " conv3_block1_preact_bn (BatchN  (None, 28, 28, 256)  1024       ['conv2_block3_out[0][0]']       \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " conv3_block1_preact_relu (Acti  (None, 28, 28, 256)  0          ['conv3_block1_preact_bn[0][0]'] \n",
      " vation)                                                                                          \n",
      "                                                                                                  \n",
      " conv3_block1_1_conv (Conv2D)   (None, 28, 28, 128)  32768       ['conv3_block1_preact_relu[0][0]'\n",
      "                                                                 ]                                \n",
      "                                                                                                  \n",
      " conv3_block1_1_bn (BatchNormal  (None, 28, 28, 128)  512        ['conv3_block1_1_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv3_block1_1_relu (Activatio  (None, 28, 28, 128)  0          ['conv3_block1_1_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv3_block1_2_pad (ZeroPaddin  (None, 30, 30, 128)  0          ['conv3_block1_1_relu[0][0]']    \n",
      " g2D)                                                                                             \n",
      "                                                                                                  \n",
      " conv3_block1_2_conv (Conv2D)   (None, 28, 28, 128)  147456      ['conv3_block1_2_pad[0][0]']     \n",
      "                                                                                                  \n",
      " conv3_block1_2_bn (BatchNormal  (None, 28, 28, 128)  512        ['conv3_block1_2_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv3_block1_2_relu (Activatio  (None, 28, 28, 128)  0          ['conv3_block1_2_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv3_block1_0_conv (Conv2D)   (None, 28, 28, 512)  131584      ['conv3_block1_preact_relu[0][0]'\n",
      "                                                                 ]                                \n",
      "                                                                                                  \n",
      " conv3_block1_3_conv (Conv2D)   (None, 28, 28, 512)  66048       ['conv3_block1_2_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv3_block1_out (Add)         (None, 28, 28, 512)  0           ['conv3_block1_0_conv[0][0]',    \n",
      "                                                                  'conv3_block1_3_conv[0][0]']    \n",
      "                                                                                                  \n",
      " conv3_block2_preact_bn (BatchN  (None, 28, 28, 512)  2048       ['conv3_block1_out[0][0]']       \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " conv3_block2_preact_relu (Acti  (None, 28, 28, 512)  0          ['conv3_block2_preact_bn[0][0]'] \n",
      " vation)                                                                                          \n",
      "                                                                                                  \n",
      " conv3_block2_1_conv (Conv2D)   (None, 28, 28, 128)  65536       ['conv3_block2_preact_relu[0][0]'\n",
      "                                                                 ]                                \n",
      "                                                                                                  \n",
      " conv3_block2_1_bn (BatchNormal  (None, 28, 28, 128)  512        ['conv3_block2_1_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv3_block2_1_relu (Activatio  (None, 28, 28, 128)  0          ['conv3_block2_1_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv3_block2_2_pad (ZeroPaddin  (None, 30, 30, 128)  0          ['conv3_block2_1_relu[0][0]']    \n",
      " g2D)                                                                                             \n",
      "                                                                                                  \n",
      " conv3_block2_2_conv (Conv2D)   (None, 28, 28, 128)  147456      ['conv3_block2_2_pad[0][0]']     \n",
      "                                                                                                  \n",
      " conv3_block2_2_bn (BatchNormal  (None, 28, 28, 128)  512        ['conv3_block2_2_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv3_block2_2_relu (Activatio  (None, 28, 28, 128)  0          ['conv3_block2_2_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv3_block2_3_conv (Conv2D)   (None, 28, 28, 512)  66048       ['conv3_block2_2_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv3_block2_out (Add)         (None, 28, 28, 512)  0           ['conv3_block1_out[0][0]',       \n",
      "                                                                  'conv3_block2_3_conv[0][0]']    \n",
      "                                                                                                  \n",
      " conv3_block3_preact_bn (BatchN  (None, 28, 28, 512)  2048       ['conv3_block2_out[0][0]']       \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " conv3_block3_preact_relu (Acti  (None, 28, 28, 512)  0          ['conv3_block3_preact_bn[0][0]'] \n",
      " vation)                                                                                          \n",
      "                                                                                                  \n",
      " conv3_block3_1_conv (Conv2D)   (None, 28, 28, 128)  65536       ['conv3_block3_preact_relu[0][0]'\n",
      "                                                                 ]                                \n",
      "                                                                                                  \n",
      " conv3_block3_1_bn (BatchNormal  (None, 28, 28, 128)  512        ['conv3_block3_1_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv3_block3_1_relu (Activatio  (None, 28, 28, 128)  0          ['conv3_block3_1_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv3_block3_2_pad (ZeroPaddin  (None, 30, 30, 128)  0          ['conv3_block3_1_relu[0][0]']    \n",
      " g2D)                                                                                             \n",
      "                                                                                                  \n",
      " conv3_block3_2_conv (Conv2D)   (None, 28, 28, 128)  147456      ['conv3_block3_2_pad[0][0]']     \n",
      "                                                                                                  \n",
      " conv3_block3_2_bn (BatchNormal  (None, 28, 28, 128)  512        ['conv3_block3_2_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv3_block3_2_relu (Activatio  (None, 28, 28, 128)  0          ['conv3_block3_2_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv3_block3_3_conv (Conv2D)   (None, 28, 28, 512)  66048       ['conv3_block3_2_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv3_block3_out (Add)         (None, 28, 28, 512)  0           ['conv3_block2_out[0][0]',       \n",
      "                                                                  'conv3_block3_3_conv[0][0]']    \n",
      "                                                                                                  \n",
      " conv3_block4_preact_bn (BatchN  (None, 28, 28, 512)  2048       ['conv3_block3_out[0][0]']       \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " conv3_block4_preact_relu (Acti  (None, 28, 28, 512)  0          ['conv3_block4_preact_bn[0][0]'] \n",
      " vation)                                                                                          \n",
      "                                                                                                  \n",
      " conv3_block4_1_conv (Conv2D)   (None, 28, 28, 128)  65536       ['conv3_block4_preact_relu[0][0]'\n",
      "                                                                 ]                                \n",
      "                                                                                                  \n",
      " conv3_block4_1_bn (BatchNormal  (None, 28, 28, 128)  512        ['conv3_block4_1_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv3_block4_1_relu (Activatio  (None, 28, 28, 128)  0          ['conv3_block4_1_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv3_block4_2_pad (ZeroPaddin  (None, 30, 30, 128)  0          ['conv3_block4_1_relu[0][0]']    \n",
      " g2D)                                                                                             \n",
      "                                                                                                  \n",
      " conv3_block4_2_conv (Conv2D)   (None, 14, 14, 128)  147456      ['conv3_block4_2_pad[0][0]']     \n",
      "                                                                                                  \n",
      " conv3_block4_2_bn (BatchNormal  (None, 14, 14, 128)  512        ['conv3_block4_2_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv3_block4_2_relu (Activatio  (None, 14, 14, 128)  0          ['conv3_block4_2_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " max_pooling2d_16 (MaxPooling2D  (None, 14, 14, 512)  0          ['conv3_block3_out[0][0]']       \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " conv3_block4_3_conv (Conv2D)   (None, 14, 14, 512)  66048       ['conv3_block4_2_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv3_block4_out (Add)         (None, 14, 14, 512)  0           ['max_pooling2d_16[0][0]',       \n",
      "                                                                  'conv3_block4_3_conv[0][0]']    \n",
      "                                                                                                  \n",
      " conv4_block1_preact_bn (BatchN  (None, 14, 14, 512)  2048       ['conv3_block4_out[0][0]']       \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " conv4_block1_preact_relu (Acti  (None, 14, 14, 512)  0          ['conv4_block1_preact_bn[0][0]'] \n",
      " vation)                                                                                          \n",
      "                                                                                                  \n",
      " conv4_block1_1_conv (Conv2D)   (None, 14, 14, 256)  131072      ['conv4_block1_preact_relu[0][0]'\n",
      "                                                                 ]                                \n",
      "                                                                                                  \n",
      " conv4_block1_1_bn (BatchNormal  (None, 14, 14, 256)  1024       ['conv4_block1_1_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv4_block1_1_relu (Activatio  (None, 14, 14, 256)  0          ['conv4_block1_1_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv4_block1_2_pad (ZeroPaddin  (None, 16, 16, 256)  0          ['conv4_block1_1_relu[0][0]']    \n",
      " g2D)                                                                                             \n",
      "                                                                                                  \n",
      " conv4_block1_2_conv (Conv2D)   (None, 14, 14, 256)  589824      ['conv4_block1_2_pad[0][0]']     \n",
      "                                                                                                  \n",
      " conv4_block1_2_bn (BatchNormal  (None, 14, 14, 256)  1024       ['conv4_block1_2_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv4_block1_2_relu (Activatio  (None, 14, 14, 256)  0          ['conv4_block1_2_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv4_block1_0_conv (Conv2D)   (None, 14, 14, 1024  525312      ['conv4_block1_preact_relu[0][0]'\n",
      "                                )                                ]                                \n",
      "                                                                                                  \n",
      " conv4_block1_3_conv (Conv2D)   (None, 14, 14, 1024  263168      ['conv4_block1_2_relu[0][0]']    \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " conv4_block1_out (Add)         (None, 14, 14, 1024  0           ['conv4_block1_0_conv[0][0]',    \n",
      "                                )                                 'conv4_block1_3_conv[0][0]']    \n",
      "                                                                                                  \n",
      " conv4_block2_preact_bn (BatchN  (None, 14, 14, 1024  4096       ['conv4_block1_out[0][0]']       \n",
      " ormalization)                  )                                                                 \n",
      "                                                                                                  \n",
      " conv4_block2_preact_relu (Acti  (None, 14, 14, 1024  0          ['conv4_block2_preact_bn[0][0]'] \n",
      " vation)                        )                                                                 \n",
      "                                                                                                  \n",
      " conv4_block2_1_conv (Conv2D)   (None, 14, 14, 256)  262144      ['conv4_block2_preact_relu[0][0]'\n",
      "                                                                 ]                                \n",
      "                                                                                                  \n",
      " conv4_block2_1_bn (BatchNormal  (None, 14, 14, 256)  1024       ['conv4_block2_1_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv4_block2_1_relu (Activatio  (None, 14, 14, 256)  0          ['conv4_block2_1_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv4_block2_2_pad (ZeroPaddin  (None, 16, 16, 256)  0          ['conv4_block2_1_relu[0][0]']    \n",
      " g2D)                                                                                             \n",
      "                                                                                                  \n",
      " conv4_block2_2_conv (Conv2D)   (None, 14, 14, 256)  589824      ['conv4_block2_2_pad[0][0]']     \n",
      "                                                                                                  \n",
      " conv4_block2_2_bn (BatchNormal  (None, 14, 14, 256)  1024       ['conv4_block2_2_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv4_block2_2_relu (Activatio  (None, 14, 14, 256)  0          ['conv4_block2_2_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv4_block2_3_conv (Conv2D)   (None, 14, 14, 1024  263168      ['conv4_block2_2_relu[0][0]']    \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " conv4_block2_out (Add)         (None, 14, 14, 1024  0           ['conv4_block1_out[0][0]',       \n",
      "                                )                                 'conv4_block2_3_conv[0][0]']    \n",
      "                                                                                                  \n",
      " conv4_block3_preact_bn (BatchN  (None, 14, 14, 1024  4096       ['conv4_block2_out[0][0]']       \n",
      " ormalization)                  )                                                                 \n",
      "                                                                                                  \n",
      " conv4_block3_preact_relu (Acti  (None, 14, 14, 1024  0          ['conv4_block3_preact_bn[0][0]'] \n",
      " vation)                        )                                                                 \n",
      "                                                                                                  \n",
      " conv4_block3_1_conv (Conv2D)   (None, 14, 14, 256)  262144      ['conv4_block3_preact_relu[0][0]'\n",
      "                                                                 ]                                \n",
      "                                                                                                  \n",
      " conv4_block3_1_bn (BatchNormal  (None, 14, 14, 256)  1024       ['conv4_block3_1_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv4_block3_1_relu (Activatio  (None, 14, 14, 256)  0          ['conv4_block3_1_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv4_block3_2_pad (ZeroPaddin  (None, 16, 16, 256)  0          ['conv4_block3_1_relu[0][0]']    \n",
      " g2D)                                                                                             \n",
      "                                                                                                  \n",
      " conv4_block3_2_conv (Conv2D)   (None, 14, 14, 256)  589824      ['conv4_block3_2_pad[0][0]']     \n",
      "                                                                                                  \n",
      " conv4_block3_2_bn (BatchNormal  (None, 14, 14, 256)  1024       ['conv4_block3_2_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv4_block3_2_relu (Activatio  (None, 14, 14, 256)  0          ['conv4_block3_2_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv4_block3_3_conv (Conv2D)   (None, 14, 14, 1024  263168      ['conv4_block3_2_relu[0][0]']    \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " conv4_block3_out (Add)         (None, 14, 14, 1024  0           ['conv4_block2_out[0][0]',       \n",
      "                                )                                 'conv4_block3_3_conv[0][0]']    \n",
      "                                                                                                  \n",
      " conv4_block4_preact_bn (BatchN  (None, 14, 14, 1024  4096       ['conv4_block3_out[0][0]']       \n",
      " ormalization)                  )                                                                 \n",
      "                                                                                                  \n",
      " conv4_block4_preact_relu (Acti  (None, 14, 14, 1024  0          ['conv4_block4_preact_bn[0][0]'] \n",
      " vation)                        )                                                                 \n",
      "                                                                                                  \n",
      " conv4_block4_1_conv (Conv2D)   (None, 14, 14, 256)  262144      ['conv4_block4_preact_relu[0][0]'\n",
      "                                                                 ]                                \n",
      "                                                                                                  \n",
      " conv4_block4_1_bn (BatchNormal  (None, 14, 14, 256)  1024       ['conv4_block4_1_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv4_block4_1_relu (Activatio  (None, 14, 14, 256)  0          ['conv4_block4_1_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv4_block4_2_pad (ZeroPaddin  (None, 16, 16, 256)  0          ['conv4_block4_1_relu[0][0]']    \n",
      " g2D)                                                                                             \n",
      "                                                                                                  \n",
      " conv4_block4_2_conv (Conv2D)   (None, 14, 14, 256)  589824      ['conv4_block4_2_pad[0][0]']     \n",
      "                                                                                                  \n",
      " conv4_block4_2_bn (BatchNormal  (None, 14, 14, 256)  1024       ['conv4_block4_2_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv4_block4_2_relu (Activatio  (None, 14, 14, 256)  0          ['conv4_block4_2_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv4_block4_3_conv (Conv2D)   (None, 14, 14, 1024  263168      ['conv4_block4_2_relu[0][0]']    \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " conv4_block4_out (Add)         (None, 14, 14, 1024  0           ['conv4_block3_out[0][0]',       \n",
      "                                )                                 'conv4_block4_3_conv[0][0]']    \n",
      "                                                                                                  \n",
      " conv4_block5_preact_bn (BatchN  (None, 14, 14, 1024  4096       ['conv4_block4_out[0][0]']       \n",
      " ormalization)                  )                                                                 \n",
      "                                                                                                  \n",
      " conv4_block5_preact_relu (Acti  (None, 14, 14, 1024  0          ['conv4_block5_preact_bn[0][0]'] \n",
      " vation)                        )                                                                 \n",
      "                                                                                                  \n",
      " conv4_block5_1_conv (Conv2D)   (None, 14, 14, 256)  262144      ['conv4_block5_preact_relu[0][0]'\n",
      "                                                                 ]                                \n",
      "                                                                                                  \n",
      " conv4_block5_1_bn (BatchNormal  (None, 14, 14, 256)  1024       ['conv4_block5_1_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv4_block5_1_relu (Activatio  (None, 14, 14, 256)  0          ['conv4_block5_1_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv4_block5_2_pad (ZeroPaddin  (None, 16, 16, 256)  0          ['conv4_block5_1_relu[0][0]']    \n",
      " g2D)                                                                                             \n",
      "                                                                                                  \n",
      " conv4_block5_2_conv (Conv2D)   (None, 14, 14, 256)  589824      ['conv4_block5_2_pad[0][0]']     \n",
      "                                                                                                  \n",
      " conv4_block5_2_bn (BatchNormal  (None, 14, 14, 256)  1024       ['conv4_block5_2_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv4_block5_2_relu (Activatio  (None, 14, 14, 256)  0          ['conv4_block5_2_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv4_block5_3_conv (Conv2D)   (None, 14, 14, 1024  263168      ['conv4_block5_2_relu[0][0]']    \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " conv4_block5_out (Add)         (None, 14, 14, 1024  0           ['conv4_block4_out[0][0]',       \n",
      "                                )                                 'conv4_block5_3_conv[0][0]']    \n",
      "                                                                                                  \n",
      " conv4_block6_preact_bn (BatchN  (None, 14, 14, 1024  4096       ['conv4_block5_out[0][0]']       \n",
      " ormalization)                  )                                                                 \n",
      "                                                                                                  \n",
      " conv4_block6_preact_relu (Acti  (None, 14, 14, 1024  0          ['conv4_block6_preact_bn[0][0]'] \n",
      " vation)                        )                                                                 \n",
      "                                                                                                  \n",
      " conv4_block6_1_conv (Conv2D)   (None, 14, 14, 256)  262144      ['conv4_block6_preact_relu[0][0]'\n",
      "                                                                 ]                                \n",
      "                                                                                                  \n",
      " conv4_block6_1_bn (BatchNormal  (None, 14, 14, 256)  1024       ['conv4_block6_1_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv4_block6_1_relu (Activatio  (None, 14, 14, 256)  0          ['conv4_block6_1_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv4_block6_2_pad (ZeroPaddin  (None, 16, 16, 256)  0          ['conv4_block6_1_relu[0][0]']    \n",
      " g2D)                                                                                             \n",
      "                                                                                                  \n",
      " conv4_block6_2_conv (Conv2D)   (None, 7, 7, 256)    589824      ['conv4_block6_2_pad[0][0]']     \n",
      "                                                                                                  \n",
      " conv4_block6_2_bn (BatchNormal  (None, 7, 7, 256)   1024        ['conv4_block6_2_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv4_block6_2_relu (Activatio  (None, 7, 7, 256)   0           ['conv4_block6_2_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " max_pooling2d_17 (MaxPooling2D  (None, 7, 7, 1024)  0           ['conv4_block5_out[0][0]']       \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " conv4_block6_3_conv (Conv2D)   (None, 7, 7, 1024)   263168      ['conv4_block6_2_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv4_block6_out (Add)         (None, 7, 7, 1024)   0           ['max_pooling2d_17[0][0]',       \n",
      "                                                                  'conv4_block6_3_conv[0][0]']    \n",
      "                                                                                                  \n",
      " conv5_block1_preact_bn (BatchN  (None, 7, 7, 1024)  4096        ['conv4_block6_out[0][0]']       \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " conv5_block1_preact_relu (Acti  (None, 7, 7, 1024)  0           ['conv5_block1_preact_bn[0][0]'] \n",
      " vation)                                                                                          \n",
      "                                                                                                  \n",
      " conv5_block1_1_conv (Conv2D)   (None, 7, 7, 512)    524288      ['conv5_block1_preact_relu[0][0]'\n",
      "                                                                 ]                                \n",
      "                                                                                                  \n",
      " conv5_block1_1_bn (BatchNormal  (None, 7, 7, 512)   2048        ['conv5_block1_1_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv5_block1_1_relu (Activatio  (None, 7, 7, 512)   0           ['conv5_block1_1_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv5_block1_2_pad (ZeroPaddin  (None, 9, 9, 512)   0           ['conv5_block1_1_relu[0][0]']    \n",
      " g2D)                                                                                             \n",
      "                                                                                                  \n",
      " conv5_block1_2_conv (Conv2D)   (None, 7, 7, 512)    2359296     ['conv5_block1_2_pad[0][0]']     \n",
      "                                                                                                  \n",
      " conv5_block1_2_bn (BatchNormal  (None, 7, 7, 512)   2048        ['conv5_block1_2_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv5_block1_2_relu (Activatio  (None, 7, 7, 512)   0           ['conv5_block1_2_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv5_block1_0_conv (Conv2D)   (None, 7, 7, 2048)   2099200     ['conv5_block1_preact_relu[0][0]'\n",
      "                                                                 ]                                \n",
      "                                                                                                  \n",
      " conv5_block1_3_conv (Conv2D)   (None, 7, 7, 2048)   1050624     ['conv5_block1_2_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv5_block1_out (Add)         (None, 7, 7, 2048)   0           ['conv5_block1_0_conv[0][0]',    \n",
      "                                                                  'conv5_block1_3_conv[0][0]']    \n",
      "                                                                                                  \n",
      " conv5_block2_preact_bn (BatchN  (None, 7, 7, 2048)  8192        ['conv5_block1_out[0][0]']       \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " conv5_block2_preact_relu (Acti  (None, 7, 7, 2048)  0           ['conv5_block2_preact_bn[0][0]'] \n",
      " vation)                                                                                          \n",
      "                                                                                                  \n",
      " conv5_block2_1_conv (Conv2D)   (None, 7, 7, 512)    1048576     ['conv5_block2_preact_relu[0][0]'\n",
      "                                                                 ]                                \n",
      "                                                                                                  \n",
      " conv5_block2_1_bn (BatchNormal  (None, 7, 7, 512)   2048        ['conv5_block2_1_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv5_block2_1_relu (Activatio  (None, 7, 7, 512)   0           ['conv5_block2_1_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv5_block2_2_pad (ZeroPaddin  (None, 9, 9, 512)   0           ['conv5_block2_1_relu[0][0]']    \n",
      " g2D)                                                                                             \n",
      "                                                                                                  \n",
      " conv5_block2_2_conv (Conv2D)   (None, 7, 7, 512)    2359296     ['conv5_block2_2_pad[0][0]']     \n",
      "                                                                                                  \n",
      " conv5_block2_2_bn (BatchNormal  (None, 7, 7, 512)   2048        ['conv5_block2_2_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv5_block2_2_relu (Activatio  (None, 7, 7, 512)   0           ['conv5_block2_2_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv5_block2_3_conv (Conv2D)   (None, 7, 7, 2048)   1050624     ['conv5_block2_2_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv5_block2_out (Add)         (None, 7, 7, 2048)   0           ['conv5_block1_out[0][0]',       \n",
      "                                                                  'conv5_block2_3_conv[0][0]']    \n",
      "                                                                                                  \n",
      " conv5_block3_preact_bn (BatchN  (None, 7, 7, 2048)  8192        ['conv5_block2_out[0][0]']       \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " conv5_block3_preact_relu (Acti  (None, 7, 7, 2048)  0           ['conv5_block3_preact_bn[0][0]'] \n",
      " vation)                                                                                          \n",
      "                                                                                                  \n",
      " conv5_block3_1_conv (Conv2D)   (None, 7, 7, 512)    1048576     ['conv5_block3_preact_relu[0][0]'\n",
      "                                                                 ]                                \n",
      "                                                                                                  \n",
      " conv5_block3_1_bn (BatchNormal  (None, 7, 7, 512)   2048        ['conv5_block3_1_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv5_block3_1_relu (Activatio  (None, 7, 7, 512)   0           ['conv5_block3_1_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv5_block3_2_pad (ZeroPaddin  (None, 9, 9, 512)   0           ['conv5_block3_1_relu[0][0]']    \n",
      " g2D)                                                                                             \n",
      "                                                                                                  \n",
      " conv5_block3_2_conv (Conv2D)   (None, 7, 7, 512)    2359296     ['conv5_block3_2_pad[0][0]']     \n",
      "                                                                                                  \n",
      " conv5_block3_2_bn (BatchNormal  (None, 7, 7, 512)   2048        ['conv5_block3_2_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " input_10 (InputLayer)          [(None, 31)]         0           []                               \n",
      "                                                                                                  \n",
      " conv5_block3_2_relu (Activatio  (None, 7, 7, 512)   0           ['conv5_block3_2_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " dense_28 (Dense)               (None, 128)          4096        ['input_10[0][0]']               \n",
      "                                                                                                  \n",
      " conv5_block3_3_conv (Conv2D)   (None, 7, 7, 2048)   1050624     ['conv5_block3_2_relu[0][0]']    \n",
      "                                                                                                  \n",
      " batch_normalization_24 (BatchN  (None, 128)         512         ['dense_28[0][0]']               \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " conv5_block3_out (Add)         (None, 7, 7, 2048)   0           ['conv5_block2_out[0][0]',       \n",
      "                                                                  'conv5_block3_3_conv[0][0]']    \n",
      "                                                                                                  \n",
      " dropout_24 (Dropout)           (None, 128)          0           ['batch_normalization_24[0][0]'] \n",
      "                                                                                                  \n",
      " post_bn (BatchNormalization)   (None, 7, 7, 2048)   8192        ['conv5_block3_out[0][0]']       \n",
      "                                                                                                  \n",
      " dense_29 (Dense)               (None, 256)          33024       ['dropout_24[0][0]']             \n",
      "                                                                                                  \n",
      " post_relu (Activation)         (None, 7, 7, 2048)   0           ['post_bn[0][0]']                \n",
      "                                                                                                  \n",
      " batch_normalization_25 (BatchN  (None, 256)         1024        ['dense_29[0][0]']               \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " global_average_pooling2d_1 (Gl  (None, 2048)        0           ['post_relu[0][0]']              \n",
      " obalAveragePooling2D)                                                                            \n",
      "                                                                                                  \n",
      " dropout_25 (Dropout)           (None, 256)          0           ['batch_normalization_25[0][0]'] \n",
      "                                                                                                  \n",
      " dense_27 (Dense)               (None, 256)          524544      ['global_average_pooling2d_1[0][0\n",
      "                                                                 ]']                              \n",
      "                                                                                                  \n",
      " dense_30 (Dense)               (None, 128)          32896       ['dropout_25[0][0]']             \n",
      "                                                                                                  \n",
      " batch_normalization_23 (BatchN  (None, 256)         1024        ['dense_27[0][0]']               \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " batch_normalization_26 (BatchN  (None, 128)         512         ['dense_30[0][0]']               \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " dropout_23 (Dropout)           (None, 256)          0           ['batch_normalization_23[0][0]'] \n",
      "                                                                                                  \n",
      " dropout_26 (Dropout)           (None, 128)          0           ['batch_normalization_26[0][0]'] \n",
      "                                                                                                  \n",
      " concatenate_4 (Concatenate)    (None, 384)          0           ['dropout_23[0][0]',             \n",
      "                                                                  'dropout_26[0][0]']             \n",
      "                                                                                                  \n",
      " dense_31 (Dense)               (None, 256)          98560       ['concatenate_4[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_27 (BatchN  (None, 256)         1024        ['dense_31[0][0]']               \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " dropout_27 (Dropout)           (None, 256)          0           ['batch_normalization_27[0][0]'] \n",
      "                                                                                                  \n",
      " dense_32 (Dense)               (None, 128)          32896       ['dropout_27[0][0]']             \n",
      "                                                                                                  \n",
      " batch_normalization_28 (BatchN  (None, 128)         512         ['dense_32[0][0]']               \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " dropout_28 (Dropout)           (None, 128)          0           ['batch_normalization_28[0][0]'] \n",
      "                                                                                                  \n",
      " dense_33 (Dense)               (None, 1)            129         ['dropout_28[0][0]']             \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 24,295,553\n",
      "Trainable params: 728,449\n",
      "Non-trainable params: 23,567,104\n",
      "__________________________________________________________________________________________________\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "module 'pydot' has no attribute 'InvocationException'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "File \u001b[1;32m~\\.conda\\envs\\tf-gpu\\lib\\site-packages\\pydot\\core.py:1851\u001b[0m, in \u001b[0;36mDot.create\u001b[1;34m(self, prog, format, encoding)\u001b[0m\n\u001b[0;32m   1850\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1851\u001b[0m     stdout_data, stderr_data, process \u001b[38;5;241m=\u001b[39m \u001b[43mcall_graphviz\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1852\u001b[0m \u001b[43m        \u001b[49m\u001b[43mprogram\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprog\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1853\u001b[0m \u001b[43m        \u001b[49m\u001b[43marguments\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43marguments\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1854\u001b[0m \u001b[43m        \u001b[49m\u001b[43mworking_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtmp_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1855\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1856\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[1;32m~\\.conda\\envs\\tf-gpu\\lib\\site-packages\\pydot\\core.py:211\u001b[0m, in \u001b[0;36mcall_graphviz\u001b[1;34m(program, arguments, working_dir, **kwargs)\u001b[0m\n\u001b[0;32m    209\u001b[0m program_with_args \u001b[38;5;241m=\u001b[39m [program] \u001b[38;5;241m+\u001b[39m arguments\n\u001b[1;32m--> 211\u001b[0m process \u001b[38;5;241m=\u001b[39m subprocess\u001b[38;5;241m.\u001b[39mPopen(\n\u001b[0;32m    212\u001b[0m     program_with_args,\n\u001b[0;32m    213\u001b[0m     env\u001b[38;5;241m=\u001b[39menv,\n\u001b[0;32m    214\u001b[0m     cwd\u001b[38;5;241m=\u001b[39mworking_dir,\n\u001b[0;32m    215\u001b[0m     shell\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    216\u001b[0m     stderr\u001b[38;5;241m=\u001b[39msubprocess\u001b[38;5;241m.\u001b[39mPIPE,\n\u001b[0;32m    217\u001b[0m     stdout\u001b[38;5;241m=\u001b[39msubprocess\u001b[38;5;241m.\u001b[39mPIPE,\n\u001b[0;32m    218\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m    219\u001b[0m )\n\u001b[0;32m    220\u001b[0m stdout_data, stderr_data \u001b[38;5;241m=\u001b[39m process\u001b[38;5;241m.\u001b[39mcommunicate()\n",
      "File \u001b[1;32m~\\.conda\\envs\\tf-gpu\\lib\\subprocess.py:951\u001b[0m, in \u001b[0;36mPopen.__init__\u001b[1;34m(self, args, bufsize, executable, stdin, stdout, stderr, preexec_fn, close_fds, shell, cwd, env, universal_newlines, startupinfo, creationflags, restore_signals, start_new_session, pass_fds, user, group, extra_groups, encoding, errors, text, umask)\u001b[0m\n\u001b[0;32m    948\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstderr \u001b[38;5;241m=\u001b[39m io\u001b[38;5;241m.\u001b[39mTextIOWrapper(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstderr,\n\u001b[0;32m    949\u001b[0m                     encoding\u001b[38;5;241m=\u001b[39mencoding, errors\u001b[38;5;241m=\u001b[39merrors)\n\u001b[1;32m--> 951\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execute_child\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexecutable\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpreexec_fn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclose_fds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    952\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mpass_fds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcwd\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    953\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mstartupinfo\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreationflags\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshell\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    954\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mp2cread\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mp2cwrite\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    955\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mc2pread\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mc2pwrite\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    956\u001b[0m \u001b[43m                        \u001b[49m\u001b[43merrread\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merrwrite\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    957\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mrestore_signals\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    958\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mgid\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muid\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mumask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    959\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mstart_new_session\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    960\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m:\n\u001b[0;32m    961\u001b[0m     \u001b[38;5;66;03m# Cleanup if the child failed starting.\u001b[39;00m\n",
      "File \u001b[1;32m~\\.conda\\envs\\tf-gpu\\lib\\subprocess.py:1436\u001b[0m, in \u001b[0;36mPopen._execute_child\u001b[1;34m(self, args, executable, preexec_fn, close_fds, pass_fds, cwd, env, startupinfo, creationflags, shell, p2cread, p2cwrite, c2pread, c2pwrite, errread, errwrite, unused_restore_signals, unused_gid, unused_gids, unused_uid, unused_umask, unused_start_new_session)\u001b[0m\n\u001b[0;32m   1435\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1436\u001b[0m     hp, ht, pid, tid \u001b[38;5;241m=\u001b[39m \u001b[43m_winapi\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mCreateProcess\u001b[49m\u001b[43m(\u001b[49m\u001b[43mexecutable\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1437\u001b[0m \u001b[43m                             \u001b[49m\u001b[38;5;66;43;03m# no special security\u001b[39;49;00m\n\u001b[0;32m   1438\u001b[0m \u001b[43m                             \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m   1439\u001b[0m \u001b[43m                             \u001b[49m\u001b[38;5;28;43mint\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mclose_fds\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1440\u001b[0m \u001b[43m                             \u001b[49m\u001b[43mcreationflags\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1441\u001b[0m \u001b[43m                             \u001b[49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1442\u001b[0m \u001b[43m                             \u001b[49m\u001b[43mcwd\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1443\u001b[0m \u001b[43m                             \u001b[49m\u001b[43mstartupinfo\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1444\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m   1445\u001b[0m     \u001b[38;5;66;03m# Child is launched. Close the parent's copy of those pipe\u001b[39;00m\n\u001b[0;32m   1446\u001b[0m     \u001b[38;5;66;03m# handles that only the child should have open.  You need\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1449\u001b[0m     \u001b[38;5;66;03m# pipe will not close when the child process exits and the\u001b[39;00m\n\u001b[0;32m   1450\u001b[0m     \u001b[38;5;66;03m# ReadFile will hang.\u001b[39;00m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [WinError 2] The system cannot find the file specified",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "File \u001b[1;32m~\\.conda\\envs\\tf-gpu\\lib\\site-packages\\keras\\utils\\vis_utils.py:57\u001b[0m, in \u001b[0;36mcheck_graphviz\u001b[1;34m()\u001b[0m\n\u001b[0;32m     54\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     55\u001b[0m     \u001b[38;5;66;03m# Attempt to create an image of a blank graph\u001b[39;00m\n\u001b[0;32m     56\u001b[0m     \u001b[38;5;66;03m# to check the pydot/graphviz installation.\u001b[39;00m\n\u001b[1;32m---> 57\u001b[0m     \u001b[43mpydot\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mDot\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpydot\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mDot\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     58\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[1;32m~\\.conda\\envs\\tf-gpu\\lib\\site-packages\\pydot\\core.py:1860\u001b[0m, in \u001b[0;36mDot.create\u001b[1;34m(self, prog, format, encoding)\u001b[0m\n\u001b[0;32m   1859\u001b[0m     args[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mprog\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m not found in path.\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m-> 1860\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m(\u001b[38;5;241m*\u001b[39margs)\n\u001b[0;32m   1861\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [WinError 2] \"dot\" not found in path.",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 375\u001b[0m\n\u001b[0;32m    372\u001b[0m model\u001b[38;5;241m.\u001b[39msummary()\n\u001b[0;32m    374\u001b[0m \u001b[38;5;66;03m# Save model architecture as image\u001b[39;00m\n\u001b[1;32m--> 375\u001b[0m \u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkeras\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mutils\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mplot_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mto_file\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmodel_architecture.png\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshow_shapes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshow_layer_names\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m    376\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mModel architecture has been saved as \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmodel_architecture.png\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    378\u001b[0m \u001b[38;5;66;03m# Calculate and output additional metrics\u001b[39;00m\n\u001b[0;32m    379\u001b[0m \u001b[38;5;66;03m# ROC AUC\u001b[39;00m\n",
      "File \u001b[1;32m~\\.conda\\envs\\tf-gpu\\lib\\site-packages\\keras\\utils\\vis_utils.py:436\u001b[0m, in \u001b[0;36mplot_model\u001b[1;34m(model, to_file, show_shapes, show_dtype, show_layer_names, rankdir, expand_nested, dpi, layer_range, show_layer_activations)\u001b[0m\n\u001b[0;32m    429\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m model\u001b[38;5;241m.\u001b[39mbuilt:\n\u001b[0;32m    430\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    431\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThis model has not yet been built. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    432\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBuild the model first by calling `build()` or by calling \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    433\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mthe model on a batch of data.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    434\u001b[0m     )\n\u001b[1;32m--> 436\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[43mcheck_graphviz\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[0;32m    437\u001b[0m     message \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    438\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou must install pydot (`pip install pydot`) \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    439\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mand install graphviz \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    440\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m(see instructions at https://graphviz.gitlab.io/download/) \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    441\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfor plot_model to work.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    442\u001b[0m     )\n\u001b[0;32m    443\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIPython.core.magics.namespace\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m sys\u001b[38;5;241m.\u001b[39mmodules:\n\u001b[0;32m    444\u001b[0m         \u001b[38;5;66;03m# We don't raise an exception here in order to avoid crashing\u001b[39;00m\n\u001b[0;32m    445\u001b[0m         \u001b[38;5;66;03m# notebook tests where graphviz is not available.\u001b[39;00m\n",
      "File \u001b[1;32m~\\.conda\\envs\\tf-gpu\\lib\\site-packages\\keras\\utils\\vis_utils.py:59\u001b[0m, in \u001b[0;36mcheck_graphviz\u001b[1;34m()\u001b[0m\n\u001b[0;32m     57\u001b[0m     pydot\u001b[38;5;241m.\u001b[39mDot\u001b[38;5;241m.\u001b[39mcreate(pydot\u001b[38;5;241m.\u001b[39mDot())\n\u001b[0;32m     58\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m---> 59\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mOSError\u001b[39;00m, \u001b[43mpydot\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mInvocationException\u001b[49m):\n\u001b[0;32m     60\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "\u001b[1;31mAttributeError\u001b[0m: module 'pydot' has no attribute 'InvocationException'"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models, Input, mixed_precision\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import confusion_matrix, classification_report, f1_score, roc_auc_score, precision_recall_curve, average_precision_score, accuracy_score, roc_curve\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Load the DataFrame\n",
    "file_path = r\"C:\\Users\\meh91075\\Downloads\\merged_features_filtered.csv\"\n",
    "df = pd.read_csv(file_path)\n",
    "print(f\"Original CSV data loaded. Number of rows: {len(df)}\")\n",
    "print(f\"Original distribution of labels:\")\n",
    "print(df['label_majority'].value_counts())\n",
    "\n",
    "# Optimize GPU configuration\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    try:\n",
    "        for gpu in gpus:\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)\n",
    "        logical_gpus = tf.config.experimental.list_logical_devices('GPU')\n",
    "        print(f\"{len(gpus)} Physical GPUs, {len(logical_gpus)} Logical GPUs\")\n",
    "    except RuntimeError as e:\n",
    "        print(e)\n",
    "\n",
    "# Activate Mixed Precision\n",
    "if tf.test.is_built_with_cuda():\n",
    "    policy = mixed_precision.Policy('mixed_float16')\n",
    "    mixed_precision.set_global_policy(policy)\n",
    "    print(\"Mixed precision policy set to mixed_float16\")\n",
    "else:\n",
    "    print(\"CUDA not available, using default precision\")\n",
    "\n",
    "# Paths and Configuration\n",
    "base_dir = \"C:/Users/meh91075/Desktop/02_AutoSplit/02_AutoSplit/10_Datasets/01_Autosplit_dataset\"\n",
    "base_dir1 = \"C:/Users/meh91075/Downloads/04_Standardparts\"\n",
    "filtered_dir = os.path.join(base_dir1, 'png')\n",
    "non_filtered_dir = os.path.join(base_dir, 'New_non_filtered_data')\n",
    "\n",
    "img_height, img_width = 224, 224\n",
    "batch_size = 32\n",
    "\n",
    "def get_image_paths(directory):\n",
    "    return [os.path.join(root, file) for root, _, files in os.walk(directory) \n",
    "            for file in files if file.lower().endswith(('.png', '.jpg', '.jpeg'))]\n",
    "\n",
    "filtered_images = get_image_paths(filtered_dir)\n",
    "non_filtered_images = get_image_paths(non_filtered_dir)\n",
    "print(f\"Found images: Filtered: {len(filtered_images)}, Not filtered: {len(non_filtered_images)}\")\n",
    "\n",
    "def extract_filename(path):\n",
    "    basename = os.path.basename(path)\n",
    "    return basename.replace('_ISO', '').replace('_ISOAlt', '').split('.')[0]\n",
    "\n",
    "image_df = pd.DataFrame({\n",
    "    'image_path': filtered_images + non_filtered_images,\n",
    "    'label': [1] * len(filtered_images) + [0] * len(non_filtered_images),\n",
    "    'filename': [extract_filename(path) for path in filtered_images + non_filtered_images]\n",
    "})\n",
    "\n",
    "print(\"Sample of extracted filenames:\")\n",
    "print(image_df['filename'].head())\n",
    "\n",
    "# Adjusted merge process\n",
    "merged_df = pd.merge(df, image_df[['filename', 'image_path']], on='filename', how='left')\n",
    "merged_df['label'] = merged_df['label_majority'].map({1: 0, 2: 0, 3: 0, 4: 1})\n",
    "merged_df['has_image'] = merged_df['image_path'].notna()\n",
    "\n",
    "# Filter only entries with associated images\n",
    "merged_df_with_images = merged_df[merged_df['has_image']]\n",
    "\n",
    "# Undersampling of the majority class\n",
    "majority_class = merged_df_with_images[merged_df_with_images['label'] == 0]\n",
    "minority_class = merged_df_with_images[merged_df_with_images['label'] == 1]\n",
    "\n",
    "majority_downsampled = majority_class.sample(n=2000, random_state=42)\n",
    "balanced_df = pd.concat([majority_downsampled, minority_class])\n",
    "\n",
    "print(f\"After undersampling: Number of rows: {len(balanced_df)}\")\n",
    "print(\"Distribution of labels after undersampling:\")\n",
    "print(balanced_df['label'].value_counts())\n",
    "\n",
    "# Train-Test-Split\n",
    "train_df, val_df = train_test_split(balanced_df, test_size=0.2, stratify=balanced_df['label'], random_state=42)\n",
    "print(f\"Training data: {len(train_df)}, Validation data: {len(val_df)}\")\n",
    "\n",
    "# Determine the numeric columns\n",
    "numeric_columns = df.select_dtypes(include=[np.number]).columns.tolist()\n",
    "numeric_columns = [col for col in numeric_columns if col not in ['label_majority', 'label']]\n",
    "\n",
    "# Normalization of numeric data\n",
    "scaler = StandardScaler()\n",
    "train_df[numeric_columns] = scaler.fit_transform(train_df[numeric_columns])\n",
    "val_df[numeric_columns] = scaler.transform(val_df[numeric_columns])\n",
    "\n",
    "# Optimized data generator function with data augmentation\n",
    "def create_dataset(dataframe, is_training=True):\n",
    "    def parse_function(filename, label, *numeric_data):\n",
    "        img = tf.io.read_file(filename)\n",
    "        img = tf.image.decode_jpeg(img, channels=3)\n",
    "        img = tf.image.resize(img, [img_height, img_width])\n",
    "        img = tf.keras.applications.resnet_v2.preprocess_input(img)\n",
    "        \n",
    "        if is_training:\n",
    "            img = tf.image.random_flip_left_right(img)\n",
    "            img = tf.image.random_brightness(img, max_delta=0.2)\n",
    "            img = tf.image.random_contrast(img, lower=0.8, upper=1.2)\n",
    "            img = tf.image.random_saturation(img, lower=0.8, upper=1.2)\n",
    "            img = tf.image.random_hue(img, max_delta=0.1)\n",
    "        \n",
    "        numeric_data = tf.convert_to_tensor(numeric_data, dtype=tf.float32)\n",
    "        numeric_data = tf.squeeze(numeric_data)\n",
    "        return (img, numeric_data), label\n",
    "\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((\n",
    "        dataframe['image_path'].values,\n",
    "        dataframe['label'].values,\n",
    "        dataframe[numeric_columns].values\n",
    "    ))\n",
    "    \n",
    "    dataset = dataset.map(parse_function, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "    \n",
    "    if is_training:\n",
    "        dataset = dataset.shuffle(buffer_size=len(dataframe)).repeat()\n",
    "    else:\n",
    "        dataset = dataset.repeat()\n",
    "    \n",
    "    dataset = dataset.batch(batch_size)\n",
    "    dataset = dataset.prefetch(tf.data.AUTOTUNE)\n",
    "    return dataset\n",
    "\n",
    "# Create the datasets\n",
    "train_dataset = create_dataset(train_df)\n",
    "val_dataset = create_dataset(val_df, is_training=False)\n",
    "\n",
    "# Calculate steps per epoch\n",
    "steps_per_epoch = len(train_df) // batch_size\n",
    "validation_steps = len(val_df) // batch_size\n",
    "\n",
    "# Optimized model definition\n",
    "def create_optimized_hybrid_model(img_height, img_width, num_numeric_features):\n",
    "    # Image processing branch (CNN)\n",
    "    img_input = Input(shape=(img_height, img_width, 3))\n",
    "    \n",
    "    # Use a pre-trained model for feature extraction\n",
    "    base_model = tf.keras.applications.ResNet50V2(include_top=False, weights='imagenet', input_tensor=img_input)\n",
    "    \n",
    "    # Freeze the pre-trained layers\n",
    "    base_model.trainable = False\n",
    "    \n",
    "    x = base_model.output\n",
    "    x = layers.GlobalAveragePooling2D()(x)\n",
    "    x = layers.Dense(256, activation='relu')(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.Dropout(0.5)(x)\n",
    "    \n",
    "    # Numerical data branch (MLP)\n",
    "    num_input = Input(shape=(num_numeric_features,))\n",
    "    y = layers.Dense(128, activation='relu')(num_input)\n",
    "    y = layers.BatchNormalization()(y)\n",
    "    y = layers.Dropout(0.3)(y)\n",
    "    y = layers.Dense(256, activation='relu')(y)\n",
    "    y = layers.BatchNormalization()(y)\n",
    "    y = layers.Dropout(0.4)(y)\n",
    "    y = layers.Dense(128, activation='relu')(y)\n",
    "    y = layers.BatchNormalization()(y)\n",
    "    y = layers.Dropout(0.3)(y)\n",
    "    \n",
    "    # Combining the branches\n",
    "    combined = layers.concatenate([x, y])\n",
    "    \n",
    "    # Final layers\n",
    "    z = layers.Dense(256, activation='relu')(combined)\n",
    "    z = layers.BatchNormalization()(z)\n",
    "    z = layers.Dropout(0.5)(z)\n",
    "    z = layers.Dense(128, activation='relu')(z)\n",
    "    z = layers.BatchNormalization()(z)\n",
    "    z = layers.Dropout(0.5)(z)\n",
    "    \n",
    "    # Output layer\n",
    "    output = layers.Dense(1, activation='sigmoid')(z)\n",
    "    \n",
    "    model = models.Model(inputs=[img_input, num_input], outputs=output)\n",
    "    return model\n",
    "\n",
    "# Learning Rate Logger Callback\n",
    "class LearningRateLogger(tf.keras.callbacks.Callback):\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        if logs is not None:\n",
    "            lr = self.model.optimizer.learning_rate\n",
    "            if isinstance(lr, tf.keras.optimizers.schedules.LearningRateSchedule):\n",
    "                lr = lr(self.model.optimizer.iterations)\n",
    "            logs['lr'] = tf.keras.backend.get_value(lr)\n",
    "\n",
    "# Custom ReduceLROnPlateau Callback\n",
    "class CustomReduceLROnPlateau(tf.keras.callbacks.Callback):\n",
    "    def __init__(self, factor=0.1, patience=10, min_lr=0, monitor='val_loss', mode='min'):\n",
    "        super(CustomReduceLROnPlateau, self).__init__()\n",
    "        self.factor = factor\n",
    "        self.patience = patience\n",
    "        self.min_lr = min_lr\n",
    "        self.monitor = monitor\n",
    "        self.mode = mode\n",
    "        self.best = np.Inf if mode == 'min' else -np.Inf\n",
    "        self.wait = 0\n",
    "        self.best_weights = None\n",
    "\n",
    "    def on_train_begin(self, logs=None):\n",
    "        self.best = np.Inf if self.mode == 'min' else -np.Inf\n",
    "        self.wait = 0\n",
    "        self.best_weights = None\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        logs = logs or {}\n",
    "        current = logs.get(self.monitor)\n",
    "        if current is None:\n",
    "            return\n",
    "        \n",
    "        if self.mode == 'min' and current < self.best:\n",
    "            self.best = current\n",
    "            self.wait = 0\n",
    "            self.best_weights = self.model.get_weights()\n",
    "        elif self.mode == 'max' and current > self.best:\n",
    "            self.best = current\n",
    "            self.wait = 0\n",
    "            self.best_weights = self.model.get_weights()\n",
    "        else:\n",
    "            self.wait += 1\n",
    "        \n",
    "        if self.wait >= self.patience:\n",
    "            current_lr = self.model.optimizer.learning_rate\n",
    "            if isinstance(current_lr, tf.keras.optimizers.schedules.LearningRateSchedule):\n",
    "                current_lr = current_lr(self.model.optimizer.iterations)\n",
    "            current_lr = float(current_lr.numpy())\n",
    "            \n",
    "            if current_lr > self.min_lr:\n",
    "                new_lr = max(current_lr * self.factor, self.min_lr)\n",
    "                new_lr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(\n",
    "                    initial_learning_rate=new_lr,\n",
    "                    decay_steps=1000,\n",
    "                    decay_rate=0.9,\n",
    "                    staircase=True\n",
    "                )\n",
    "                self.model.optimizer.learning_rate = new_lr_schedule\n",
    "                print(f'\\nEpoch {epoch+1}: ReduceLROnPlateau reducing learning rate to {new_lr}.')\n",
    "                self.wait = 0\n",
    "                self.model.set_weights(self.best_weights)\n",
    "\n",
    "# Compile and train\n",
    "initial_learning_rate = 1e-3\n",
    "lr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(\n",
    "    initial_learning_rate, decay_steps=1000, decay_rate=0.9, staircase=True\n",
    ")\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=lr_schedule)\n",
    "\n",
    "# Multi-GPU Strategy\n",
    "strategy = tf.distribute.MirroredStrategy()\n",
    "with strategy.scope():\n",
    "    model = create_optimized_hybrid_model(img_height, img_width, len(numeric_columns))\n",
    "    model.compile(\n",
    "        optimizer=optimizer,\n",
    "        loss='binary_crossentropy',\n",
    "        metrics=['accuracy', tf.keras.metrics.AUC(name='auc'), tf.keras.metrics.Precision(), tf.keras.metrics.Recall()]\n",
    "    )\n",
    "\n",
    "# Callbacks\n",
    "reduce_lr = CustomReduceLROnPlateau(monitor='val_auc', factor=0.2, patience=5, min_lr=1e-6, mode='max')\n",
    "model_checkpoint = tf.keras.callbacks.ModelCheckpoint('best_model.h5', save_best_only=True, monitor='val_auc', mode='max')\n",
    "lr_logger = LearningRateLogger()\n",
    "\n",
    "# Training\n",
    "history = model.fit(\n",
    "    train_dataset,\n",
    "    epochs=100,\n",
    "    steps_per_epoch=steps_per_epoch,\n",
    "    validation_data=val_dataset,\n",
    "    validation_steps=validation_steps,\n",
    "    callbacks=[reduce_lr, model_checkpoint, lr_logger]\n",
    ")\n",
    "\n",
    "# Save model\n",
    "model.save('hybrid_model_optimized_final_fake.h5')\n",
    "\n",
    "# Evaluation\n",
    "val_loss, val_accuracy, val_auc, val_precision, val_recall = model.evaluate(val_dataset, steps=validation_steps)\n",
    "print(f\"Validation Loss: {val_loss:.4f}\")\n",
    "print(f\"Validation Accuracy: {val_accuracy:.4f}\")\n",
    "print(f\"Validation AUC: {val_auc:.4f}\")\n",
    "print(f\"Validation Precision: {val_precision:.4f}\")\n",
    "print(f\"Validation Recall: {val_recall:.4f}\")\n",
    "\n",
    "# Predictions for the validation dataset\n",
    "y_pred = model.predict(val_dataset, steps=validation_steps)\n",
    "y_pred_classes = (y_pred > 0.5).astype(\"int32\")\n",
    "y_true = np.concatenate([y for _, y in val_dataset.take(validation_steps)], axis=0)\n",
    "\n",
    "# Confusion matrix and classification report\n",
    "conf_matrix = confusion_matrix(y_true, y_pred_classes)\n",
    "class_report = classification_report(y_true, y_pred_classes)\n",
    "\n",
    "print(\"Confusion Matrix:\")\n",
    "print(conf_matrix)\n",
    "print(\"\\nClassification Report:\")\n",
    "print(class_report)\n",
    "\n",
    "# Visualization of training history\n",
    "plt.figure(figsize=(15, 5))\n",
    "plt.subplot(1, 3, 1)\n",
    "plt.plot(history.history['accuracy'], label='Training Accuracy')\n",
    "plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
    "plt.title('Model Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(1, 3, 2)\n",
    "plt.plot(history.history['loss'], label='Training Loss')\n",
    "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "plt.title('Model Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(1, 3, 3)\n",
    "plt.plot(history.history['auc'], label='Training AUC')\n",
    "plt.plot(history.history['val_auc'], label='Validation AUC')\n",
    "plt.title('Model AUC')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('AUC')\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('training_history.png')\n",
    "plt.close()\n",
    "\n",
    "# Visualization of learning rate\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(history.history['lr'])\n",
    "plt.title('Learning Rate over Epochs')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Learning Rate')\n",
    "plt.yscale('log')\n",
    "plt.savefig('learning_rate.png')\n",
    "plt.close()\n",
    "\n",
    "# Visualization of confusion matrix\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.xlabel('Predicted Label')\n",
    "plt.ylabel('True Label')\n",
    "plt.savefig('confusion_matrix.png')\n",
    "plt.close()\n",
    "\n",
    "print(\"Visualizations have been saved as PNG files.\")\n",
    "\n",
    "# Save predictions and true labels\n",
    "predictions_df = pd.DataFrame({\n",
    "    'True_Label': y_true,\n",
    "    'Predicted_Probability': y_pred.flatten(),\n",
    "    'Predicted_Class': y_pred_classes.flatten()\n",
    "})\n",
    "predictions_df.to_csv('predictions.csv', index=False)\n",
    "print(\"Predictions have been saved in 'predictions.csv'.\")\n",
    "\n",
    "# Model summary\n",
    "model.summary()\n",
    "\n",
    "# Save model architecture as image\n",
    "tf.keras.utils.plot_model(model, to_file='model_architecture.png', show_shapes=True, show_layer_names=True)\n",
    "print(\"Model architecture has been saved as 'model_architecture.png'.\")\n",
    "\n",
    "# Calculate and output additional metrics\n",
    "# ROC AUC\n",
    "roc_auc = roc_auc_score(y_true, y_pred)\n",
    "print(f\"ROC AUC: {roc_auc:.4f}\")\n",
    "\n",
    "# Precision-Recall AUC\n",
    "pr_auc = average_precision_score(y_true, y_pred)\n",
    "print(f\"Precision-Recall AUC: {pr_auc:.4f}\")\n",
    "\n",
    "# F1-Score at various thresholds\n",
    "thresholds = np.arange(0, 1.01, 0.01)\n",
    "f1_scores = [f1_score(y_true, y_pred > threshold) for threshold in thresholds]\n",
    "best_threshold = thresholds[np.argmax(f1_scores)]\n",
    "best_f1 = max(f1_scores)\n",
    "print(f\"Best F1-Score: {best_f1:.4f} at threshold {best_threshold:.2f}\")\n",
    "\n",
    "# Visualization of Precision-Recall curve\n",
    "precision, recall, _ = precision_recall_curve(y_true, y_pred)\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(recall, precision, marker='.')\n",
    "plt.title('Precision-Recall Curve')\n",
    "plt.xlabel('Recall')\n",
    "plt.ylabel('Precision')\n",
    "plt.savefig('precision_recall_curve.png')\n",
    "plt.close()\n",
    "print(\"Precision-Recall curve has been saved as 'precision_recall_curve.png'.\")\n",
    "\n",
    "# Visualization of ROC curve\n",
    "fpr, tpr, _ = roc_curve(y_true, y_pred)\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(fpr, tpr, label=f'ROC Curve (AUC = {roc_auc:.2f})')\n",
    "plt.plot([0, 1], [0, 1], linestyle='--', label='Random Classifier')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver Operating Characteristic (ROC) Curve')\n",
    "plt.legend()\n",
    "plt.savefig('roc_curve.png')\n",
    "plt.close()\n",
    "print(\"ROC curve has been saved as 'roc_curve.png'.\")\n",
    "\n",
    "# Save training history\n",
    "with open('training_history.txt', 'w') as f:\n",
    "    for key, values in history.history.items():\n",
    "        f.write(f\"{key}: {values}\\n\")\n",
    "print(\"Training history has been saved in 'training_history.txt'.\")\n",
    "\n",
    "# Analyze misclassified examples\n",
    "misclassified = np.where(y_pred_classes.flatten() != y_true)[0]\n",
    "print(f\"\\nNumber of misclassified examples: {len(misclassified)}\")\n",
    "\n",
    "if len(misclassified) > 0:\n",
    "    print(\"\\nDetails of some misclassified examples:\")\n",
    "    for i in misclassified[:5]:  # Show details for the first 5 misclassified examples\n",
    "        true_label = y_true[i]\n",
    "        pred_label = y_pred_classes[i][0]\n",
    "        pred_prob = y_pred[i][0]\n",
    "        print(f\"Example {i}: True Label: {true_label}, Predicted Label: {pred_label}, Predicted Probability: {pred_prob:.4f}\")\n",
    "\n",
    "# Model performance across different thresholds\n",
    "accuracies = [accuracy_score(y_true, y_pred > threshold) for threshold in thresholds]\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(thresholds, accuracies, label='Accuracy')\n",
    "plt.plot(thresholds, f1_scores, label='F1-Score')\n",
    "plt.axvline(x=0.5, color='r', linestyle='--', label='Default Threshold (0.5)')\n",
    "plt.xlabel('Threshold')\n",
    "plt.ylabel('Score')\n",
    "plt.title('Model Performance vs. Classification Threshold')\n",
    "plt.legend()\n",
    "plt.savefig('performance_vs_threshold.png')\n",
    "plt.close()\n",
    "print(\"Performance vs. Threshold plot has been saved as 'performance_vs_threshold.png'.\")\n",
    "\n",
    "# Optional: SHAP Analysis\n",
    "try:\n",
    "    import shap\n",
    "    \n",
    "    # Select a subset of validation data for SHAP analysis\n",
    "    num_explain = 100\n",
    "    explain_dataset = val_dataset.take(num_explain).unbatch()\n",
    "    background_dataset = train_dataset.take(num_explain).unbatch()\n",
    "    \n",
    "    # Create an Explainer\n",
    "    explainer = shap.DeepExplainer(model, background_dataset)\n",
    "    \n",
    "    # Calculate SHAP values\n",
    "    shap_values = explainer.shap_values(explain_dataset)\n",
    "    \n",
    "    # Visualize SHAP values\n",
    "    shap.summary_plot(shap_values[0], plot_type=\"bar\", feature_names=numeric_columns, show=False)\n",
    "    plt.savefig('shap_summary.png')\n",
    "    plt.close()\n",
    "    print(\"SHAP summary has been saved as 'shap_summary.png'.\")\n",
    "except ImportError:\n",
    "    print(\"SHAP is not installed. Skipping SHAP analysis.\")\n",
    "\n",
    "print(\"\\nAll analyses and visualizations are complete.\")\n",
    "\n",
    "# Summary of key metrics\n",
    "print(\"\\nSummary of key metrics:\")\n",
    "print(f\"Validation Accuracy: {val_accuracy:.4f}\")\n",
    "print(f\"Validation AUC: {val_auc:.4f}\")\n",
    "print(f\"Validation Precision: {val_precision:.4f}\")\n",
    "print(f\"Validation Recall: {val_recall:.4f}\")\n",
    "print(f\"ROC AUC: {roc_auc:.4f}\")\n",
    "print(f\"Precision-Recall AUC: {pr_auc:.4f}\")\n",
    "print(f\"Best F1-Score: {best_f1:.4f} at threshold {best_threshold:.2f}\")\n",
    "\n",
    "# Save summary to a text file\n",
    "with open('model_summary.txt', 'w') as f:\n",
    "    f.write(\"Summary of key metrics:\\n\")\n",
    "    f.write(f\"Validation Accuracy: {val_accuracy:.4f}\\n\")\n",
    "    f.write(f\"Validation AUC: {val_auc:.4f}\\n\")\n",
    "    f.write(f\"Validation Precision: {val_precision:.4f}\\n\")\n",
    "    f.write(f\"Validation Recall: {val_recall:.4f}\\n\")\n",
    "    f.write(f\"ROC AUC: {roc_auc:.4f}\\n\")\n",
    "    f.write(f\"Precision-Recall AUC: {pr_auc:.4f}\\n\")\n",
    "    f.write(f\"Best F1-Score: {best_f1:.4f} at threshold {best_threshold:.2f}\\n\")\n",
    "\n",
    "print(\"\\nA summary of key metrics has been saved in 'model_summary.txt'.\")\n",
    "print(\"\\nThe script has been executed successfully. All results and visualizations have been saved.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98e708c2-b43a-4422-bb93-e91235b5355f",
   "metadata": {},
   "source": [
    "# hybrid_model_optimized_final.h5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4fdbce6f-b5e4-488c-baf6-22002f44ec8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original CSV data loaded. Number of rows: 19274\n",
      "Original distribution of labels:\n",
      "label_majority\n",
      "2    10850\n",
      "1     3485\n",
      "4     2745\n",
      "3     2194\n",
      "Name: count, dtype: int64\n",
      "1 Physical GPUs, 1 Logical GPUs\n",
      "Mixed precision policy set to mixed_float16\n",
      "Found images: Filtered: 5664, Not filtered: 27994\n",
      "Sample of extracted filenames:\n",
      "0       00042065\n",
      "1    00042065Alt\n",
      "2       00043749\n",
      "3    00043749Alt\n",
      "4       00080967\n",
      "Name: filename, dtype: object\n",
      "After undersampling: Number of rows: 4499\n",
      "Distribution of labels after undersampling:\n",
      "label\n",
      "1    2499\n",
      "0    2000\n",
      "Name: count, dtype: int64\n",
      "Training data: 3599, Validation data: 900\n",
      "INFO:tensorflow:Using MirroredStrategy with devices ('/job:localhost/replica:0/task:0/device:GPU:0',)\n",
      "Epoch 1/100\n",
      "112/112 [==============================] - 28s 132ms/step - loss: 0.6059 - accuracy: 0.7266 - auc: 0.8002 - precision_2: 0.7569 - recall_2: 0.7477 - val_loss: 0.4200 - val_accuracy: 0.8304 - val_auc: 0.8883 - val_precision_2: 0.8046 - val_recall_2: 0.9177 - lr: 0.0010\n",
      "Epoch 2/100\n",
      "112/112 [==============================] - 12s 111ms/step - loss: 0.4671 - accuracy: 0.7949 - auc: 0.8723 - precision_2: 0.8057 - recall_2: 0.8304 - val_loss: 0.4019 - val_accuracy: 0.8214 - val_auc: 0.9098 - val_precision_2: 0.7884 - val_recall_2: 0.9277 - lr: 0.0010\n",
      "Epoch 3/100\n",
      "112/112 [==============================] - 12s 104ms/step - loss: 0.4126 - accuracy: 0.8220 - auc: 0.8966 - precision_2: 0.8265 - recall_2: 0.8613 - val_loss: 0.3985 - val_accuracy: 0.8337 - val_auc: 0.9075 - val_precision_2: 0.7913 - val_recall_2: 0.9518 - lr: 0.0010\n",
      "Epoch 4/100\n",
      "112/112 [==============================] - 12s 111ms/step - loss: 0.3562 - accuracy: 0.8415 - auc: 0.9193 - precision_2: 0.8393 - recall_2: 0.8840 - val_loss: 0.3644 - val_accuracy: 0.8438 - val_auc: 0.9174 - val_precision_2: 0.8108 - val_recall_2: 0.9378 - lr: 0.0010\n",
      "Epoch 5/100\n",
      "112/112 [==============================] - 12s 111ms/step - loss: 0.3402 - accuracy: 0.8549 - auc: 0.9272 - precision_2: 0.8569 - recall_2: 0.8855 - val_loss: 0.3440 - val_accuracy: 0.8504 - val_auc: 0.9269 - val_precision_2: 0.8227 - val_recall_2: 0.9317 - lr: 0.0010\n",
      "Epoch 6/100\n",
      "112/112 [==============================] - 12s 104ms/step - loss: 0.3303 - accuracy: 0.8555 - auc: 0.9300 - precision_2: 0.8564 - recall_2: 0.8914 - val_loss: 0.3632 - val_accuracy: 0.8359 - val_auc: 0.9254 - val_precision_2: 0.7940 - val_recall_2: 0.9518 - lr: 0.0010\n",
      "Epoch 7/100\n",
      "112/112 [==============================] - 12s 111ms/step - loss: 0.3106 - accuracy: 0.8705 - auc: 0.9378 - precision_2: 0.8680 - recall_2: 0.9031 - val_loss: 0.3462 - val_accuracy: 0.8449 - val_auc: 0.9317 - val_precision_2: 0.8090 - val_recall_2: 0.9438 - lr: 0.0010\n",
      "Epoch 8/100\n",
      "112/112 [==============================] - 12s 104ms/step - loss: 0.2971 - accuracy: 0.8797 - auc: 0.9431 - precision_2: 0.8707 - recall_2: 0.9186 - val_loss: 0.3465 - val_accuracy: 0.8549 - val_auc: 0.9308 - val_precision_2: 0.8183 - val_recall_2: 0.9498 - lr: 0.0010\n",
      "Epoch 9/100\n",
      "112/112 [==============================] - 12s 111ms/step - loss: 0.3017 - accuracy: 0.8711 - auc: 0.9413 - precision_2: 0.8731 - recall_2: 0.9001 - val_loss: 0.3622 - val_accuracy: 0.8415 - val_auc: 0.9348 - val_precision_2: 0.7957 - val_recall_2: 0.9618 - lr: 9.0000e-04\n",
      "Epoch 10/100\n",
      "112/112 [==============================] - 12s 104ms/step - loss: 0.2873 - accuracy: 0.8789 - auc: 0.9465 - precision_2: 0.8750 - recall_2: 0.9135 - val_loss: 0.3512 - val_accuracy: 0.8605 - val_auc: 0.9271 - val_precision_2: 0.8266 - val_recall_2: 0.9478 - lr: 9.0000e-04\n",
      "Epoch 11/100\n",
      "112/112 [==============================] - 12s 111ms/step - loss: 0.2692 - accuracy: 0.8915 - auc: 0.9539 - precision_2: 0.8846 - recall_2: 0.9235 - val_loss: 0.3377 - val_accuracy: 0.8538 - val_auc: 0.9408 - val_precision_2: 0.8180 - val_recall_2: 0.9478 - lr: 9.0000e-04\n",
      "Epoch 12/100\n",
      "112/112 [==============================] - 12s 104ms/step - loss: 0.2799 - accuracy: 0.8797 - auc: 0.9490 - precision_2: 0.8752 - recall_2: 0.9150 - val_loss: 0.3130 - val_accuracy: 0.8728 - val_auc: 0.9394 - val_precision_2: 0.8569 - val_recall_2: 0.9257 - lr: 9.0000e-04\n",
      "Epoch 13/100\n",
      "112/112 [==============================] - 12s 104ms/step - loss: 0.2668 - accuracy: 0.8909 - auc: 0.9540 - precision_2: 0.8892 - recall_2: 0.9190 - val_loss: 0.3257 - val_accuracy: 0.8650 - val_auc: 0.9332 - val_precision_2: 0.8446 - val_recall_2: 0.9277 - lr: 9.0000e-04\n",
      "Epoch 14/100\n",
      "112/112 [==============================] - 12s 104ms/step - loss: 0.2619 - accuracy: 0.8959 - auc: 0.9559 - precision_2: 0.8956 - recall_2: 0.9187 - val_loss: 0.3199 - val_accuracy: 0.8672 - val_auc: 0.9397 - val_precision_2: 0.8296 - val_recall_2: 0.9578 - lr: 9.0000e-04\n",
      "Epoch 15/100\n",
      "112/112 [==============================] - 12s 112ms/step - loss: 0.2559 - accuracy: 0.8923 - auc: 0.9579 - precision_2: 0.8902 - recall_2: 0.9189 - val_loss: 0.3139 - val_accuracy: 0.8638 - val_auc: 0.9433 - val_precision_2: 0.8381 - val_recall_2: 0.9357 - lr: 9.0000e-04\n",
      "Epoch 16/100\n",
      "112/112 [==============================] - 12s 111ms/step - loss: 0.2500 - accuracy: 0.9007 - auc: 0.9591 - precision_2: 0.8965 - recall_2: 0.9296 - val_loss: 0.2955 - val_accuracy: 0.8772 - val_auc: 0.9469 - val_precision_2: 0.8688 - val_recall_2: 0.9177 - lr: 9.0000e-04\n",
      "Epoch 17/100\n",
      "112/112 [==============================] - 12s 104ms/step - loss: 0.2426 - accuracy: 0.9029 - auc: 0.9616 - precision_2: 0.9036 - recall_2: 0.9236 - val_loss: 0.3060 - val_accuracy: 0.8739 - val_auc: 0.9427 - val_precision_2: 0.8506 - val_recall_2: 0.9378 - lr: 9.0000e-04\n",
      "Epoch 18/100\n",
      "112/112 [==============================] - 12s 104ms/step - loss: 0.2523 - accuracy: 0.8956 - auc: 0.9590 - precision_2: 0.8959 - recall_2: 0.9184 - val_loss: 0.3255 - val_accuracy: 0.8672 - val_auc: 0.9459 - val_precision_2: 0.8296 - val_recall_2: 0.9578 - lr: 8.1000e-04\n",
      "Epoch 19/100\n",
      "112/112 [==============================] - 12s 104ms/step - loss: 0.2496 - accuracy: 0.9015 - auc: 0.9595 - precision_2: 0.8949 - recall_2: 0.9322 - val_loss: 0.3097 - val_accuracy: 0.8761 - val_auc: 0.9394 - val_precision_2: 0.8672 - val_recall_2: 0.9177 - lr: 8.1000e-04\n",
      "Epoch 20/100\n",
      "112/112 [==============================] - 12s 104ms/step - loss: 0.2368 - accuracy: 0.9040 - auc: 0.9639 - precision_2: 0.9032 - recall_2: 0.9251 - val_loss: 0.3294 - val_accuracy: 0.8571 - val_auc: 0.9453 - val_precision_2: 0.8269 - val_recall_2: 0.9398 - lr: 8.1000e-04\n",
      "Epoch 21/100\n",
      "112/112 [==============================] - ETA: 0s - loss: 0.2337 - accuracy: 0.9009 - auc: 0.9646 - precision_2: 0.8976 - recall_2: 0.9293\n",
      "Epoch 21: ReduceLROnPlateau reducing learning rate to 0.0001619999995455146.\n",
      "112/112 [==============================] - 12s 106ms/step - loss: 0.2337 - accuracy: 0.9009 - auc: 0.9646 - precision_2: 0.8976 - recall_2: 0.9293 - val_loss: 0.3122 - val_accuracy: 0.8717 - val_auc: 0.9425 - val_precision_2: 0.8662 - val_recall_2: 0.9096 - lr: 1.3122e-04\n",
      "Epoch 22/100\n",
      "112/112 [==============================] - 12s 104ms/step - loss: 0.2437 - accuracy: 0.9012 - auc: 0.9615 - precision_2: 0.8965 - recall_2: 0.9282 - val_loss: 0.3234 - val_accuracy: 0.8728 - val_auc: 0.9348 - val_precision_2: 0.8650 - val_recall_2: 0.9137 - lr: 1.3122e-04\n",
      "Epoch 23/100\n",
      "112/112 [==============================] - 12s 104ms/step - loss: 0.2471 - accuracy: 0.8920 - auc: 0.9601 - precision_2: 0.8827 - recall_2: 0.9285 - val_loss: 0.3166 - val_accuracy: 0.8705 - val_auc: 0.9406 - val_precision_2: 0.8498 - val_recall_2: 0.9317 - lr: 1.3122e-04\n",
      "Epoch 24/100\n",
      "112/112 [==============================] - 12s 104ms/step - loss: 0.2413 - accuracy: 0.9029 - auc: 0.9628 - precision_2: 0.9039 - recall_2: 0.9246 - val_loss: 0.3353 - val_accuracy: 0.8650 - val_auc: 0.9322 - val_precision_2: 0.8484 - val_recall_2: 0.9217 - lr: 1.3122e-04\n",
      "Epoch 25/100\n",
      "112/112 [==============================] - 11s 103ms/step - loss: 0.2342 - accuracy: 0.9040 - auc: 0.9647 - precision_2: 0.8994 - recall_2: 0.9303 - val_loss: 0.3127 - val_accuracy: 0.8705 - val_auc: 0.9425 - val_precision_2: 0.8645 - val_recall_2: 0.9096 - lr: 1.3122e-04\n",
      "Epoch 26/100\n",
      "112/112 [==============================] - ETA: 0s - loss: 0.2283 - accuracy: 0.9065 - auc: 0.9657 - precision_2: 0.9047 - recall_2: 0.9305\n",
      "Epoch 26: ReduceLROnPlateau reducing learning rate to 2.6243997854180635e-05.\n",
      "112/112 [==============================] - 12s 105ms/step - loss: 0.2283 - accuracy: 0.9065 - auc: 0.9657 - precision_2: 0.9047 - recall_2: 0.9305 - val_loss: 0.3252 - val_accuracy: 0.8661 - val_auc: 0.9451 - val_precision_2: 0.8387 - val_recall_2: 0.9398 - lr: 2.1258e-05\n",
      "Epoch 27/100\n",
      "112/112 [==============================] - 11s 102ms/step - loss: 0.2574 - accuracy: 0.8931 - auc: 0.9573 - precision_2: 0.8881 - recall_2: 0.9250 - val_loss: 0.3384 - val_accuracy: 0.8627 - val_auc: 0.9416 - val_precision_2: 0.8238 - val_recall_2: 0.9578 - lr: 1.9132e-05\n",
      "Epoch 28/100\n",
      "112/112 [==============================] - 11s 102ms/step - loss: 0.2479 - accuracy: 0.9001 - auc: 0.9612 - precision_2: 0.9000 - recall_2: 0.9226 - val_loss: 0.3188 - val_accuracy: 0.8717 - val_auc: 0.9422 - val_precision_2: 0.8488 - val_recall_2: 0.9357 - lr: 1.9132e-05\n",
      "Epoch 29/100\n",
      "112/112 [==============================] - 11s 102ms/step - loss: 0.2251 - accuracy: 0.9110 - auc: 0.9666 - precision_2: 0.9083 - recall_2: 0.9350 - val_loss: 0.3185 - val_accuracy: 0.8661 - val_auc: 0.9438 - val_precision_2: 0.8436 - val_recall_2: 0.9317 - lr: 1.9132e-05\n",
      "Epoch 30/100\n",
      "112/112 [==============================] - 11s 103ms/step - loss: 0.2337 - accuracy: 0.9026 - auc: 0.9649 - precision_2: 0.8991 - recall_2: 0.9261 - val_loss: 0.3201 - val_accuracy: 0.8661 - val_auc: 0.9437 - val_precision_2: 0.8363 - val_recall_2: 0.9438 - lr: 1.9132e-05\n",
      "Epoch 31/100\n",
      "112/112 [==============================] - ETA: 0s - loss: 0.2287 - accuracy: 0.9088 - auc: 0.9658 - precision_2: 0.9027 - recall_2: 0.9374\n",
      "Epoch 31: ReduceLROnPlateau reducing learning rate to 3.826374813797884e-06.\n",
      "112/112 [==============================] - 12s 105ms/step - loss: 0.2287 - accuracy: 0.9088 - auc: 0.9658 - precision_2: 0.9027 - recall_2: 0.9374 - val_loss: 0.3145 - val_accuracy: 0.8661 - val_auc: 0.9419 - val_precision_2: 0.8566 - val_recall_2: 0.9116 - lr: 2.7894e-06\n",
      "Epoch 32/100\n",
      "112/112 [==============================] - 11s 102ms/step - loss: 0.2376 - accuracy: 0.9090 - auc: 0.9630 - precision_2: 0.9071 - recall_2: 0.9325 - val_loss: 0.3232 - val_accuracy: 0.8616 - val_auc: 0.9435 - val_precision_2: 0.8339 - val_recall_2: 0.9378 - lr: 2.7894e-06\n",
      "Epoch 33/100\n",
      "112/112 [==============================] - 11s 102ms/step - loss: 0.2388 - accuracy: 0.9032 - auc: 0.9626 - precision_2: 0.8992 - recall_2: 0.9295 - val_loss: 0.3194 - val_accuracy: 0.8672 - val_auc: 0.9454 - val_precision_2: 0.8330 - val_recall_2: 0.9518 - lr: 2.7894e-06\n",
      "Epoch 34/100\n",
      "112/112 [==============================] - 11s 102ms/step - loss: 0.2470 - accuracy: 0.9040 - auc: 0.9606 - precision_2: 0.9001 - recall_2: 0.9293 - val_loss: 0.3374 - val_accuracy: 0.8650 - val_auc: 0.9419 - val_precision_2: 0.8313 - val_recall_2: 0.9498 - lr: 2.7894e-06\n",
      "Epoch 35/100\n",
      "112/112 [==============================] - 11s 102ms/step - loss: 0.2306 - accuracy: 0.9132 - auc: 0.9647 - precision_2: 0.9108 - recall_2: 0.9375 - val_loss: 0.3440 - val_accuracy: 0.8638 - val_auc: 0.9378 - val_precision_2: 0.8381 - val_recall_2: 0.9357 - lr: 2.7894e-06\n",
      "Epoch 36/100\n",
      "112/112 [==============================] - ETA: 0s - loss: 0.2224 - accuracy: 0.9082 - auc: 0.9677 - precision_2: 0.9047 - recall_2: 0.9309\n",
      "Epoch 36: ReduceLROnPlateau reducing learning rate to 1e-06.\n",
      "112/112 [==============================] - 12s 105ms/step - loss: 0.2224 - accuracy: 0.9082 - auc: 0.9677 - precision_2: 0.9047 - recall_2: 0.9309 - val_loss: 0.3202 - val_accuracy: 0.8728 - val_auc: 0.9464 - val_precision_2: 0.8453 - val_recall_2: 0.9438 - lr: 6.5610e-07\n",
      "Epoch 37/100\n",
      "112/112 [==============================] - 11s 102ms/step - loss: 0.2419 - accuracy: 0.8973 - auc: 0.9630 - precision_2: 0.8963 - recall_2: 0.9231 - val_loss: 0.3388 - val_accuracy: 0.8516 - val_auc: 0.9429 - val_precision_2: 0.8130 - val_recall_2: 0.9518 - lr: 6.5610e-07\n",
      "Epoch 38/100\n",
      "112/112 [==============================] - 11s 103ms/step - loss: 0.2351 - accuracy: 0.9085 - auc: 0.9639 - precision_2: 0.9006 - recall_2: 0.9371 - val_loss: 0.3260 - val_accuracy: 0.8694 - val_auc: 0.9440 - val_precision_2: 0.8396 - val_recall_2: 0.9458 - lr: 6.5610e-07\n",
      "Epoch 39/100\n",
      "112/112 [==============================] - 12s 110ms/step - loss: 0.2270 - accuracy: 0.9149 - auc: 0.9663 - precision_2: 0.9098 - recall_2: 0.9411 - val_loss: 0.2903 - val_accuracy: 0.8828 - val_auc: 0.9487 - val_precision_2: 0.8619 - val_recall_2: 0.9398 - lr: 6.5610e-07\n",
      "Epoch 40/100\n",
      "112/112 [==============================] - 11s 103ms/step - loss: 0.2298 - accuracy: 0.9102 - auc: 0.9661 - precision_2: 0.9086 - recall_2: 0.9298 - val_loss: 0.3152 - val_accuracy: 0.8705 - val_auc: 0.9431 - val_precision_2: 0.8524 - val_recall_2: 0.9277 - lr: 6.5610e-07\n",
      "Epoch 41/100\n",
      "112/112 [==============================] - 12s 110ms/step - loss: 0.2217 - accuracy: 0.9102 - auc: 0.9672 - precision_2: 0.9102 - recall_2: 0.9319 - val_loss: 0.3003 - val_accuracy: 0.8750 - val_auc: 0.9503 - val_precision_2: 0.8459 - val_recall_2: 0.9478 - lr: 6.5610e-07\n",
      "Epoch 42/100\n",
      "112/112 [==============================] - 11s 103ms/step - loss: 0.2184 - accuracy: 0.9185 - auc: 0.9686 - precision_2: 0.9155 - recall_2: 0.9411 - val_loss: 0.3157 - val_accuracy: 0.8717 - val_auc: 0.9450 - val_precision_2: 0.8488 - val_recall_2: 0.9357 - lr: 6.5610e-07\n",
      "Epoch 43/100\n",
      "112/112 [==============================] - 11s 103ms/step - loss: 0.2286 - accuracy: 0.9060 - auc: 0.9661 - precision_2: 0.9012 - recall_2: 0.9325 - val_loss: 0.2999 - val_accuracy: 0.8761 - val_auc: 0.9461 - val_precision_2: 0.8577 - val_recall_2: 0.9317 - lr: 6.5610e-07\n",
      "Epoch 44/100\n",
      "112/112 [==============================] - 11s 102ms/step - loss: 0.2090 - accuracy: 0.9146 - auc: 0.9719 - precision_2: 0.9072 - recall_2: 0.9428 - val_loss: 0.3116 - val_accuracy: 0.8705 - val_auc: 0.9460 - val_precision_2: 0.8645 - val_recall_2: 0.9096 - lr: 6.5610e-07\n",
      "Epoch 45/100\n",
      "112/112 [==============================] - 11s 102ms/step - loss: 0.2201 - accuracy: 0.9090 - auc: 0.9693 - precision_2: 0.9130 - recall_2: 0.9240 - val_loss: 0.3009 - val_accuracy: 0.8717 - val_auc: 0.9487 - val_precision_2: 0.8579 - val_recall_2: 0.9217 - lr: 5.9049e-07\n",
      "Epoch 46/100\n",
      "112/112 [==============================] - 11s 103ms/step - loss: 0.2079 - accuracy: 0.9188 - auc: 0.9717 - precision_2: 0.9173 - recall_2: 0.9381 - val_loss: 0.3128 - val_accuracy: 0.8694 - val_auc: 0.9458 - val_precision_2: 0.8445 - val_recall_2: 0.9378 - lr: 5.9049e-07\n",
      "Epoch 47/100\n",
      "112/112 [==============================] - 12s 110ms/step - loss: 0.1857 - accuracy: 0.9283 - auc: 0.9777 - precision_2: 0.9303 - recall_2: 0.9411 - val_loss: 0.3146 - val_accuracy: 0.8739 - val_auc: 0.9520 - val_precision_2: 0.8431 - val_recall_2: 0.9498 - lr: 5.9049e-07\n",
      "Epoch 48/100\n",
      "112/112 [==============================] - 12s 110ms/step - loss: 0.2063 - accuracy: 0.9169 - auc: 0.9721 - precision_2: 0.9082 - recall_2: 0.9464 - val_loss: 0.2880 - val_accuracy: 0.8817 - val_auc: 0.9539 - val_precision_2: 0.8630 - val_recall_2: 0.9357 - lr: 5.9049e-07\n",
      "Epoch 49/100\n",
      "112/112 [==============================] - 11s 103ms/step - loss: 0.1810 - accuracy: 0.9308 - auc: 0.9782 - precision_2: 0.9247 - recall_2: 0.9526 - val_loss: 0.2963 - val_accuracy: 0.8750 - val_auc: 0.9518 - val_precision_2: 0.8496 - val_recall_2: 0.9418 - lr: 5.9049e-07\n",
      "Epoch 50/100\n",
      "112/112 [==============================] - 12s 110ms/step - loss: 0.1947 - accuracy: 0.9191 - auc: 0.9746 - precision_2: 0.9181 - recall_2: 0.9392 - val_loss: 0.2923 - val_accuracy: 0.8772 - val_auc: 0.9542 - val_precision_2: 0.8606 - val_recall_2: 0.9297 - lr: 5.9049e-07\n",
      "Epoch 51/100\n",
      "112/112 [==============================] - 11s 103ms/step - loss: 0.1920 - accuracy: 0.9275 - auc: 0.9756 - precision_2: 0.9246 - recall_2: 0.9452 - val_loss: 0.3054 - val_accuracy: 0.8828 - val_auc: 0.9476 - val_precision_2: 0.8743 - val_recall_2: 0.9217 - lr: 5.9049e-07\n",
      "Epoch 52/100\n",
      "112/112 [==============================] - 11s 102ms/step - loss: 0.2111 - accuracy: 0.9143 - auc: 0.9710 - precision_2: 0.9150 - recall_2: 0.9320 - val_loss: 0.3060 - val_accuracy: 0.8717 - val_auc: 0.9492 - val_precision_2: 0.8488 - val_recall_2: 0.9357 - lr: 5.9049e-07\n",
      "Epoch 53/100\n",
      "112/112 [==============================] - 11s 102ms/step - loss: 0.1912 - accuracy: 0.9266 - auc: 0.9756 - precision_2: 0.9183 - recall_2: 0.9521 - val_loss: 0.2903 - val_accuracy: 0.8828 - val_auc: 0.9513 - val_precision_2: 0.8876 - val_recall_2: 0.9036 - lr: 5.9049e-07\n",
      "Epoch 54/100\n",
      "112/112 [==============================] - 11s 102ms/step - loss: 0.1884 - accuracy: 0.9210 - auc: 0.9766 - precision_2: 0.9183 - recall_2: 0.9426 - val_loss: 0.3064 - val_accuracy: 0.8739 - val_auc: 0.9482 - val_precision_2: 0.8723 - val_recall_2: 0.9056 - lr: 5.3144e-07\n",
      "Epoch 55/100\n",
      "112/112 [==============================] - 11s 102ms/step - loss: 0.1760 - accuracy: 0.9347 - auc: 0.9790 - precision_2: 0.9331 - recall_2: 0.9509 - val_loss: 0.3101 - val_accuracy: 0.8638 - val_auc: 0.9486 - val_precision_2: 0.8431 - val_recall_2: 0.9277 - lr: 5.3144e-07\n",
      "Epoch 56/100\n",
      "112/112 [==============================] - 12s 110ms/step - loss: 0.1800 - accuracy: 0.9300 - auc: 0.9780 - precision_2: 0.9275 - recall_2: 0.9484 - val_loss: 0.3003 - val_accuracy: 0.8772 - val_auc: 0.9545 - val_precision_2: 0.8566 - val_recall_2: 0.9357 - lr: 5.3144e-07\n",
      "Epoch 57/100\n",
      "112/112 [==============================] - 11s 103ms/step - loss: 0.1767 - accuracy: 0.9325 - auc: 0.9789 - precision_2: 0.9298 - recall_2: 0.9495 - val_loss: 0.2973 - val_accuracy: 0.8728 - val_auc: 0.9496 - val_precision_2: 0.8623 - val_recall_2: 0.9177 - lr: 5.3144e-07\n",
      "Epoch 58/100\n",
      "112/112 [==============================] - 11s 103ms/step - loss: 0.1945 - accuracy: 0.9266 - auc: 0.9746 - precision_2: 0.9214 - recall_2: 0.9503 - val_loss: 0.2833 - val_accuracy: 0.8750 - val_auc: 0.9536 - val_precision_2: 0.8628 - val_recall_2: 0.9217 - lr: 5.3144e-07\n",
      "Epoch 59/100\n",
      "112/112 [==============================] - 11s 103ms/step - loss: 0.1897 - accuracy: 0.9252 - auc: 0.9759 - precision_2: 0.9184 - recall_2: 0.9482 - val_loss: 0.2901 - val_accuracy: 0.8761 - val_auc: 0.9499 - val_precision_2: 0.8686 - val_recall_2: 0.9157 - lr: 5.3144e-07\n",
      "Epoch 60/100\n",
      "112/112 [==============================] - 11s 102ms/step - loss: 0.1680 - accuracy: 0.9378 - auc: 0.9806 - precision_2: 0.9344 - recall_2: 0.9546 - val_loss: 0.3087 - val_accuracy: 0.8783 - val_auc: 0.9503 - val_precision_2: 0.8609 - val_recall_2: 0.9317 - lr: 5.3144e-07\n",
      "Epoch 61/100\n",
      "112/112 [==============================] - 11s 102ms/step - loss: 0.1777 - accuracy: 0.9344 - auc: 0.9778 - precision_2: 0.9308 - recall_2: 0.9544 - val_loss: 0.3136 - val_accuracy: 0.8772 - val_auc: 0.9525 - val_precision_2: 0.8502 - val_recall_2: 0.9458 - lr: 5.3144e-07\n",
      "Epoch 62/100\n",
      "112/112 [==============================] - 12s 103ms/step - loss: 0.1757 - accuracy: 0.9311 - auc: 0.9791 - precision_2: 0.9213 - recall_2: 0.9548 - val_loss: 0.3030 - val_accuracy: 0.8839 - val_auc: 0.9510 - val_precision_2: 0.8662 - val_recall_2: 0.9357 - lr: 5.3144e-07\n",
      "Epoch 63/100\n",
      "112/112 [==============================] - 11s 102ms/step - loss: 0.1852 - accuracy: 0.9289 - auc: 0.9768 - precision_2: 0.9299 - recall_2: 0.9443 - val_loss: 0.3024 - val_accuracy: 0.8772 - val_auc: 0.9500 - val_precision_2: 0.8593 - val_recall_2: 0.9317 - lr: 4.7830e-07\n",
      "Epoch 64/100\n",
      "112/112 [==============================] - 11s 103ms/step - loss: 0.1683 - accuracy: 0.9339 - auc: 0.9807 - precision_2: 0.9278 - recall_2: 0.9561 - val_loss: 0.3023 - val_accuracy: 0.8795 - val_auc: 0.9499 - val_precision_2: 0.8693 - val_recall_2: 0.9217 - lr: 4.7830e-07\n",
      "Epoch 65/100\n",
      "112/112 [==============================] - 11s 102ms/step - loss: 0.1599 - accuracy: 0.9420 - auc: 0.9821 - precision_2: 0.9343 - recall_2: 0.9619 - val_loss: 0.3076 - val_accuracy: 0.8772 - val_auc: 0.9497 - val_precision_2: 0.8702 - val_recall_2: 0.9157 - lr: 4.7830e-07\n",
      "Epoch 66/100\n",
      "112/112 [==============================] - 11s 102ms/step - loss: 0.1660 - accuracy: 0.9375 - auc: 0.9808 - precision_2: 0.9407 - recall_2: 0.9490 - val_loss: 0.3182 - val_accuracy: 0.8806 - val_auc: 0.9518 - val_precision_2: 0.8641 - val_recall_2: 0.9317 - lr: 4.7830e-07\n",
      "Epoch 67/100\n",
      "112/112 [==============================] - 11s 102ms/step - loss: 0.1765 - accuracy: 0.9308 - auc: 0.9787 - precision_2: 0.9264 - recall_2: 0.9498 - val_loss: 0.3155 - val_accuracy: 0.8783 - val_auc: 0.9460 - val_precision_2: 0.8777 - val_recall_2: 0.9076 - lr: 4.7830e-07\n",
      "Epoch 68/100\n",
      "112/112 [==============================] - 11s 102ms/step - loss: 0.1515 - accuracy: 0.9453 - auc: 0.9839 - precision_2: 0.9416 - recall_2: 0.9622 - val_loss: 0.3259 - val_accuracy: 0.8761 - val_auc: 0.9450 - val_precision_2: 0.8728 - val_recall_2: 0.9096 - lr: 4.7830e-07\n",
      "Epoch 69/100\n",
      "112/112 [==============================] - 12s 110ms/step - loss: 0.1698 - accuracy: 0.9350 - auc: 0.9805 - precision_2: 0.9296 - recall_2: 0.9560 - val_loss: 0.3012 - val_accuracy: 0.8783 - val_auc: 0.9562 - val_precision_2: 0.8479 - val_recall_2: 0.9518 - lr: 4.7830e-07\n",
      "Epoch 70/100\n",
      "112/112 [==============================] - 11s 102ms/step - loss: 0.1554 - accuracy: 0.9400 - auc: 0.9835 - precision_2: 0.9337 - recall_2: 0.9581 - val_loss: 0.3019 - val_accuracy: 0.8862 - val_auc: 0.9513 - val_precision_2: 0.8750 - val_recall_2: 0.9277 - lr: 4.7830e-07\n",
      "Epoch 71/100\n",
      "112/112 [==============================] - 11s 103ms/step - loss: 0.1449 - accuracy: 0.9431 - auc: 0.9857 - precision_2: 0.9429 - recall_2: 0.9560 - val_loss: 0.3139 - val_accuracy: 0.8839 - val_auc: 0.9506 - val_precision_2: 0.8760 - val_recall_2: 0.9217 - lr: 4.7830e-07\n",
      "Epoch 72/100\n",
      "112/112 [==============================] - 11s 102ms/step - loss: 0.1528 - accuracy: 0.9436 - auc: 0.9849 - precision_2: 0.9385 - recall_2: 0.9612 - val_loss: 0.3293 - val_accuracy: 0.8783 - val_auc: 0.9443 - val_precision_2: 0.8867 - val_recall_2: 0.8956 - lr: 4.3047e-07\n",
      "Epoch 73/100\n",
      "112/112 [==============================] - 11s 102ms/step - loss: 0.1629 - accuracy: 0.9361 - auc: 0.9824 - precision_2: 0.9304 - recall_2: 0.9562 - val_loss: 0.3078 - val_accuracy: 0.8839 - val_auc: 0.9495 - val_precision_2: 0.8760 - val_recall_2: 0.9217 - lr: 4.3047e-07\n",
      "Epoch 74/100\n",
      "112/112 [==============================] - 11s 103ms/step - loss: 0.1534 - accuracy: 0.9422 - auc: 0.9837 - precision_2: 0.9421 - recall_2: 0.9548 - val_loss: 0.3128 - val_accuracy: 0.8795 - val_auc: 0.9497 - val_precision_2: 0.8638 - val_recall_2: 0.9297 - lr: 4.3047e-07\n",
      "Epoch 75/100\n",
      "112/112 [==============================] - 11s 102ms/step - loss: 0.1463 - accuracy: 0.9422 - auc: 0.9853 - precision_2: 0.9393 - recall_2: 0.9585 - val_loss: 0.3289 - val_accuracy: 0.8839 - val_auc: 0.9490 - val_precision_2: 0.8731 - val_recall_2: 0.9257 - lr: 4.3047e-07\n",
      "Epoch 76/100\n",
      "112/112 [==============================] - 11s 102ms/step - loss: 0.1474 - accuracy: 0.9445 - auc: 0.9851 - precision_2: 0.9395 - recall_2: 0.9610 - val_loss: 0.3369 - val_accuracy: 0.8795 - val_auc: 0.9472 - val_precision_2: 0.8533 - val_recall_2: 0.9458 - lr: 4.3047e-07\n",
      "Epoch 77/100\n",
      "112/112 [==============================] - 11s 102ms/step - loss: 0.1458 - accuracy: 0.9453 - auc: 0.9850 - precision_2: 0.9406 - recall_2: 0.9631 - val_loss: 0.3242 - val_accuracy: 0.8795 - val_auc: 0.9502 - val_precision_2: 0.8625 - val_recall_2: 0.9317 - lr: 4.3047e-07\n",
      "Epoch 78/100\n",
      "112/112 [==============================] - 12s 103ms/step - loss: 0.1494 - accuracy: 0.9436 - auc: 0.9842 - precision_2: 0.9411 - recall_2: 0.9598 - val_loss: 0.3079 - val_accuracy: 0.8761 - val_auc: 0.9513 - val_precision_2: 0.8672 - val_recall_2: 0.9177 - lr: 4.3047e-07\n",
      "Epoch 79/100\n",
      "112/112 [==============================] - 11s 102ms/step - loss: 0.1367 - accuracy: 0.9489 - auc: 0.9865 - precision_2: 0.9419 - recall_2: 0.9664 - val_loss: 0.3458 - val_accuracy: 0.8761 - val_auc: 0.9422 - val_precision_2: 0.8925 - val_recall_2: 0.8835 - lr: 4.3047e-07\n",
      "Epoch 80/100\n",
      "112/112 [==============================] - 11s 103ms/step - loss: 0.1444 - accuracy: 0.9439 - auc: 0.9862 - precision_2: 0.9455 - recall_2: 0.9545 - val_loss: 0.3132 - val_accuracy: 0.8839 - val_auc: 0.9521 - val_precision_2: 0.8803 - val_recall_2: 0.9157 - lr: 4.3047e-07\n",
      "Epoch 81/100\n",
      "112/112 [==============================] - 11s 102ms/step - loss: 0.1378 - accuracy: 0.9484 - auc: 0.9866 - precision_2: 0.9473 - recall_2: 0.9602 - val_loss: 0.3219 - val_accuracy: 0.8817 - val_auc: 0.9489 - val_precision_2: 0.8755 - val_recall_2: 0.9177 - lr: 3.8742e-07\n",
      "Epoch 82/100\n",
      "112/112 [==============================] - 11s 102ms/step - loss: 0.1360 - accuracy: 0.9475 - auc: 0.9874 - precision_2: 0.9419 - recall_2: 0.9647 - val_loss: 0.3246 - val_accuracy: 0.8817 - val_auc: 0.9509 - val_precision_2: 0.8684 - val_recall_2: 0.9277 - lr: 3.8742e-07\n",
      "Epoch 83/100\n",
      "112/112 [==============================] - 11s 103ms/step - loss: 0.1414 - accuracy: 0.9487 - auc: 0.9860 - precision_2: 0.9497 - recall_2: 0.9600 - val_loss: 0.3308 - val_accuracy: 0.8806 - val_auc: 0.9477 - val_precision_2: 0.8696 - val_recall_2: 0.9237 - lr: 3.8742e-07\n",
      "Epoch 84/100\n",
      "112/112 [==============================] - 11s 103ms/step - loss: 0.1240 - accuracy: 0.9528 - auc: 0.9892 - precision_2: 0.9481 - recall_2: 0.9669 - val_loss: 0.3326 - val_accuracy: 0.8806 - val_auc: 0.9466 - val_precision_2: 0.8682 - val_recall_2: 0.9257 - lr: 3.8742e-07\n",
      "Epoch 85/100\n",
      "112/112 [==============================] - 11s 102ms/step - loss: 0.1344 - accuracy: 0.9473 - auc: 0.9877 - precision_2: 0.9464 - recall_2: 0.9592 - val_loss: 0.3297 - val_accuracy: 0.8862 - val_auc: 0.9472 - val_precision_2: 0.8736 - val_recall_2: 0.9297 - lr: 3.8742e-07\n",
      "Epoch 86/100\n",
      "112/112 [==============================] - 11s 102ms/step - loss: 0.1160 - accuracy: 0.9579 - auc: 0.9904 - precision_2: 0.9548 - recall_2: 0.9705 - val_loss: 0.3317 - val_accuracy: 0.8828 - val_auc: 0.9460 - val_precision_2: 0.8701 - val_recall_2: 0.9277 - lr: 3.8742e-07\n",
      "Epoch 87/100\n",
      "112/112 [==============================] - 11s 102ms/step - loss: 0.1287 - accuracy: 0.9498 - auc: 0.9888 - precision_2: 0.9506 - recall_2: 0.9592 - val_loss: 0.3465 - val_accuracy: 0.8739 - val_auc: 0.9468 - val_precision_2: 0.8532 - val_recall_2: 0.9337 - lr: 3.8742e-07\n",
      "Epoch 88/100\n",
      "112/112 [==============================] - 11s 102ms/step - loss: 0.1420 - accuracy: 0.9498 - auc: 0.9853 - precision_2: 0.9446 - recall_2: 0.9656 - val_loss: 0.3403 - val_accuracy: 0.8728 - val_auc: 0.9451 - val_precision_2: 0.8542 - val_recall_2: 0.9297 - lr: 3.8742e-07\n",
      "Epoch 89/100\n",
      "112/112 [==============================] - 11s 102ms/step - loss: 0.1312 - accuracy: 0.9495 - auc: 0.9884 - precision_2: 0.9524 - recall_2: 0.9585 - val_loss: 0.3247 - val_accuracy: 0.8850 - val_auc: 0.9494 - val_precision_2: 0.8748 - val_recall_2: 0.9257 - lr: 3.8742e-07\n",
      "Epoch 90/100\n",
      "112/112 [==============================] - 11s 103ms/step - loss: 0.1344 - accuracy: 0.9489 - auc: 0.9874 - precision_2: 0.9434 - recall_2: 0.9642 - val_loss: 0.3410 - val_accuracy: 0.8806 - val_auc: 0.9496 - val_precision_2: 0.8627 - val_recall_2: 0.9337 - lr: 3.4868e-07\n",
      "Epoch 91/100\n",
      "112/112 [==============================] - 11s 103ms/step - loss: 0.1223 - accuracy: 0.9562 - auc: 0.9895 - precision_2: 0.9562 - recall_2: 0.9653 - val_loss: 0.3296 - val_accuracy: 0.8828 - val_auc: 0.9488 - val_precision_2: 0.8786 - val_recall_2: 0.9157 - lr: 3.4868e-07\n",
      "Epoch 92/100\n",
      "112/112 [==============================] - 11s 102ms/step - loss: 0.1338 - accuracy: 0.9492 - auc: 0.9873 - precision_2: 0.9486 - recall_2: 0.9609 - val_loss: 0.3371 - val_accuracy: 0.8772 - val_auc: 0.9457 - val_precision_2: 0.8688 - val_recall_2: 0.9177 - lr: 3.4868e-07\n",
      "Epoch 93/100\n",
      "112/112 [==============================] - 11s 102ms/step - loss: 0.1149 - accuracy: 0.9568 - auc: 0.9902 - precision_2: 0.9580 - recall_2: 0.9652 - val_loss: 0.3379 - val_accuracy: 0.8884 - val_auc: 0.9486 - val_precision_2: 0.8902 - val_recall_2: 0.9116 - lr: 3.4868e-07\n",
      "Epoch 94/100\n",
      "112/112 [==============================] - 11s 103ms/step - loss: 0.1238 - accuracy: 0.9542 - auc: 0.9888 - precision_2: 0.9473 - recall_2: 0.9704 - val_loss: 0.3328 - val_accuracy: 0.8884 - val_auc: 0.9499 - val_precision_2: 0.8699 - val_recall_2: 0.9398 - lr: 3.4868e-07\n",
      "Epoch 95/100\n",
      "112/112 [==============================] - 11s 103ms/step - loss: 0.1243 - accuracy: 0.9551 - auc: 0.9889 - precision_2: 0.9579 - recall_2: 0.9622 - val_loss: 0.3657 - val_accuracy: 0.8873 - val_auc: 0.9470 - val_precision_2: 0.8669 - val_recall_2: 0.9418 - lr: 3.4868e-07\n",
      "Epoch 96/100\n",
      "112/112 [==============================] - 11s 103ms/step - loss: 0.1219 - accuracy: 0.9593 - auc: 0.9887 - precision_2: 0.9543 - recall_2: 0.9742 - val_loss: 0.3278 - val_accuracy: 0.8917 - val_auc: 0.9481 - val_precision_2: 0.8863 - val_recall_2: 0.9237 - lr: 3.4868e-07\n",
      "Epoch 97/100\n",
      "112/112 [==============================] - 11s 103ms/step - loss: 0.1213 - accuracy: 0.9548 - auc: 0.9898 - precision_2: 0.9470 - recall_2: 0.9713 - val_loss: 0.3460 - val_accuracy: 0.8750 - val_auc: 0.9456 - val_precision_2: 0.8712 - val_recall_2: 0.9096 - lr: 3.4868e-07\n",
      "Epoch 98/100\n",
      "112/112 [==============================] - 12s 103ms/step - loss: 0.1184 - accuracy: 0.9559 - auc: 0.9900 - precision_2: 0.9575 - recall_2: 0.9642 - val_loss: 0.3536 - val_accuracy: 0.8862 - val_auc: 0.9493 - val_precision_2: 0.8736 - val_recall_2: 0.9297 - lr: 3.4868e-07\n",
      "Epoch 99/100\n",
      "112/112 [==============================] - 11s 103ms/step - loss: 0.0999 - accuracy: 0.9637 - auc: 0.9930 - precision_2: 0.9589 - recall_2: 0.9763 - val_loss: 0.3574 - val_accuracy: 0.8828 - val_auc: 0.9469 - val_precision_2: 0.8743 - val_recall_2: 0.9217 - lr: 3.1381e-07\n",
      "Epoch 100/100\n",
      "112/112 [==============================] - 11s 103ms/step - loss: 0.1133 - accuracy: 0.9565 - auc: 0.9914 - precision_2: 0.9528 - recall_2: 0.9691 - val_loss: 0.3560 - val_accuracy: 0.8817 - val_auc: 0.9453 - val_precision_2: 0.8712 - val_recall_2: 0.9237 - lr: 3.1381e-07\n",
      "28/28 [==============================] - 2s 72ms/step - loss: 0.3560 - accuracy: 0.8817 - auc: 0.9453 - precision_2: 0.8712 - recall_2: 0.9237\n",
      "Validation Loss: 0.3560\n",
      "Validation Accuracy: 0.8817\n",
      "Validation AUC: 0.9453\n",
      "Validation Precision: 0.8712\n",
      "Validation Recall: 0.9237\n",
      "28/28 [==============================] - 4s 65ms/step\n",
      "Confusion Matrix:\n",
      "[[330  68]\n",
      " [ 38 460]]\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.90      0.83      0.86       398\n",
      "           1       0.87      0.92      0.90       498\n",
      "\n",
      "    accuracy                           0.88       896\n",
      "   macro avg       0.88      0.88      0.88       896\n",
      "weighted avg       0.88      0.88      0.88       896\n",
      "\n",
      "Visualizations have been saved as PNG files.\n",
      "Predictions have been saved in 'predictions.csv'.\n",
      "Model: \"model_2\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_5 (InputLayer)           [(None, 224, 224, 3  0           []                               \n",
      "                                )]                                                                \n",
      "                                                                                                  \n",
      " conv1_pad (ZeroPadding2D)      (None, 230, 230, 3)  0           ['input_5[0][0]']                \n",
      "                                                                                                  \n",
      " conv1_conv (Conv2D)            (None, 112, 112, 64  9472        ['conv1_pad[0][0]']              \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " pool1_pad (ZeroPadding2D)      (None, 114, 114, 64  0           ['conv1_conv[0][0]']             \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " pool1_pool (MaxPooling2D)      (None, 56, 56, 64)   0           ['pool1_pad[0][0]']              \n",
      "                                                                                                  \n",
      " conv2_block1_preact_bn (BatchN  (None, 56, 56, 64)  256         ['pool1_pool[0][0]']             \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " conv2_block1_preact_relu (Acti  (None, 56, 56, 64)  0           ['conv2_block1_preact_bn[0][0]'] \n",
      " vation)                                                                                          \n",
      "                                                                                                  \n",
      " conv2_block1_1_conv (Conv2D)   (None, 56, 56, 64)   4096        ['conv2_block1_preact_relu[0][0]'\n",
      "                                                                 ]                                \n",
      "                                                                                                  \n",
      " conv2_block1_1_bn (BatchNormal  (None, 56, 56, 64)  256         ['conv2_block1_1_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv2_block1_1_relu (Activatio  (None, 56, 56, 64)  0           ['conv2_block1_1_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv2_block1_2_pad (ZeroPaddin  (None, 58, 58, 64)  0           ['conv2_block1_1_relu[0][0]']    \n",
      " g2D)                                                                                             \n",
      "                                                                                                  \n",
      " conv2_block1_2_conv (Conv2D)   (None, 56, 56, 64)   36864       ['conv2_block1_2_pad[0][0]']     \n",
      "                                                                                                  \n",
      " conv2_block1_2_bn (BatchNormal  (None, 56, 56, 64)  256         ['conv2_block1_2_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv2_block1_2_relu (Activatio  (None, 56, 56, 64)  0           ['conv2_block1_2_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv2_block1_0_conv (Conv2D)   (None, 56, 56, 256)  16640       ['conv2_block1_preact_relu[0][0]'\n",
      "                                                                 ]                                \n",
      "                                                                                                  \n",
      " conv2_block1_3_conv (Conv2D)   (None, 56, 56, 256)  16640       ['conv2_block1_2_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv2_block1_out (Add)         (None, 56, 56, 256)  0           ['conv2_block1_0_conv[0][0]',    \n",
      "                                                                  'conv2_block1_3_conv[0][0]']    \n",
      "                                                                                                  \n",
      " conv2_block2_preact_bn (BatchN  (None, 56, 56, 256)  1024       ['conv2_block1_out[0][0]']       \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " conv2_block2_preact_relu (Acti  (None, 56, 56, 256)  0          ['conv2_block2_preact_bn[0][0]'] \n",
      " vation)                                                                                          \n",
      "                                                                                                  \n",
      " conv2_block2_1_conv (Conv2D)   (None, 56, 56, 64)   16384       ['conv2_block2_preact_relu[0][0]'\n",
      "                                                                 ]                                \n",
      "                                                                                                  \n",
      " conv2_block2_1_bn (BatchNormal  (None, 56, 56, 64)  256         ['conv2_block2_1_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv2_block2_1_relu (Activatio  (None, 56, 56, 64)  0           ['conv2_block2_1_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv2_block2_2_pad (ZeroPaddin  (None, 58, 58, 64)  0           ['conv2_block2_1_relu[0][0]']    \n",
      " g2D)                                                                                             \n",
      "                                                                                                  \n",
      " conv2_block2_2_conv (Conv2D)   (None, 56, 56, 64)   36864       ['conv2_block2_2_pad[0][0]']     \n",
      "                                                                                                  \n",
      " conv2_block2_2_bn (BatchNormal  (None, 56, 56, 64)  256         ['conv2_block2_2_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv2_block2_2_relu (Activatio  (None, 56, 56, 64)  0           ['conv2_block2_2_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv2_block2_3_conv (Conv2D)   (None, 56, 56, 256)  16640       ['conv2_block2_2_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv2_block2_out (Add)         (None, 56, 56, 256)  0           ['conv2_block1_out[0][0]',       \n",
      "                                                                  'conv2_block2_3_conv[0][0]']    \n",
      "                                                                                                  \n",
      " conv2_block3_preact_bn (BatchN  (None, 56, 56, 256)  1024       ['conv2_block2_out[0][0]']       \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " conv2_block3_preact_relu (Acti  (None, 56, 56, 256)  0          ['conv2_block3_preact_bn[0][0]'] \n",
      " vation)                                                                                          \n",
      "                                                                                                  \n",
      " conv2_block3_1_conv (Conv2D)   (None, 56, 56, 64)   16384       ['conv2_block3_preact_relu[0][0]'\n",
      "                                                                 ]                                \n",
      "                                                                                                  \n",
      " conv2_block3_1_bn (BatchNormal  (None, 56, 56, 64)  256         ['conv2_block3_1_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv2_block3_1_relu (Activatio  (None, 56, 56, 64)  0           ['conv2_block3_1_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv2_block3_2_pad (ZeroPaddin  (None, 58, 58, 64)  0           ['conv2_block3_1_relu[0][0]']    \n",
      " g2D)                                                                                             \n",
      "                                                                                                  \n",
      " conv2_block3_2_conv (Conv2D)   (None, 28, 28, 64)   36864       ['conv2_block3_2_pad[0][0]']     \n",
      "                                                                                                  \n",
      " conv2_block3_2_bn (BatchNormal  (None, 28, 28, 64)  256         ['conv2_block3_2_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv2_block3_2_relu (Activatio  (None, 28, 28, 64)  0           ['conv2_block3_2_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " max_pooling2d_6 (MaxPooling2D)  (None, 28, 28, 256)  0          ['conv2_block2_out[0][0]']       \n",
      "                                                                                                  \n",
      " conv2_block3_3_conv (Conv2D)   (None, 28, 28, 256)  16640       ['conv2_block3_2_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv2_block3_out (Add)         (None, 28, 28, 256)  0           ['max_pooling2d_6[0][0]',        \n",
      "                                                                  'conv2_block3_3_conv[0][0]']    \n",
      "                                                                                                  \n",
      " conv3_block1_preact_bn (BatchN  (None, 28, 28, 256)  1024       ['conv2_block3_out[0][0]']       \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " conv3_block1_preact_relu (Acti  (None, 28, 28, 256)  0          ['conv3_block1_preact_bn[0][0]'] \n",
      " vation)                                                                                          \n",
      "                                                                                                  \n",
      " conv3_block1_1_conv (Conv2D)   (None, 28, 28, 128)  32768       ['conv3_block1_preact_relu[0][0]'\n",
      "                                                                 ]                                \n",
      "                                                                                                  \n",
      " conv3_block1_1_bn (BatchNormal  (None, 28, 28, 128)  512        ['conv3_block1_1_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv3_block1_1_relu (Activatio  (None, 28, 28, 128)  0          ['conv3_block1_1_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv3_block1_2_pad (ZeroPaddin  (None, 30, 30, 128)  0          ['conv3_block1_1_relu[0][0]']    \n",
      " g2D)                                                                                             \n",
      "                                                                                                  \n",
      " conv3_block1_2_conv (Conv2D)   (None, 28, 28, 128)  147456      ['conv3_block1_2_pad[0][0]']     \n",
      "                                                                                                  \n",
      " conv3_block1_2_bn (BatchNormal  (None, 28, 28, 128)  512        ['conv3_block1_2_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv3_block1_2_relu (Activatio  (None, 28, 28, 128)  0          ['conv3_block1_2_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv3_block1_0_conv (Conv2D)   (None, 28, 28, 512)  131584      ['conv3_block1_preact_relu[0][0]'\n",
      "                                                                 ]                                \n",
      "                                                                                                  \n",
      " conv3_block1_3_conv (Conv2D)   (None, 28, 28, 512)  66048       ['conv3_block1_2_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv3_block1_out (Add)         (None, 28, 28, 512)  0           ['conv3_block1_0_conv[0][0]',    \n",
      "                                                                  'conv3_block1_3_conv[0][0]']    \n",
      "                                                                                                  \n",
      " conv3_block2_preact_bn (BatchN  (None, 28, 28, 512)  2048       ['conv3_block1_out[0][0]']       \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " conv3_block2_preact_relu (Acti  (None, 28, 28, 512)  0          ['conv3_block2_preact_bn[0][0]'] \n",
      " vation)                                                                                          \n",
      "                                                                                                  \n",
      " conv3_block2_1_conv (Conv2D)   (None, 28, 28, 128)  65536       ['conv3_block2_preact_relu[0][0]'\n",
      "                                                                 ]                                \n",
      "                                                                                                  \n",
      " conv3_block2_1_bn (BatchNormal  (None, 28, 28, 128)  512        ['conv3_block2_1_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv3_block2_1_relu (Activatio  (None, 28, 28, 128)  0          ['conv3_block2_1_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv3_block2_2_pad (ZeroPaddin  (None, 30, 30, 128)  0          ['conv3_block2_1_relu[0][0]']    \n",
      " g2D)                                                                                             \n",
      "                                                                                                  \n",
      " conv3_block2_2_conv (Conv2D)   (None, 28, 28, 128)  147456      ['conv3_block2_2_pad[0][0]']     \n",
      "                                                                                                  \n",
      " conv3_block2_2_bn (BatchNormal  (None, 28, 28, 128)  512        ['conv3_block2_2_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv3_block2_2_relu (Activatio  (None, 28, 28, 128)  0          ['conv3_block2_2_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv3_block2_3_conv (Conv2D)   (None, 28, 28, 512)  66048       ['conv3_block2_2_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv3_block2_out (Add)         (None, 28, 28, 512)  0           ['conv3_block1_out[0][0]',       \n",
      "                                                                  'conv3_block2_3_conv[0][0]']    \n",
      "                                                                                                  \n",
      " conv3_block3_preact_bn (BatchN  (None, 28, 28, 512)  2048       ['conv3_block2_out[0][0]']       \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " conv3_block3_preact_relu (Acti  (None, 28, 28, 512)  0          ['conv3_block3_preact_bn[0][0]'] \n",
      " vation)                                                                                          \n",
      "                                                                                                  \n",
      " conv3_block3_1_conv (Conv2D)   (None, 28, 28, 128)  65536       ['conv3_block3_preact_relu[0][0]'\n",
      "                                                                 ]                                \n",
      "                                                                                                  \n",
      " conv3_block3_1_bn (BatchNormal  (None, 28, 28, 128)  512        ['conv3_block3_1_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv3_block3_1_relu (Activatio  (None, 28, 28, 128)  0          ['conv3_block3_1_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv3_block3_2_pad (ZeroPaddin  (None, 30, 30, 128)  0          ['conv3_block3_1_relu[0][0]']    \n",
      " g2D)                                                                                             \n",
      "                                                                                                  \n",
      " conv3_block3_2_conv (Conv2D)   (None, 28, 28, 128)  147456      ['conv3_block3_2_pad[0][0]']     \n",
      "                                                                                                  \n",
      " conv3_block3_2_bn (BatchNormal  (None, 28, 28, 128)  512        ['conv3_block3_2_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv3_block3_2_relu (Activatio  (None, 28, 28, 128)  0          ['conv3_block3_2_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv3_block3_3_conv (Conv2D)   (None, 28, 28, 512)  66048       ['conv3_block3_2_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv3_block3_out (Add)         (None, 28, 28, 512)  0           ['conv3_block2_out[0][0]',       \n",
      "                                                                  'conv3_block3_3_conv[0][0]']    \n",
      "                                                                                                  \n",
      " conv3_block4_preact_bn (BatchN  (None, 28, 28, 512)  2048       ['conv3_block3_out[0][0]']       \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " conv3_block4_preact_relu (Acti  (None, 28, 28, 512)  0          ['conv3_block4_preact_bn[0][0]'] \n",
      " vation)                                                                                          \n",
      "                                                                                                  \n",
      " conv3_block4_1_conv (Conv2D)   (None, 28, 28, 128)  65536       ['conv3_block4_preact_relu[0][0]'\n",
      "                                                                 ]                                \n",
      "                                                                                                  \n",
      " conv3_block4_1_bn (BatchNormal  (None, 28, 28, 128)  512        ['conv3_block4_1_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv3_block4_1_relu (Activatio  (None, 28, 28, 128)  0          ['conv3_block4_1_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv3_block4_2_pad (ZeroPaddin  (None, 30, 30, 128)  0          ['conv3_block4_1_relu[0][0]']    \n",
      " g2D)                                                                                             \n",
      "                                                                                                  \n",
      " conv3_block4_2_conv (Conv2D)   (None, 14, 14, 128)  147456      ['conv3_block4_2_pad[0][0]']     \n",
      "                                                                                                  \n",
      " conv3_block4_2_bn (BatchNormal  (None, 14, 14, 128)  512        ['conv3_block4_2_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv3_block4_2_relu (Activatio  (None, 14, 14, 128)  0          ['conv3_block4_2_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " max_pooling2d_7 (MaxPooling2D)  (None, 14, 14, 512)  0          ['conv3_block3_out[0][0]']       \n",
      "                                                                                                  \n",
      " conv3_block4_3_conv (Conv2D)   (None, 14, 14, 512)  66048       ['conv3_block4_2_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv3_block4_out (Add)         (None, 14, 14, 512)  0           ['max_pooling2d_7[0][0]',        \n",
      "                                                                  'conv3_block4_3_conv[0][0]']    \n",
      "                                                                                                  \n",
      " conv4_block1_preact_bn (BatchN  (None, 14, 14, 512)  2048       ['conv3_block4_out[0][0]']       \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " conv4_block1_preact_relu (Acti  (None, 14, 14, 512)  0          ['conv4_block1_preact_bn[0][0]'] \n",
      " vation)                                                                                          \n",
      "                                                                                                  \n",
      " conv4_block1_1_conv (Conv2D)   (None, 14, 14, 256)  131072      ['conv4_block1_preact_relu[0][0]'\n",
      "                                                                 ]                                \n",
      "                                                                                                  \n",
      " conv4_block1_1_bn (BatchNormal  (None, 14, 14, 256)  1024       ['conv4_block1_1_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv4_block1_1_relu (Activatio  (None, 14, 14, 256)  0          ['conv4_block1_1_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv4_block1_2_pad (ZeroPaddin  (None, 16, 16, 256)  0          ['conv4_block1_1_relu[0][0]']    \n",
      " g2D)                                                                                             \n",
      "                                                                                                  \n",
      " conv4_block1_2_conv (Conv2D)   (None, 14, 14, 256)  589824      ['conv4_block1_2_pad[0][0]']     \n",
      "                                                                                                  \n",
      " conv4_block1_2_bn (BatchNormal  (None, 14, 14, 256)  1024       ['conv4_block1_2_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv4_block1_2_relu (Activatio  (None, 14, 14, 256)  0          ['conv4_block1_2_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv4_block1_0_conv (Conv2D)   (None, 14, 14, 1024  525312      ['conv4_block1_preact_relu[0][0]'\n",
      "                                )                                ]                                \n",
      "                                                                                                  \n",
      " conv4_block1_3_conv (Conv2D)   (None, 14, 14, 1024  263168      ['conv4_block1_2_relu[0][0]']    \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " conv4_block1_out (Add)         (None, 14, 14, 1024  0           ['conv4_block1_0_conv[0][0]',    \n",
      "                                )                                 'conv4_block1_3_conv[0][0]']    \n",
      "                                                                                                  \n",
      " conv4_block2_preact_bn (BatchN  (None, 14, 14, 1024  4096       ['conv4_block1_out[0][0]']       \n",
      " ormalization)                  )                                                                 \n",
      "                                                                                                  \n",
      " conv4_block2_preact_relu (Acti  (None, 14, 14, 1024  0          ['conv4_block2_preact_bn[0][0]'] \n",
      " vation)                        )                                                                 \n",
      "                                                                                                  \n",
      " conv4_block2_1_conv (Conv2D)   (None, 14, 14, 256)  262144      ['conv4_block2_preact_relu[0][0]'\n",
      "                                                                 ]                                \n",
      "                                                                                                  \n",
      " conv4_block2_1_bn (BatchNormal  (None, 14, 14, 256)  1024       ['conv4_block2_1_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv4_block2_1_relu (Activatio  (None, 14, 14, 256)  0          ['conv4_block2_1_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv4_block2_2_pad (ZeroPaddin  (None, 16, 16, 256)  0          ['conv4_block2_1_relu[0][0]']    \n",
      " g2D)                                                                                             \n",
      "                                                                                                  \n",
      " conv4_block2_2_conv (Conv2D)   (None, 14, 14, 256)  589824      ['conv4_block2_2_pad[0][0]']     \n",
      "                                                                                                  \n",
      " conv4_block2_2_bn (BatchNormal  (None, 14, 14, 256)  1024       ['conv4_block2_2_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv4_block2_2_relu (Activatio  (None, 14, 14, 256)  0          ['conv4_block2_2_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv4_block2_3_conv (Conv2D)   (None, 14, 14, 1024  263168      ['conv4_block2_2_relu[0][0]']    \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " conv4_block2_out (Add)         (None, 14, 14, 1024  0           ['conv4_block1_out[0][0]',       \n",
      "                                )                                 'conv4_block2_3_conv[0][0]']    \n",
      "                                                                                                  \n",
      " conv4_block3_preact_bn (BatchN  (None, 14, 14, 1024  4096       ['conv4_block2_out[0][0]']       \n",
      " ormalization)                  )                                                                 \n",
      "                                                                                                  \n",
      " conv4_block3_preact_relu (Acti  (None, 14, 14, 1024  0          ['conv4_block3_preact_bn[0][0]'] \n",
      " vation)                        )                                                                 \n",
      "                                                                                                  \n",
      " conv4_block3_1_conv (Conv2D)   (None, 14, 14, 256)  262144      ['conv4_block3_preact_relu[0][0]'\n",
      "                                                                 ]                                \n",
      "                                                                                                  \n",
      " conv4_block3_1_bn (BatchNormal  (None, 14, 14, 256)  1024       ['conv4_block3_1_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv4_block3_1_relu (Activatio  (None, 14, 14, 256)  0          ['conv4_block3_1_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv4_block3_2_pad (ZeroPaddin  (None, 16, 16, 256)  0          ['conv4_block3_1_relu[0][0]']    \n",
      " g2D)                                                                                             \n",
      "                                                                                                  \n",
      " conv4_block3_2_conv (Conv2D)   (None, 14, 14, 256)  589824      ['conv4_block3_2_pad[0][0]']     \n",
      "                                                                                                  \n",
      " conv4_block3_2_bn (BatchNormal  (None, 14, 14, 256)  1024       ['conv4_block3_2_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv4_block3_2_relu (Activatio  (None, 14, 14, 256)  0          ['conv4_block3_2_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv4_block3_3_conv (Conv2D)   (None, 14, 14, 1024  263168      ['conv4_block3_2_relu[0][0]']    \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " conv4_block3_out (Add)         (None, 14, 14, 1024  0           ['conv4_block2_out[0][0]',       \n",
      "                                )                                 'conv4_block3_3_conv[0][0]']    \n",
      "                                                                                                  \n",
      " conv4_block4_preact_bn (BatchN  (None, 14, 14, 1024  4096       ['conv4_block3_out[0][0]']       \n",
      " ormalization)                  )                                                                 \n",
      "                                                                                                  \n",
      " conv4_block4_preact_relu (Acti  (None, 14, 14, 1024  0          ['conv4_block4_preact_bn[0][0]'] \n",
      " vation)                        )                                                                 \n",
      "                                                                                                  \n",
      " conv4_block4_1_conv (Conv2D)   (None, 14, 14, 256)  262144      ['conv4_block4_preact_relu[0][0]'\n",
      "                                                                 ]                                \n",
      "                                                                                                  \n",
      " conv4_block4_1_bn (BatchNormal  (None, 14, 14, 256)  1024       ['conv4_block4_1_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv4_block4_1_relu (Activatio  (None, 14, 14, 256)  0          ['conv4_block4_1_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv4_block4_2_pad (ZeroPaddin  (None, 16, 16, 256)  0          ['conv4_block4_1_relu[0][0]']    \n",
      " g2D)                                                                                             \n",
      "                                                                                                  \n",
      " conv4_block4_2_conv (Conv2D)   (None, 14, 14, 256)  589824      ['conv4_block4_2_pad[0][0]']     \n",
      "                                                                                                  \n",
      " conv4_block4_2_bn (BatchNormal  (None, 14, 14, 256)  1024       ['conv4_block4_2_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv4_block4_2_relu (Activatio  (None, 14, 14, 256)  0          ['conv4_block4_2_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv4_block4_3_conv (Conv2D)   (None, 14, 14, 1024  263168      ['conv4_block4_2_relu[0][0]']    \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " conv4_block4_out (Add)         (None, 14, 14, 1024  0           ['conv4_block3_out[0][0]',       \n",
      "                                )                                 'conv4_block4_3_conv[0][0]']    \n",
      "                                                                                                  \n",
      " conv4_block5_preact_bn (BatchN  (None, 14, 14, 1024  4096       ['conv4_block4_out[0][0]']       \n",
      " ormalization)                  )                                                                 \n",
      "                                                                                                  \n",
      " conv4_block5_preact_relu (Acti  (None, 14, 14, 1024  0          ['conv4_block5_preact_bn[0][0]'] \n",
      " vation)                        )                                                                 \n",
      "                                                                                                  \n",
      " conv4_block5_1_conv (Conv2D)   (None, 14, 14, 256)  262144      ['conv4_block5_preact_relu[0][0]'\n",
      "                                                                 ]                                \n",
      "                                                                                                  \n",
      " conv4_block5_1_bn (BatchNormal  (None, 14, 14, 256)  1024       ['conv4_block5_1_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv4_block5_1_relu (Activatio  (None, 14, 14, 256)  0          ['conv4_block5_1_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv4_block5_2_pad (ZeroPaddin  (None, 16, 16, 256)  0          ['conv4_block5_1_relu[0][0]']    \n",
      " g2D)                                                                                             \n",
      "                                                                                                  \n",
      " conv4_block5_2_conv (Conv2D)   (None, 14, 14, 256)  589824      ['conv4_block5_2_pad[0][0]']     \n",
      "                                                                                                  \n",
      " conv4_block5_2_bn (BatchNormal  (None, 14, 14, 256)  1024       ['conv4_block5_2_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv4_block5_2_relu (Activatio  (None, 14, 14, 256)  0          ['conv4_block5_2_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv4_block5_3_conv (Conv2D)   (None, 14, 14, 1024  263168      ['conv4_block5_2_relu[0][0]']    \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " conv4_block5_out (Add)         (None, 14, 14, 1024  0           ['conv4_block4_out[0][0]',       \n",
      "                                )                                 'conv4_block5_3_conv[0][0]']    \n",
      "                                                                                                  \n",
      " conv4_block6_preact_bn (BatchN  (None, 14, 14, 1024  4096       ['conv4_block5_out[0][0]']       \n",
      " ormalization)                  )                                                                 \n",
      "                                                                                                  \n",
      " conv4_block6_preact_relu (Acti  (None, 14, 14, 1024  0          ['conv4_block6_preact_bn[0][0]'] \n",
      " vation)                        )                                                                 \n",
      "                                                                                                  \n",
      " conv4_block6_1_conv (Conv2D)   (None, 14, 14, 256)  262144      ['conv4_block6_preact_relu[0][0]'\n",
      "                                                                 ]                                \n",
      "                                                                                                  \n",
      " conv4_block6_1_bn (BatchNormal  (None, 14, 14, 256)  1024       ['conv4_block6_1_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv4_block6_1_relu (Activatio  (None, 14, 14, 256)  0          ['conv4_block6_1_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv4_block6_2_pad (ZeroPaddin  (None, 16, 16, 256)  0          ['conv4_block6_1_relu[0][0]']    \n",
      " g2D)                                                                                             \n",
      "                                                                                                  \n",
      " conv4_block6_2_conv (Conv2D)   (None, 7, 7, 256)    589824      ['conv4_block6_2_pad[0][0]']     \n",
      "                                                                                                  \n",
      " conv4_block6_2_bn (BatchNormal  (None, 7, 7, 256)   1024        ['conv4_block6_2_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv4_block6_2_relu (Activatio  (None, 7, 7, 256)   0           ['conv4_block6_2_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " max_pooling2d_8 (MaxPooling2D)  (None, 7, 7, 1024)  0           ['conv4_block5_out[0][0]']       \n",
      "                                                                                                  \n",
      " conv4_block6_3_conv (Conv2D)   (None, 7, 7, 1024)   263168      ['conv4_block6_2_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv4_block6_out (Add)         (None, 7, 7, 1024)   0           ['max_pooling2d_8[0][0]',        \n",
      "                                                                  'conv4_block6_3_conv[0][0]']    \n",
      "                                                                                                  \n",
      " conv5_block1_preact_bn (BatchN  (None, 7, 7, 1024)  4096        ['conv4_block6_out[0][0]']       \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " conv5_block1_preact_relu (Acti  (None, 7, 7, 1024)  0           ['conv5_block1_preact_bn[0][0]'] \n",
      " vation)                                                                                          \n",
      "                                                                                                  \n",
      " conv5_block1_1_conv (Conv2D)   (None, 7, 7, 512)    524288      ['conv5_block1_preact_relu[0][0]'\n",
      "                                                                 ]                                \n",
      "                                                                                                  \n",
      " conv5_block1_1_bn (BatchNormal  (None, 7, 7, 512)   2048        ['conv5_block1_1_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv5_block1_1_relu (Activatio  (None, 7, 7, 512)   0           ['conv5_block1_1_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv5_block1_2_pad (ZeroPaddin  (None, 9, 9, 512)   0           ['conv5_block1_1_relu[0][0]']    \n",
      " g2D)                                                                                             \n",
      "                                                                                                  \n",
      " conv5_block1_2_conv (Conv2D)   (None, 7, 7, 512)    2359296     ['conv5_block1_2_pad[0][0]']     \n",
      "                                                                                                  \n",
      " conv5_block1_2_bn (BatchNormal  (None, 7, 7, 512)   2048        ['conv5_block1_2_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv5_block1_2_relu (Activatio  (None, 7, 7, 512)   0           ['conv5_block1_2_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv5_block1_0_conv (Conv2D)   (None, 7, 7, 2048)   2099200     ['conv5_block1_preact_relu[0][0]'\n",
      "                                                                 ]                                \n",
      "                                                                                                  \n",
      " conv5_block1_3_conv (Conv2D)   (None, 7, 7, 2048)   1050624     ['conv5_block1_2_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv5_block1_out (Add)         (None, 7, 7, 2048)   0           ['conv5_block1_0_conv[0][0]',    \n",
      "                                                                  'conv5_block1_3_conv[0][0]']    \n",
      "                                                                                                  \n",
      " conv5_block2_preact_bn (BatchN  (None, 7, 7, 2048)  8192        ['conv5_block1_out[0][0]']       \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " conv5_block2_preact_relu (Acti  (None, 7, 7, 2048)  0           ['conv5_block2_preact_bn[0][0]'] \n",
      " vation)                                                                                          \n",
      "                                                                                                  \n",
      " conv5_block2_1_conv (Conv2D)   (None, 7, 7, 512)    1048576     ['conv5_block2_preact_relu[0][0]'\n",
      "                                                                 ]                                \n",
      "                                                                                                  \n",
      " conv5_block2_1_bn (BatchNormal  (None, 7, 7, 512)   2048        ['conv5_block2_1_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv5_block2_1_relu (Activatio  (None, 7, 7, 512)   0           ['conv5_block2_1_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv5_block2_2_pad (ZeroPaddin  (None, 9, 9, 512)   0           ['conv5_block2_1_relu[0][0]']    \n",
      " g2D)                                                                                             \n",
      "                                                                                                  \n",
      " conv5_block2_2_conv (Conv2D)   (None, 7, 7, 512)    2359296     ['conv5_block2_2_pad[0][0]']     \n",
      "                                                                                                  \n",
      " conv5_block2_2_bn (BatchNormal  (None, 7, 7, 512)   2048        ['conv5_block2_2_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv5_block2_2_relu (Activatio  (None, 7, 7, 512)   0           ['conv5_block2_2_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv5_block2_3_conv (Conv2D)   (None, 7, 7, 2048)   1050624     ['conv5_block2_2_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv5_block2_out (Add)         (None, 7, 7, 2048)   0           ['conv5_block1_out[0][0]',       \n",
      "                                                                  'conv5_block2_3_conv[0][0]']    \n",
      "                                                                                                  \n",
      " conv5_block3_preact_bn (BatchN  (None, 7, 7, 2048)  8192        ['conv5_block2_out[0][0]']       \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " conv5_block3_preact_relu (Acti  (None, 7, 7, 2048)  0           ['conv5_block3_preact_bn[0][0]'] \n",
      " vation)                                                                                          \n",
      "                                                                                                  \n",
      " conv5_block3_1_conv (Conv2D)   (None, 7, 7, 512)    1048576     ['conv5_block3_preact_relu[0][0]'\n",
      "                                                                 ]                                \n",
      "                                                                                                  \n",
      " conv5_block3_1_bn (BatchNormal  (None, 7, 7, 512)   2048        ['conv5_block3_1_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv5_block3_1_relu (Activatio  (None, 7, 7, 512)   0           ['conv5_block3_1_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv5_block3_2_pad (ZeroPaddin  (None, 9, 9, 512)   0           ['conv5_block3_1_relu[0][0]']    \n",
      " g2D)                                                                                             \n",
      "                                                                                                  \n",
      " conv5_block3_2_conv (Conv2D)   (None, 7, 7, 512)    2359296     ['conv5_block3_2_pad[0][0]']     \n",
      "                                                                                                  \n",
      " conv5_block3_2_bn (BatchNormal  (None, 7, 7, 512)   2048        ['conv5_block3_2_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " input_6 (InputLayer)           [(None, 31)]         0           []                               \n",
      "                                                                                                  \n",
      " conv5_block3_2_relu (Activatio  (None, 7, 7, 512)   0           ['conv5_block3_2_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " dense_15 (Dense)               (None, 128)          4096        ['input_6[0][0]']                \n",
      "                                                                                                  \n",
      " conv5_block3_3_conv (Conv2D)   (None, 7, 7, 2048)   1050624     ['conv5_block3_2_relu[0][0]']    \n",
      "                                                                                                  \n",
      " batch_normalization_13 (BatchN  (None, 128)         512         ['dense_15[0][0]']               \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " conv5_block3_out (Add)         (None, 7, 7, 2048)   0           ['conv5_block2_out[0][0]',       \n",
      "                                                                  'conv5_block3_3_conv[0][0]']    \n",
      "                                                                                                  \n",
      " dropout_13 (Dropout)           (None, 128)          0           ['batch_normalization_13[0][0]'] \n",
      "                                                                                                  \n",
      " post_bn (BatchNormalization)   (None, 7, 7, 2048)   8192        ['conv5_block3_out[0][0]']       \n",
      "                                                                                                  \n",
      " dense_16 (Dense)               (None, 256)          33024       ['dropout_13[0][0]']             \n",
      "                                                                                                  \n",
      " post_relu (Activation)         (None, 7, 7, 2048)   0           ['post_bn[0][0]']                \n",
      "                                                                                                  \n",
      " batch_normalization_14 (BatchN  (None, 256)         1024        ['dense_16[0][0]']               \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " global_average_pooling2d_2 (Gl  (None, 2048)        0           ['post_relu[0][0]']              \n",
      " obalAveragePooling2D)                                                                            \n",
      "                                                                                                  \n",
      " dropout_14 (Dropout)           (None, 256)          0           ['batch_normalization_14[0][0]'] \n",
      "                                                                                                  \n",
      " dense_14 (Dense)               (None, 256)          524544      ['global_average_pooling2d_2[0][0\n",
      "                                                                 ]']                              \n",
      "                                                                                                  \n",
      " dense_17 (Dense)               (None, 128)          32896       ['dropout_14[0][0]']             \n",
      "                                                                                                  \n",
      " batch_normalization_12 (BatchN  (None, 256)         1024        ['dense_14[0][0]']               \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " batch_normalization_15 (BatchN  (None, 128)         512         ['dense_17[0][0]']               \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " dropout_12 (Dropout)           (None, 256)          0           ['batch_normalization_12[0][0]'] \n",
      "                                                                                                  \n",
      " dropout_15 (Dropout)           (None, 128)          0           ['batch_normalization_15[0][0]'] \n",
      "                                                                                                  \n",
      " concatenate_2 (Concatenate)    (None, 384)          0           ['dropout_12[0][0]',             \n",
      "                                                                  'dropout_15[0][0]']             \n",
      "                                                                                                  \n",
      " dense_18 (Dense)               (None, 256)          98560       ['concatenate_2[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_16 (BatchN  (None, 256)         1024        ['dense_18[0][0]']               \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " dropout_16 (Dropout)           (None, 256)          0           ['batch_normalization_16[0][0]'] \n",
      "                                                                                                  \n",
      " dense_19 (Dense)               (None, 128)          32896       ['dropout_16[0][0]']             \n",
      "                                                                                                  \n",
      " batch_normalization_17 (BatchN  (None, 128)         512         ['dense_19[0][0]']               \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " dropout_17 (Dropout)           (None, 128)          0           ['batch_normalization_17[0][0]'] \n",
      "                                                                                                  \n",
      " dense_20 (Dense)               (None, 1)            129         ['dropout_17[0][0]']             \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 24,295,553\n",
      "Trainable params: 728,449\n",
      "Non-trainable params: 23,567,104\n",
      "__________________________________________________________________________________________________\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "module 'pydot' has no attribute 'InvocationException'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "File \u001b[1;32m~\\.conda\\envs\\tf-gpu\\lib\\site-packages\\pydot\\core.py:1851\u001b[0m, in \u001b[0;36mDot.create\u001b[1;34m(self, prog, format, encoding)\u001b[0m\n\u001b[0;32m   1850\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1851\u001b[0m     stdout_data, stderr_data, process \u001b[38;5;241m=\u001b[39m \u001b[43mcall_graphviz\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1852\u001b[0m \u001b[43m        \u001b[49m\u001b[43mprogram\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprog\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1853\u001b[0m \u001b[43m        \u001b[49m\u001b[43marguments\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43marguments\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1854\u001b[0m \u001b[43m        \u001b[49m\u001b[43mworking_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtmp_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1855\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1856\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[1;32m~\\.conda\\envs\\tf-gpu\\lib\\site-packages\\pydot\\core.py:211\u001b[0m, in \u001b[0;36mcall_graphviz\u001b[1;34m(program, arguments, working_dir, **kwargs)\u001b[0m\n\u001b[0;32m    209\u001b[0m program_with_args \u001b[38;5;241m=\u001b[39m [program] \u001b[38;5;241m+\u001b[39m arguments\n\u001b[1;32m--> 211\u001b[0m process \u001b[38;5;241m=\u001b[39m subprocess\u001b[38;5;241m.\u001b[39mPopen(\n\u001b[0;32m    212\u001b[0m     program_with_args,\n\u001b[0;32m    213\u001b[0m     env\u001b[38;5;241m=\u001b[39menv,\n\u001b[0;32m    214\u001b[0m     cwd\u001b[38;5;241m=\u001b[39mworking_dir,\n\u001b[0;32m    215\u001b[0m     shell\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    216\u001b[0m     stderr\u001b[38;5;241m=\u001b[39msubprocess\u001b[38;5;241m.\u001b[39mPIPE,\n\u001b[0;32m    217\u001b[0m     stdout\u001b[38;5;241m=\u001b[39msubprocess\u001b[38;5;241m.\u001b[39mPIPE,\n\u001b[0;32m    218\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m    219\u001b[0m )\n\u001b[0;32m    220\u001b[0m stdout_data, stderr_data \u001b[38;5;241m=\u001b[39m process\u001b[38;5;241m.\u001b[39mcommunicate()\n",
      "File \u001b[1;32m~\\.conda\\envs\\tf-gpu\\lib\\subprocess.py:951\u001b[0m, in \u001b[0;36mPopen.__init__\u001b[1;34m(self, args, bufsize, executable, stdin, stdout, stderr, preexec_fn, close_fds, shell, cwd, env, universal_newlines, startupinfo, creationflags, restore_signals, start_new_session, pass_fds, user, group, extra_groups, encoding, errors, text, umask)\u001b[0m\n\u001b[0;32m    948\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstderr \u001b[38;5;241m=\u001b[39m io\u001b[38;5;241m.\u001b[39mTextIOWrapper(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstderr,\n\u001b[0;32m    949\u001b[0m                     encoding\u001b[38;5;241m=\u001b[39mencoding, errors\u001b[38;5;241m=\u001b[39merrors)\n\u001b[1;32m--> 951\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execute_child\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexecutable\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpreexec_fn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclose_fds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    952\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mpass_fds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcwd\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    953\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mstartupinfo\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreationflags\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshell\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    954\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mp2cread\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mp2cwrite\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    955\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mc2pread\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mc2pwrite\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    956\u001b[0m \u001b[43m                        \u001b[49m\u001b[43merrread\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merrwrite\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    957\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mrestore_signals\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    958\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mgid\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muid\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mumask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    959\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mstart_new_session\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    960\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m:\n\u001b[0;32m    961\u001b[0m     \u001b[38;5;66;03m# Cleanup if the child failed starting.\u001b[39;00m\n",
      "File \u001b[1;32m~\\.conda\\envs\\tf-gpu\\lib\\subprocess.py:1436\u001b[0m, in \u001b[0;36mPopen._execute_child\u001b[1;34m(self, args, executable, preexec_fn, close_fds, pass_fds, cwd, env, startupinfo, creationflags, shell, p2cread, p2cwrite, c2pread, c2pwrite, errread, errwrite, unused_restore_signals, unused_gid, unused_gids, unused_uid, unused_umask, unused_start_new_session)\u001b[0m\n\u001b[0;32m   1435\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1436\u001b[0m     hp, ht, pid, tid \u001b[38;5;241m=\u001b[39m \u001b[43m_winapi\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mCreateProcess\u001b[49m\u001b[43m(\u001b[49m\u001b[43mexecutable\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1437\u001b[0m \u001b[43m                             \u001b[49m\u001b[38;5;66;43;03m# no special security\u001b[39;49;00m\n\u001b[0;32m   1438\u001b[0m \u001b[43m                             \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m   1439\u001b[0m \u001b[43m                             \u001b[49m\u001b[38;5;28;43mint\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mclose_fds\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1440\u001b[0m \u001b[43m                             \u001b[49m\u001b[43mcreationflags\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1441\u001b[0m \u001b[43m                             \u001b[49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1442\u001b[0m \u001b[43m                             \u001b[49m\u001b[43mcwd\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1443\u001b[0m \u001b[43m                             \u001b[49m\u001b[43mstartupinfo\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1444\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m   1445\u001b[0m     \u001b[38;5;66;03m# Child is launched. Close the parent's copy of those pipe\u001b[39;00m\n\u001b[0;32m   1446\u001b[0m     \u001b[38;5;66;03m# handles that only the child should have open.  You need\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1449\u001b[0m     \u001b[38;5;66;03m# pipe will not close when the child process exits and the\u001b[39;00m\n\u001b[0;32m   1450\u001b[0m     \u001b[38;5;66;03m# ReadFile will hang.\u001b[39;00m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [WinError 2] The system cannot find the file specified",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "File \u001b[1;32m~\\.conda\\envs\\tf-gpu\\lib\\site-packages\\keras\\utils\\vis_utils.py:57\u001b[0m, in \u001b[0;36mcheck_graphviz\u001b[1;34m()\u001b[0m\n\u001b[0;32m     54\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     55\u001b[0m     \u001b[38;5;66;03m# Attempt to create an image of a blank graph\u001b[39;00m\n\u001b[0;32m     56\u001b[0m     \u001b[38;5;66;03m# to check the pydot/graphviz installation.\u001b[39;00m\n\u001b[1;32m---> 57\u001b[0m     \u001b[43mpydot\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mDot\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpydot\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mDot\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     58\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[1;32m~\\.conda\\envs\\tf-gpu\\lib\\site-packages\\pydot\\core.py:1860\u001b[0m, in \u001b[0;36mDot.create\u001b[1;34m(self, prog, format, encoding)\u001b[0m\n\u001b[0;32m   1859\u001b[0m     args[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mprog\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m not found in path.\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m-> 1860\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m(\u001b[38;5;241m*\u001b[39margs)\n\u001b[0;32m   1861\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [WinError 2] \"dot\" not found in path.",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 385\u001b[0m\n\u001b[0;32m    382\u001b[0m model\u001b[38;5;241m.\u001b[39msummary()\n\u001b[0;32m    384\u001b[0m \u001b[38;5;66;03m# Save model architecture as image\u001b[39;00m\n\u001b[1;32m--> 385\u001b[0m \u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkeras\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mutils\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mplot_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mto_file\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmodel_architecture.png\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshow_shapes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshow_layer_names\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m    386\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mModel architecture has been saved as \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmodel_architecture.png\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    388\u001b[0m \u001b[38;5;66;03m# Calculate and output additional metrics\u001b[39;00m\n\u001b[0;32m    389\u001b[0m \u001b[38;5;66;03m# ROC AUC\u001b[39;00m\n",
      "File \u001b[1;32m~\\.conda\\envs\\tf-gpu\\lib\\site-packages\\keras\\utils\\vis_utils.py:436\u001b[0m, in \u001b[0;36mplot_model\u001b[1;34m(model, to_file, show_shapes, show_dtype, show_layer_names, rankdir, expand_nested, dpi, layer_range, show_layer_activations)\u001b[0m\n\u001b[0;32m    429\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m model\u001b[38;5;241m.\u001b[39mbuilt:\n\u001b[0;32m    430\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    431\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThis model has not yet been built. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    432\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBuild the model first by calling `build()` or by calling \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    433\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mthe model on a batch of data.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    434\u001b[0m     )\n\u001b[1;32m--> 436\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[43mcheck_graphviz\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[0;32m    437\u001b[0m     message \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    438\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou must install pydot (`pip install pydot`) \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    439\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mand install graphviz \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    440\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m(see instructions at https://graphviz.gitlab.io/download/) \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    441\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfor plot_model to work.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    442\u001b[0m     )\n\u001b[0;32m    443\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIPython.core.magics.namespace\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m sys\u001b[38;5;241m.\u001b[39mmodules:\n\u001b[0;32m    444\u001b[0m         \u001b[38;5;66;03m# We don't raise an exception here in order to avoid crashing\u001b[39;00m\n\u001b[0;32m    445\u001b[0m         \u001b[38;5;66;03m# notebook tests where graphviz is not available.\u001b[39;00m\n",
      "File \u001b[1;32m~\\.conda\\envs\\tf-gpu\\lib\\site-packages\\keras\\utils\\vis_utils.py:59\u001b[0m, in \u001b[0;36mcheck_graphviz\u001b[1;34m()\u001b[0m\n\u001b[0;32m     57\u001b[0m     pydot\u001b[38;5;241m.\u001b[39mDot\u001b[38;5;241m.\u001b[39mcreate(pydot\u001b[38;5;241m.\u001b[39mDot())\n\u001b[0;32m     58\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m---> 59\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mOSError\u001b[39;00m, \u001b[43mpydot\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mInvocationException\u001b[49m):\n\u001b[0;32m     60\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "\u001b[1;31mAttributeError\u001b[0m: module 'pydot' has no attribute 'InvocationException'"
     ]
    }
   ],
   "source": [
    "                                                       ##### Asle jens ine ###########\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models, Input, mixed_precision\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import confusion_matrix, classification_report, f1_score, roc_auc_score, precision_recall_curve, average_precision_score, accuracy_score, roc_curve\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Load the DataFrame\n",
    "file_path = r\"C:\\Users\\meh91075\\Downloads\\AutoSplit_all_in_one_dataset_05112024.csv\"\n",
    "#\"C:\\Users\\meh91075\\Downloads\\merged_features_filtered.csv\"\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# Entferne label_standard\n",
    "df = df.drop('label_standard', axis=1)\n",
    "\n",
    "\n",
    "print(f\"Original CSV data loaded. Number of rows: {len(df)}\")\n",
    "print(f\"Original distribution of labels:\")\n",
    "print(df['label_majority'].value_counts())\n",
    "\n",
    "# Optimize GPU configuration\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    try:\n",
    "        for gpu in gpus:\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)\n",
    "        logical_gpus = tf.config.experimental.list_logical_devices('GPU')\n",
    "        print(f\"{len(gpus)} Physical GPUs, {len(logical_gpus)} Logical GPUs\")\n",
    "    except RuntimeError as e:\n",
    "        print(e)\n",
    "\n",
    "# Activate Mixed Precision\n",
    "if tf.test.is_built_with_cuda():\n",
    "    policy = mixed_precision.Policy('mixed_float16')\n",
    "    mixed_precision.set_global_policy(policy)\n",
    "    print(\"Mixed precision policy set to mixed_float16\")\n",
    "else:\n",
    "    print(\"CUDA not available, using default precision\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Pfade und Konfiguration\n",
    "base_dir =  r\"C:\\Users\\meh91075\\Desktop\\DataSet(S1)\"\n",
    "filtered_dir = os.path.join(base_dir, 'New_filtered_data_label0')\n",
    "non_filtered_dir = os.path.join(base_dir, 'New_non_filtered_data')\n",
    "\n",
    "\n",
    "\n",
    "img_height, img_width = 224, 224\n",
    "batch_size = 32\n",
    "\n",
    "def get_image_paths(directory):\n",
    "    return [os.path.join(root, file) for root, _, files in os.walk(directory) \n",
    "            for file in files if file.lower().endswith(('.png', '.jpg', '.jpeg'))]\n",
    "\n",
    "filtered_images = get_image_paths(filtered_dir)\n",
    "non_filtered_images = get_image_paths(non_filtered_dir)\n",
    "print(f\"Found images: Filtered: {len(filtered_images)}, Not filtered: {len(non_filtered_images)}\")\n",
    "\n",
    "def extract_filename(path):\n",
    "    basename = os.path.basename(path)\n",
    "    return basename.replace('_ISO', '').replace('_ISOAlt', '').split('.')[0]\n",
    "\n",
    "image_df = pd.DataFrame({\n",
    "    'image_path': filtered_images + non_filtered_images,\n",
    "    'label': [1] * len(filtered_images) + [0] * len(non_filtered_images),\n",
    "    'filename': [extract_filename(path) for path in filtered_images + non_filtered_images]\n",
    "})\n",
    "\n",
    "print(\"Sample of extracted filenames:\")\n",
    "print(image_df['filename'].head())\n",
    "\n",
    "# Adjusted merge process\n",
    "merged_df = pd.merge(df, image_df[['filename', 'image_path']], on='filename', how='left')\n",
    "merged_df['label'] = merged_df['label_majority'].map({1: 0, 2: 0, 3: 0, 4: 1})\n",
    "merged_df['has_image'] = merged_df['image_path'].notna()\n",
    "\n",
    "# Filter only entries with associated images\n",
    "merged_df_with_images = merged_df[merged_df['has_image']]\n",
    "\n",
    "# Undersampling of the majority class\n",
    "majority_class = merged_df_with_images[merged_df_with_images['label'] == 0]\n",
    "minority_class = merged_df_with_images[merged_df_with_images['label'] == 1]\n",
    "\n",
    "majority_downsampled = majority_class.sample(n=2000, random_state=42)\n",
    "balanced_df = pd.concat([majority_downsampled, minority_class])\n",
    "\n",
    "print(f\"After undersampling: Number of rows: {len(balanced_df)}\")\n",
    "print(\"Distribution of labels after undersampling:\")\n",
    "print(balanced_df['label'].value_counts())\n",
    "\n",
    "# Train-Test-Split\n",
    "train_df, val_df = train_test_split(balanced_df, test_size=0.2, stratify=balanced_df['label'], random_state=42)\n",
    "print(f\"Training data: {len(train_df)}, Validation data: {len(val_df)}\")\n",
    "\n",
    "# Determine the numeric columns\n",
    "numeric_columns = df.select_dtypes(include=[np.number]).columns.tolist()\n",
    "numeric_columns = [col for col in numeric_columns if col not in ['label_majority', 'label']]\n",
    "\n",
    "# Normalization of numeric data\n",
    "scaler = StandardScaler()\n",
    "train_df[numeric_columns] = scaler.fit_transform(train_df[numeric_columns])\n",
    "val_df[numeric_columns] = scaler.transform(val_df[numeric_columns])\n",
    "\n",
    "# Optimized data generator function with data augmentation\n",
    "def create_dataset(dataframe, is_training=True):\n",
    "    def parse_function(filename, label, *numeric_data):\n",
    "        img = tf.io.read_file(filename)\n",
    "        img = tf.image.decode_jpeg(img, channels=3)\n",
    "        img = tf.image.resize(img, [img_height, img_width])\n",
    "        img = tf.keras.applications.resnet_v2.preprocess_input(img)\n",
    "        \n",
    "        if is_training:\n",
    "            img = tf.image.random_flip_left_right(img)\n",
    "            img = tf.image.random_brightness(img, max_delta=0.2)\n",
    "            img = tf.image.random_contrast(img, lower=0.8, upper=1.2)\n",
    "            img = tf.image.random_saturation(img, lower=0.8, upper=1.2)\n",
    "            img = tf.image.random_hue(img, max_delta=0.1)\n",
    "        \n",
    "        numeric_data = tf.convert_to_tensor(numeric_data, dtype=tf.float32)\n",
    "        numeric_data = tf.squeeze(numeric_data)\n",
    "        return (img, numeric_data), label\n",
    "\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((\n",
    "        dataframe['image_path'].values,\n",
    "        dataframe['label'].values,\n",
    "        dataframe[numeric_columns].values\n",
    "    ))\n",
    "    \n",
    "    dataset = dataset.map(parse_function, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "    \n",
    "    if is_training:\n",
    "        dataset = dataset.shuffle(buffer_size=len(dataframe)).repeat()\n",
    "    else:\n",
    "        dataset = dataset.repeat()\n",
    "    \n",
    "    dataset = dataset.batch(batch_size)\n",
    "    dataset = dataset.prefetch(tf.data.AUTOTUNE)\n",
    "    return dataset\n",
    "\n",
    "# Create the datasets\n",
    "train_dataset = create_dataset(train_df)\n",
    "val_dataset = create_dataset(val_df, is_training=False)\n",
    "\n",
    "# Calculate steps per epoch\n",
    "steps_per_epoch = len(train_df) // batch_size\n",
    "validation_steps = len(val_df) // batch_size\n",
    "\n",
    "# Optimized model definition\n",
    "def create_optimized_hybrid_model(img_height, img_width, num_numeric_features):\n",
    "    # Image processing branch (CNN)\n",
    "    img_input = Input(shape=(img_height, img_width, 3))\n",
    "    \n",
    "    # Use a pre-trained model for feature extraction\n",
    "    base_model = tf.keras.applications.ResNet50V2(include_top=False, weights='imagenet', input_tensor=img_input)\n",
    "    \n",
    "    # Freeze the pre-trained layers\n",
    "    base_model.trainable = False\n",
    "    \n",
    "    x = base_model.output\n",
    "    x = layers.GlobalAveragePooling2D()(x)\n",
    "    x = layers.Dense(256, activation='relu')(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.Dropout(0.5)(x)\n",
    "    \n",
    "    # Numerical data branch (MLP)\n",
    "    num_input = Input(shape=(num_numeric_features,))\n",
    "    y = layers.Dense(128, activation='relu')(num_input)\n",
    "    y = layers.BatchNormalization()(y)\n",
    "    y = layers.Dropout(0.3)(y)\n",
    "    y = layers.Dense(256, activation='relu')(y)\n",
    "    y = layers.BatchNormalization()(y)\n",
    "    y = layers.Dropout(0.4)(y)\n",
    "    y = layers.Dense(128, activation='relu')(y)\n",
    "    y = layers.BatchNormalization()(y)\n",
    "    y = layers.Dropout(0.3)(y)\n",
    "    \n",
    "    # Combining the branches\n",
    "    combined = layers.concatenate([x, y])\n",
    "    \n",
    "    # Final layers\n",
    "    z = layers.Dense(256, activation='relu')(combined)\n",
    "    z = layers.BatchNormalization()(z)\n",
    "    z = layers.Dropout(0.5)(z)\n",
    "    z = layers.Dense(128, activation='relu')(z)\n",
    "    z = layers.BatchNormalization()(z)\n",
    "    z = layers.Dropout(0.5)(z)\n",
    "    \n",
    "    # Output layer\n",
    "    output = layers.Dense(1, activation='sigmoid')(z)\n",
    "    \n",
    "    model = models.Model(inputs=[img_input, num_input], outputs=output)\n",
    "    return model\n",
    "\n",
    "# Learning Rate Logger Callback\n",
    "class LearningRateLogger(tf.keras.callbacks.Callback):\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        if logs is not None:\n",
    "            lr = self.model.optimizer.learning_rate\n",
    "            if isinstance(lr, tf.keras.optimizers.schedules.LearningRateSchedule):\n",
    "                lr = lr(self.model.optimizer.iterations)\n",
    "            logs['lr'] = tf.keras.backend.get_value(lr)\n",
    "\n",
    "# Custom ReduceLROnPlateau Callback\n",
    "class CustomReduceLROnPlateau(tf.keras.callbacks.Callback):\n",
    "    def __init__(self, factor=0.1, patience=10, min_lr=0, monitor='val_loss', mode='min'):\n",
    "        super(CustomReduceLROnPlateau, self).__init__()\n",
    "        self.factor = factor\n",
    "        self.patience = patience\n",
    "        self.min_lr = min_lr\n",
    "        self.monitor = monitor\n",
    "        self.mode = mode\n",
    "        self.best = np.Inf if mode == 'min' else -np.Inf\n",
    "        self.wait = 0\n",
    "        self.best_weights = None\n",
    "\n",
    "    def on_train_begin(self, logs=None):\n",
    "        self.best = np.Inf if self.mode == 'min' else -np.Inf\n",
    "        self.wait = 0\n",
    "        self.best_weights = None\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        logs = logs or {}\n",
    "        current = logs.get(self.monitor)\n",
    "        if current is None:\n",
    "            return\n",
    "        \n",
    "        if self.mode == 'min' and current < self.best:\n",
    "            self.best = current\n",
    "            self.wait = 0\n",
    "            self.best_weights = self.model.get_weights()\n",
    "        elif self.mode == 'max' and current > self.best:\n",
    "            self.best = current\n",
    "            self.wait = 0\n",
    "            self.best_weights = self.model.get_weights()\n",
    "        else:\n",
    "            self.wait += 1\n",
    "        \n",
    "        if self.wait >= self.patience:\n",
    "            current_lr = self.model.optimizer.learning_rate\n",
    "            if isinstance(current_lr, tf.keras.optimizers.schedules.LearningRateSchedule):\n",
    "                current_lr = current_lr(self.model.optimizer.iterations)\n",
    "            current_lr = float(current_lr.numpy())\n",
    "            \n",
    "            if current_lr > self.min_lr:\n",
    "                new_lr = max(current_lr * self.factor, self.min_lr)\n",
    "                new_lr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(\n",
    "                    initial_learning_rate=new_lr,\n",
    "                    decay_steps=1000,\n",
    "                    decay_rate=0.9,\n",
    "                    staircase=True\n",
    "                )\n",
    "                self.model.optimizer.learning_rate = new_lr_schedule\n",
    "                print(f'\\nEpoch {epoch+1}: ReduceLROnPlateau reducing learning rate to {new_lr}.')\n",
    "                self.wait = 0\n",
    "                self.model.set_weights(self.best_weights)\n",
    "\n",
    "# Compile and train\n",
    "initial_learning_rate = 1e-3\n",
    "lr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(\n",
    "    initial_learning_rate, decay_steps=1000, decay_rate=0.9, staircase=True\n",
    ")\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=lr_schedule)\n",
    "\n",
    "# Multi-GPU Strategy\n",
    "strategy = tf.distribute.MirroredStrategy()\n",
    "with strategy.scope():\n",
    "    model = create_optimized_hybrid_model(img_height, img_width, len(numeric_columns))\n",
    "    model.compile(\n",
    "        optimizer=optimizer,\n",
    "        loss='binary_crossentropy',\n",
    "        metrics=['accuracy', tf.keras.metrics.AUC(name='auc'), tf.keras.metrics.Precision(), tf.keras.metrics.Recall()]\n",
    "    )\n",
    "\n",
    "# Callbacks\n",
    "reduce_lr = CustomReduceLROnPlateau(monitor='val_auc', factor=0.2, patience=5, min_lr=1e-6, mode='max')\n",
    "model_checkpoint = tf.keras.callbacks.ModelCheckpoint('best_model.h5', save_best_only=True, monitor='val_auc', mode='max')\n",
    "lr_logger = LearningRateLogger()\n",
    "\n",
    "# Training\n",
    "history = model.fit(\n",
    "    train_dataset,\n",
    "    epochs=100,\n",
    "    steps_per_epoch=steps_per_epoch,\n",
    "    validation_data=val_dataset,\n",
    "    validation_steps=validation_steps,\n",
    "    callbacks=[reduce_lr, model_checkpoint, lr_logger]\n",
    ")\n",
    "\n",
    "# Save model\n",
    "model.save('hybrid_model_optimized_final.h5')\n",
    "\n",
    "# Evaluation\n",
    "val_loss, val_accuracy, val_auc, val_precision, val_recall = model.evaluate(val_dataset, steps=validation_steps)\n",
    "print(f\"Validation Loss: {val_loss:.4f}\")\n",
    "print(f\"Validation Accuracy: {val_accuracy:.4f}\")\n",
    "print(f\"Validation AUC: {val_auc:.4f}\")\n",
    "print(f\"Validation Precision: {val_precision:.4f}\")\n",
    "print(f\"Validation Recall: {val_recall:.4f}\")\n",
    "\n",
    "# Predictions for the validation dataset\n",
    "y_pred = model.predict(val_dataset, steps=validation_steps)\n",
    "y_pred_classes = (y_pred > 0.5).astype(\"int32\")\n",
    "y_true = np.concatenate([y for _, y in val_dataset.take(validation_steps)], axis=0)\n",
    "\n",
    "# Confusion matrix and classification report\n",
    "conf_matrix = confusion_matrix(y_true, y_pred_classes)\n",
    "class_report = classification_report(y_true, y_pred_classes)\n",
    "\n",
    "print(\"Confusion Matrix:\")\n",
    "print(conf_matrix)\n",
    "print(\"\\nClassification Report:\")\n",
    "print(class_report)\n",
    "\n",
    "# Visualization of training history\n",
    "plt.figure(figsize=(15, 5))\n",
    "plt.subplot(1, 3, 1)\n",
    "plt.plot(history.history['accuracy'], label='Training Accuracy')\n",
    "plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
    "plt.title('Model Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(1, 3, 2)\n",
    "plt.plot(history.history['loss'], label='Training Loss')\n",
    "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "plt.title('Model Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(1, 3, 3)\n",
    "plt.plot(history.history['auc'], label='Training AUC')\n",
    "plt.plot(history.history['val_auc'], label='Validation AUC')\n",
    "plt.title('Model AUC')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('AUC')\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('training_history.png')\n",
    "plt.close()\n",
    "\n",
    "# Visualization of learning rate\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(history.history['lr'])\n",
    "plt.title('Learning Rate over Epochs')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Learning Rate')\n",
    "plt.yscale('log')\n",
    "plt.savefig('learning_rate.png')\n",
    "plt.close()\n",
    "\n",
    "# Visualization of confusion matrix\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.xlabel('Predicted Label')\n",
    "plt.ylabel('True Label')\n",
    "plt.savefig('confusion_matrix.png')\n",
    "plt.close()\n",
    "\n",
    "print(\"Visualizations have been saved as PNG files.\")\n",
    "\n",
    "# Save predictions and true labels\n",
    "predictions_df = pd.DataFrame({\n",
    "    'True_Label': y_true,\n",
    "    'Predicted_Probability': y_pred.flatten(),\n",
    "    'Predicted_Class': y_pred_classes.flatten()\n",
    "})\n",
    "predictions_df.to_csv('predictions.csv', index=False)\n",
    "print(\"Predictions have been saved in 'predictions.csv'.\")\n",
    "\n",
    "# Model summary\n",
    "model.summary()\n",
    "\n",
    "# Save model architecture as image\n",
    "tf.keras.utils.plot_model(model, to_file='model_architecture.png', show_shapes=True, show_layer_names=True)\n",
    "print(\"Model architecture has been saved as 'model_architecture.png'.\")\n",
    "\n",
    "# Calculate and output additional metrics\n",
    "# ROC AUC\n",
    "roc_auc = roc_auc_score(y_true, y_pred)\n",
    "print(f\"ROC AUC: {roc_auc:.4f}\")\n",
    "\n",
    "# Precision-Recall AUC\n",
    "pr_auc = average_precision_score(y_true, y_pred)\n",
    "print(f\"Precision-Recall AUC: {pr_auc:.4f}\")\n",
    "\n",
    "# F1-Score at various thresholds\n",
    "thresholds = np.arange(0, 1.01, 0.01)\n",
    "f1_scores = [f1_score(y_true, y_pred > threshold) for threshold in thresholds]\n",
    "best_threshold = thresholds[np.argmax(f1_scores)]\n",
    "best_f1 = max(f1_scores)\n",
    "print(f\"Best F1-Score: {best_f1:.4f} at threshold {best_threshold:.2f}\")\n",
    "\n",
    "# Visualization of Precision-Recall curve\n",
    "precision, recall, _ = precision_recall_curve(y_true, y_pred)\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(recall, precision, marker='.')\n",
    "plt.title('Precision-Recall Curve')\n",
    "plt.xlabel('Recall')\n",
    "plt.ylabel('Precision')\n",
    "plt.savefig('precision_recall_curve.png')\n",
    "plt.close()\n",
    "print(\"Precision-Recall curve has been saved as 'precision_recall_curve.png'.\")\n",
    "\n",
    "# Visualization of ROC curve\n",
    "fpr, tpr, _ = roc_curve(y_true, y_pred)\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(fpr, tpr, label=f'ROC Curve (AUC = {roc_auc:.2f})')\n",
    "plt.plot([0, 1], [0, 1], linestyle='--', label='Random Classifier')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver Operating Characteristic (ROC) Curve')\n",
    "plt.legend()\n",
    "plt.savefig('roc_curve.png')\n",
    "plt.close()\n",
    "print(\"ROC curve has been saved as 'roc_curve.png'.\")\n",
    "\n",
    "# Save training history\n",
    "with open('training_history.txt', 'w') as f:\n",
    "    for key, values in history.history.items():\n",
    "        f.write(f\"{key}: {values}\\n\")\n",
    "print(\"Training history has been saved in 'training_history.txt'.\")\n",
    "\n",
    "# Analyze misclassified examples\n",
    "misclassified = np.where(y_pred_classes.flatten() != y_true)[0]\n",
    "print(f\"\\nNumber of misclassified examples: {len(misclassified)}\")\n",
    "\n",
    "if len(misclassified) > 0:\n",
    "    print(\"\\nDetails of some misclassified examples:\")\n",
    "    for i in misclassified[:5]:  # Show details for the first 5 misclassified examples\n",
    "        true_label = y_true[i]\n",
    "        pred_label = y_pred_classes[i][0]\n",
    "        pred_prob = y_pred[i][0]\n",
    "        print(f\"Example {i}: True Label: {true_label}, Predicted Label: {pred_label}, Predicted Probability: {pred_prob:.4f}\")\n",
    "\n",
    "# Model performance across different thresholds\n",
    "accuracies = [accuracy_score(y_true, y_pred > threshold) for threshold in thresholds]\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(thresholds, accuracies, label='Accuracy')\n",
    "plt.plot(thresholds, f1_scores, label='F1-Score')\n",
    "plt.axvline(x=0.5, color='r', linestyle='--', label='Default Threshold (0.5)')\n",
    "plt.xlabel('Threshold')\n",
    "plt.ylabel('Score')\n",
    "plt.title('Model Performance vs. Classification Threshold')\n",
    "plt.legend()\n",
    "plt.savefig('performance_vs_threshold.png')\n",
    "plt.close()\n",
    "print(\"Performance vs. Threshold plot has been saved as 'performance_vs_threshold.png'.\")\n",
    "\n",
    "# Optional: SHAP Analysis\n",
    "try:\n",
    "    import shap\n",
    "    \n",
    "    # Select a subset of validation data for SHAP analysis\n",
    "    num_explain = 100\n",
    "    explain_dataset = val_dataset.take(num_explain).unbatch()\n",
    "    background_dataset = train_dataset.take(num_explain).unbatch()\n",
    "    \n",
    "    # Create an Explainer\n",
    "    explainer = shap.DeepExplainer(model, background_dataset)\n",
    "    \n",
    "    # Calculate SHAP values\n",
    "    shap_values = explainer.shap_values(explain_dataset)\n",
    "    \n",
    "    # Visualize SHAP values\n",
    "    shap.summary_plot(shap_values[0], plot_type=\"bar\", feature_names=numeric_columns, show=False)\n",
    "    plt.savefig('shap_summary.png')\n",
    "    plt.close()\n",
    "    print(\"SHAP summary has been saved as 'shap_summary.png'.\")\n",
    "except ImportError:\n",
    "    print(\"SHAP is not installed. Skipping SHAP analysis.\")\n",
    "\n",
    "print(\"\\nAll analyses and visualizations are complete.\")\n",
    "\n",
    "# Summary of key metrics\n",
    "print(\"\\nSummary of key metrics:\")\n",
    "print(f\"Validation Accuracy: {val_accuracy:.4f}\")\n",
    "print(f\"Validation AUC: {val_auc:.4f}\")\n",
    "print(f\"Validation Precision: {val_precision:.4f}\")\n",
    "print(f\"Validation Recall: {val_recall:.4f}\")\n",
    "print(f\"ROC AUC: {roc_auc:.4f}\")\n",
    "print(f\"Precision-Recall AUC: {pr_auc:.4f}\")\n",
    "print(f\"Best F1-Score: {best_f1:.4f} at threshold {best_threshold:.2f}\")\n",
    "\n",
    "# Save summary to a text file\n",
    "with open('model_summary.txt', 'w') as f:\n",
    "    f.write(\"Summary of key metrics:\\n\")\n",
    "    f.write(f\"Validation Accuracy: {val_accuracy:.4f}\\n\")\n",
    "    f.write(f\"Validation AUC: {val_auc:.4f}\\n\")\n",
    "    f.write(f\"Validation Precision: {val_precision:.4f}\\n\")\n",
    "    f.write(f\"Validation Recall: {val_recall:.4f}\\n\")\n",
    "    f.write(f\"ROC AUC: {roc_auc:.4f}\\n\")\n",
    "    f.write(f\"Precision-Recall AUC: {pr_auc:.4f}\\n\")\n",
    "    f.write(f\"Best F1-Score: {best_f1:.4f} at threshold {best_threshold:.2f}\\n\")\n",
    "\n",
    "print(\"\\nA summary of key metrics has been saved in 'model_summary.txt'.\")\n",
    "print(\"\\nThe script has been executed successfully. All results and visualizations have been saved.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a5f18f0-3577-4e3e-906a-00ad88c746ed",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c90a6560-8d8d-4a61-92e1-6f018c2f467e",
   "metadata": {},
   "source": [
    "## hybrid_model_optimized1.h5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "b9611889-2fa0-44a8-ad56-d3c371b5c03b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original CSV data loaded. Number of rows: 17762\n",
      "Original distribution of labels:\n",
      "label_majority\n",
      "2    9758\n",
      "1    3241\n",
      "4    2650\n",
      "3    2113\n",
      "Name: count, dtype: int64\n",
      "1 Physical GPUs, 1 Logical GPUs\n",
      "Mixed precision policy set to mixed_float16\n",
      "Found images: Filtered: 5018, Not filtered: 27994\n",
      "Sample of extracted filenames:\n",
      "0       100218_83ec552f_0000_1\n",
      "1    100218_83ec552f_0000_1Alt\n",
      "2       100622_1c7038f5_0006_1\n",
      "3    100622_1c7038f5_0006_1Alt\n",
      "4       100622_1c7038f5_0013_1\n",
      "Name: filename, dtype: object\n",
      "After undersampling: Number of rows: 4509\n",
      "Distribution of labels after undersampling:\n",
      "label\n",
      "1    2509\n",
      "0    2000\n",
      "Name: count, dtype: int64\n",
      "Training data: 3607, Validation data: 902\n",
      "INFO:tensorflow:Using MirroredStrategy with devices ('/job:localhost/replica:0/task:0/device:GPU:0',)\n",
      "Epoch 1/100\n",
      "112/112 [==============================] - 37s 217ms/step - loss: 0.8482 - accuracy: 0.5977 - auc_1: 0.6337 - precision_4: 0.6411 - recall_4: 0.6314 - val_loss: 0.8686 - val_accuracy: 0.4431 - val_auc_1: 0.8324 - val_precision_4: 1.0000 - val_recall_4: 0.0020 - lr: 0.0010\n",
      "Epoch 2/100\n",
      "112/112 [==============================] - 15s 132ms/step - loss: 0.6481 - accuracy: 0.6828 - auc_1: 0.7422 - precision_4: 0.7030 - recall_4: 0.7442 - val_loss: 0.5087 - val_accuracy: 0.7366 - val_auc_1: 0.8321 - val_precision_4: 0.7750 - val_recall_4: 0.7440 - lr: 0.0010\n",
      "Epoch 3/100\n",
      "112/112 [==============================] - 18s 165ms/step - loss: 0.5604 - accuracy: 0.7238 - auc_1: 0.7967 - precision_4: 0.7353 - recall_4: 0.7828 - val_loss: 0.4785 - val_accuracy: 0.7746 - val_auc_1: 0.8717 - val_precision_4: 0.7278 - val_recall_4: 0.9520 - lr: 0.0010\n",
      "Epoch 4/100\n",
      "112/112 [==============================] - 15s 132ms/step - loss: 0.5262 - accuracy: 0.7553 - auc_1: 0.8236 - precision_4: 0.7706 - recall_4: 0.8002 - val_loss: 0.5107 - val_accuracy: 0.7333 - val_auc_1: 0.8506 - val_precision_4: 0.8390 - val_recall_4: 0.6460 - lr: 0.0010\n",
      "Epoch 5/100\n",
      "112/112 [==============================] - 22s 194ms/step - loss: 0.4929 - accuracy: 0.7614 - auc_1: 0.8432 - precision_4: 0.7785 - recall_4: 0.7999 - val_loss: 0.4546 - val_accuracy: 0.8025 - val_auc_1: 0.8805 - val_precision_4: 0.8534 - val_recall_4: 0.7800 - lr: 0.0010\n",
      "Epoch 6/100\n",
      "112/112 [==============================] - 21s 188ms/step - loss: 0.4854 - accuracy: 0.7737 - auc_1: 0.8464 - precision_4: 0.7812 - recall_4: 0.8239 - val_loss: 0.4403 - val_accuracy: 0.8013 - val_auc_1: 0.8859 - val_precision_4: 0.8368 - val_recall_4: 0.8000 - lr: 0.0010\n",
      "Epoch 7/100\n",
      "112/112 [==============================] - 19s 174ms/step - loss: 0.4518 - accuracy: 0.7866 - auc_1: 0.8662 - precision_4: 0.7893 - recall_4: 0.8411 - val_loss: 0.4345 - val_accuracy: 0.7879 - val_auc_1: 0.8921 - val_precision_4: 0.7566 - val_recall_4: 0.9140 - lr: 0.0010\n",
      "Epoch 8/100\n",
      "112/112 [==============================] - 15s 132ms/step - loss: 0.4592 - accuracy: 0.7854 - auc_1: 0.8618 - precision_4: 0.7890 - recall_4: 0.8370 - val_loss: 0.4209 - val_accuracy: 0.8103 - val_auc_1: 0.8910 - val_precision_4: 0.8067 - val_recall_4: 0.8680 - lr: 0.0010\n",
      "Epoch 9/100\n",
      "112/112 [==============================] - 15s 132ms/step - loss: 0.4524 - accuracy: 0.7854 - auc_1: 0.8654 - precision_4: 0.7849 - recall_4: 0.8466 - val_loss: 0.4934 - val_accuracy: 0.7433 - val_auc_1: 0.8902 - val_precision_4: 0.8971 - val_recall_4: 0.6100 - lr: 9.0000e-04\n",
      "Epoch 10/100\n",
      "112/112 [==============================] - 15s 132ms/step - loss: 0.4255 - accuracy: 0.7985 - auc_1: 0.8812 - precision_4: 0.7994 - recall_4: 0.8501 - val_loss: 0.4672 - val_accuracy: 0.7679 - val_auc_1: 0.8859 - val_precision_4: 0.8763 - val_recall_4: 0.6800 - lr: 9.0000e-04\n",
      "Epoch 11/100\n",
      "112/112 [==============================] - 18s 162ms/step - loss: 0.4265 - accuracy: 0.8064 - auc_1: 0.8813 - precision_4: 0.8174 - recall_4: 0.8464 - val_loss: 0.4239 - val_accuracy: 0.8002 - val_auc_1: 0.8961 - val_precision_4: 0.7610 - val_recall_4: 0.9360 - lr: 9.0000e-04\n",
      "Epoch 12/100\n",
      "112/112 [==============================] - 18s 160ms/step - loss: 0.4048 - accuracy: 0.8161 - auc_1: 0.8954 - precision_4: 0.8100 - recall_4: 0.8689 - val_loss: 0.4495 - val_accuracy: 0.7790 - val_auc_1: 0.8965 - val_precision_4: 0.7316 - val_recall_4: 0.9540 - lr: 9.0000e-04\n",
      "Epoch 13/100\n",
      "112/112 [==============================] - 17s 156ms/step - loss: 0.4185 - accuracy: 0.8092 - auc_1: 0.8860 - precision_4: 0.8181 - recall_4: 0.8421 - val_loss: 0.4229 - val_accuracy: 0.8158 - val_auc_1: 0.8976 - val_precision_4: 0.8029 - val_recall_4: 0.8880 - lr: 9.0000e-04\n",
      "Epoch 14/100\n",
      "112/112 [==============================] - 17s 156ms/step - loss: 0.3900 - accuracy: 0.8237 - auc_1: 0.9010 - precision_4: 0.8301 - recall_4: 0.8623 - val_loss: 0.4540 - val_accuracy: 0.7879 - val_auc_1: 0.9001 - val_precision_4: 0.9101 - val_recall_4: 0.6880 - lr: 9.0000e-04\n",
      "Epoch 15/100\n",
      "112/112 [==============================] - 18s 163ms/step - loss: 0.3999 - accuracy: 0.8184 - auc_1: 0.8974 - precision_4: 0.8221 - recall_4: 0.8596 - val_loss: 0.5288 - val_accuracy: 0.6998 - val_auc_1: 0.9043 - val_precision_4: 0.9326 - val_recall_4: 0.4980 - lr: 9.0000e-04\n",
      "Epoch 16/100\n",
      "112/112 [==============================] - 15s 132ms/step - loss: 0.3998 - accuracy: 0.8234 - auc_1: 0.8973 - precision_4: 0.8231 - recall_4: 0.8674 - val_loss: 0.4569 - val_accuracy: 0.7946 - val_auc_1: 0.8762 - val_precision_4: 0.7540 - val_recall_4: 0.9380 - lr: 9.0000e-04\n",
      "Epoch 17/100\n",
      "112/112 [==============================] - 15s 132ms/step - loss: 0.3977 - accuracy: 0.8265 - auc_1: 0.8986 - precision_4: 0.8293 - recall_4: 0.8674 - val_loss: 0.4616 - val_accuracy: 0.7891 - val_auc_1: 0.8984 - val_precision_4: 0.9125 - val_recall_4: 0.6880 - lr: 9.0000e-04\n",
      "Epoch 18/100\n",
      "112/112 [==============================] - 15s 132ms/step - loss: 0.3835 - accuracy: 0.8290 - auc_1: 0.9051 - precision_4: 0.8305 - recall_4: 0.8700 - val_loss: 0.4644 - val_accuracy: 0.7757 - val_auc_1: 0.8931 - val_precision_4: 0.9030 - val_recall_4: 0.6700 - lr: 8.1000e-04\n",
      "Epoch 19/100\n",
      "112/112 [==============================] - 15s 132ms/step - loss: 0.3823 - accuracy: 0.8265 - auc_1: 0.9060 - precision_4: 0.8323 - recall_4: 0.8615 - val_loss: 0.4570 - val_accuracy: 0.7879 - val_auc_1: 0.8971 - val_precision_4: 0.7341 - val_recall_4: 0.9720 - lr: 8.1000e-04\n",
      "Epoch 20/100\n",
      "112/112 [==============================] - 16s 147ms/step - loss: 0.3937 - accuracy: 0.8189 - auc_1: 0.8998 - precision_4: 0.8251 - recall_4: 0.8546 - val_loss: 0.4210 - val_accuracy: 0.8069 - val_auc_1: 0.9059 - val_precision_4: 0.7659 - val_recall_4: 0.9420 - lr: 8.1000e-04\n",
      "Epoch 21/100\n",
      "112/112 [==============================] - 15s 131ms/step - loss: 0.3775 - accuracy: 0.8298 - auc_1: 0.9075 - precision_4: 0.8328 - recall_4: 0.8702 - val_loss: 0.4161 - val_accuracy: 0.8225 - val_auc_1: 0.9003 - val_precision_4: 0.8039 - val_recall_4: 0.9020 - lr: 8.1000e-04\n",
      "Epoch 22/100\n",
      "112/112 [==============================] - 16s 146ms/step - loss: 0.3743 - accuracy: 0.8256 - auc_1: 0.9104 - precision_4: 0.8251 - recall_4: 0.8688 - val_loss: 0.4213 - val_accuracy: 0.8103 - val_auc_1: 0.9078 - val_precision_4: 0.7687 - val_recall_4: 0.9440 - lr: 8.1000e-04\n",
      "Epoch 23/100\n",
      "112/112 [==============================] - 15s 132ms/step - loss: 0.3721 - accuracy: 0.8334 - auc_1: 0.9112 - precision_4: 0.8413 - recall_4: 0.8676 - val_loss: 0.5340 - val_accuracy: 0.7132 - val_auc_1: 0.9008 - val_precision_4: 0.9293 - val_recall_4: 0.5260 - lr: 8.1000e-04\n",
      "Epoch 24/100\n",
      "112/112 [==============================] - 16s 146ms/step - loss: 0.3674 - accuracy: 0.8337 - auc_1: 0.9137 - precision_4: 0.8345 - recall_4: 0.8724 - val_loss: 0.4290 - val_accuracy: 0.7991 - val_auc_1: 0.9103 - val_precision_4: 0.7516 - val_recall_4: 0.9560 - lr: 8.1000e-04\n",
      "Epoch 25/100\n",
      "112/112 [==============================] - 15s 132ms/step - loss: 0.3674 - accuracy: 0.8398 - auc_1: 0.9142 - precision_4: 0.8406 - recall_4: 0.8786 - val_loss: 0.3995 - val_accuracy: 0.8192 - val_auc_1: 0.9102 - val_precision_4: 0.7924 - val_recall_4: 0.9160 - lr: 8.1000e-04\n",
      "Epoch 26/100\n",
      "112/112 [==============================] - 16s 147ms/step - loss: 0.3619 - accuracy: 0.8357 - auc_1: 0.9154 - precision_4: 0.8416 - recall_4: 0.8698 - val_loss: 0.6628 - val_accuracy: 0.5804 - val_auc_1: 0.9167 - val_precision_4: 0.9627 - val_recall_4: 0.2580 - lr: 8.1000e-04\n",
      "Epoch 27/100\n",
      "112/112 [==============================] - 16s 146ms/step - loss: 0.3563 - accuracy: 0.8404 - auc_1: 0.9199 - precision_4: 0.8401 - recall_4: 0.8790 - val_loss: 0.4143 - val_accuracy: 0.8147 - val_auc_1: 0.9183 - val_precision_4: 0.7651 - val_recall_4: 0.9640 - lr: 7.2900e-04\n",
      "Epoch 28/100\n",
      "112/112 [==============================] - 15s 132ms/step - loss: 0.3635 - accuracy: 0.8365 - auc_1: 0.9153 - precision_4: 0.8466 - recall_4: 0.8636 - val_loss: 0.4175 - val_accuracy: 0.8125 - val_auc_1: 0.9066 - val_precision_4: 0.7748 - val_recall_4: 0.9360 - lr: 7.2900e-04\n",
      "Epoch 29/100\n",
      "112/112 [==============================] - 15s 133ms/step - loss: 0.3562 - accuracy: 0.8396 - auc_1: 0.9192 - precision_4: 0.8490 - recall_4: 0.8648 - val_loss: 0.4227 - val_accuracy: 0.8103 - val_auc_1: 0.9133 - val_precision_4: 0.7586 - val_recall_4: 0.9680 - lr: 7.2900e-04\n",
      "Epoch 30/100\n",
      "112/112 [==============================] - 15s 132ms/step - loss: 0.3500 - accuracy: 0.8465 - auc_1: 0.9224 - precision_4: 0.8510 - recall_4: 0.8783 - val_loss: 0.6479 - val_accuracy: 0.6819 - val_auc_1: 0.8913 - val_precision_4: 0.9424 - val_recall_4: 0.4580 - lr: 7.2900e-04\n",
      "Epoch 31/100\n",
      "112/112 [==============================] - 15s 132ms/step - loss: 0.3509 - accuracy: 0.8485 - auc_1: 0.9207 - precision_4: 0.8509 - recall_4: 0.8844 - val_loss: 0.3905 - val_accuracy: 0.8348 - val_auc_1: 0.9124 - val_precision_4: 0.8177 - val_recall_4: 0.9060 - lr: 7.2900e-04\n",
      "Epoch 32/100\n",
      "112/112 [==============================] - ETA: 0s - loss: 0.3614 - accuracy: 0.8415 - auc_1: 0.9174 - precision_4: 0.8462 - recall_4: 0.8750\n",
      "Epoch 32: ReduceLROnPlateau reducing learning rate to 0.000145800004247576.\n",
      "112/112 [==============================] - 15s 133ms/step - loss: 0.3614 - accuracy: 0.8415 - auc_1: 0.9174 - precision_4: 0.8462 - recall_4: 0.8750 - val_loss: 0.3987 - val_accuracy: 0.8337 - val_auc_1: 0.9104 - val_precision_4: 0.8042 - val_recall_4: 0.9280 - lr: 1.0629e-04\n",
      "Epoch 33/100\n",
      "112/112 [==============================] - 15s 132ms/step - loss: 0.3622 - accuracy: 0.8440 - auc_1: 0.9164 - precision_4: 0.8492 - recall_4: 0.8720 - val_loss: 0.4308 - val_accuracy: 0.8080 - val_auc_1: 0.9115 - val_precision_4: 0.9121 - val_recall_4: 0.7260 - lr: 1.0629e-04\n",
      "Epoch 34/100\n",
      "112/112 [==============================] - 15s 132ms/step - loss: 0.3617 - accuracy: 0.8354 - auc_1: 0.9168 - precision_4: 0.8358 - recall_4: 0.8753 - val_loss: 0.4410 - val_accuracy: 0.7980 - val_auc_1: 0.9052 - val_precision_4: 0.9038 - val_recall_4: 0.7140 - lr: 1.0629e-04\n",
      "Epoch 35/100\n",
      "112/112 [==============================] - 15s 132ms/step - loss: 0.3445 - accuracy: 0.8471 - auc_1: 0.9249 - precision_4: 0.8600 - recall_4: 0.8686 - val_loss: 0.4559 - val_accuracy: 0.7824 - val_auc_1: 0.9091 - val_precision_4: 0.9155 - val_recall_4: 0.6720 - lr: 1.0629e-04\n",
      "Epoch 36/100\n",
      "112/112 [==============================] - 15s 133ms/step - loss: 0.3578 - accuracy: 0.8499 - auc_1: 0.9193 - precision_4: 0.8553 - recall_4: 0.8793 - val_loss: 0.4041 - val_accuracy: 0.8482 - val_auc_1: 0.9170 - val_precision_4: 0.8730 - val_recall_4: 0.8520 - lr: 9.5659e-05\n",
      "Epoch 37/100\n",
      "112/112 [==============================] - 16s 146ms/step - loss: 0.3610 - accuracy: 0.8429 - auc_1: 0.9174 - precision_4: 0.8454 - recall_4: 0.8742 - val_loss: 0.3779 - val_accuracy: 0.8382 - val_auc_1: 0.9229 - val_precision_4: 0.8210 - val_recall_4: 0.9080 - lr: 9.5659e-05\n",
      "Epoch 38/100\n",
      "112/112 [==============================] - 15s 132ms/step - loss: 0.3438 - accuracy: 0.8518 - auc_1: 0.9247 - precision_4: 0.8562 - recall_4: 0.8831 - val_loss: 0.3783 - val_accuracy: 0.8482 - val_auc_1: 0.9227 - val_precision_4: 0.8321 - val_recall_4: 0.9120 - lr: 9.5659e-05\n",
      "Epoch 39/100\n",
      "112/112 [==============================] - 15s 132ms/step - loss: 0.3356 - accuracy: 0.8541 - auc_1: 0.9289 - precision_4: 0.8599 - recall_4: 0.8843 - val_loss: 0.4043 - val_accuracy: 0.8270 - val_auc_1: 0.9225 - val_precision_4: 0.9197 - val_recall_4: 0.7560 - lr: 9.5659e-05\n",
      "Epoch 40/100\n",
      "112/112 [==============================] - 15s 132ms/step - loss: 0.3445 - accuracy: 0.8482 - auc_1: 0.9240 - precision_4: 0.8519 - recall_4: 0.8809 - val_loss: 0.4495 - val_accuracy: 0.7734 - val_auc_1: 0.9172 - val_precision_4: 0.7168 - val_recall_4: 0.9820 - lr: 9.5659e-05\n",
      "Epoch 41/100\n",
      "112/112 [==============================] - 16s 147ms/step - loss: 0.3500 - accuracy: 0.8460 - auc_1: 0.9219 - precision_4: 0.8477 - recall_4: 0.8786 - val_loss: 0.4105 - val_accuracy: 0.8605 - val_auc_1: 0.9236 - val_precision_4: 0.8834 - val_recall_4: 0.8640 - lr: 9.5659e-05\n",
      "Epoch 42/100\n",
      "112/112 [==============================] - 16s 147ms/step - loss: 0.3329 - accuracy: 0.8535 - auc_1: 0.9302 - precision_4: 0.8621 - recall_4: 0.8764 - val_loss: 0.4324 - val_accuracy: 0.7734 - val_auc_1: 0.9284 - val_precision_4: 0.7162 - val_recall_4: 0.9840 - lr: 9.5659e-05\n",
      "Epoch 43/100\n",
      "112/112 [==============================] - 16s 147ms/step - loss: 0.3448 - accuracy: 0.8513 - auc_1: 0.9248 - precision_4: 0.8570 - recall_4: 0.8806 - val_loss: 0.3486 - val_accuracy: 0.8583 - val_auc_1: 0.9308 - val_precision_4: 0.8580 - val_recall_4: 0.8940 - lr: 9.5659e-05\n",
      "Epoch 44/100\n",
      "112/112 [==============================] - 16s 147ms/step - loss: 0.3283 - accuracy: 0.8557 - auc_1: 0.9315 - precision_4: 0.8544 - recall_4: 0.8903 - val_loss: 0.3573 - val_accuracy: 0.8538 - val_auc_1: 0.9310 - val_precision_4: 0.9020 - val_recall_4: 0.8280 - lr: 9.5659e-05\n",
      "Epoch 45/100\n",
      "112/112 [==============================] - 15s 133ms/step - loss: 0.3275 - accuracy: 0.8613 - auc_1: 0.9315 - precision_4: 0.8655 - recall_4: 0.8918 - val_loss: 0.3752 - val_accuracy: 0.8438 - val_auc_1: 0.9292 - val_precision_4: 0.8093 - val_recall_4: 0.9420 - lr: 8.6093e-05\n",
      "Epoch 46/100\n",
      "112/112 [==============================] - 15s 133ms/step - loss: 0.3238 - accuracy: 0.8624 - auc_1: 0.9334 - precision_4: 0.8673 - recall_4: 0.8886 - val_loss: 0.3675 - val_accuracy: 0.8337 - val_auc_1: 0.9281 - val_precision_4: 0.9034 - val_recall_4: 0.7860 - lr: 8.6093e-05\n",
      "Epoch 47/100\n",
      "112/112 [==============================] - 16s 147ms/step - loss: 0.3261 - accuracy: 0.8560 - auc_1: 0.9328 - precision_4: 0.8633 - recall_4: 0.8790 - val_loss: 0.3611 - val_accuracy: 0.8616 - val_auc_1: 0.9343 - val_precision_4: 0.8837 - val_recall_4: 0.8660 - lr: 8.6093e-05\n",
      "Epoch 48/100\n",
      "112/112 [==============================] - 15s 134ms/step - loss: 0.3310 - accuracy: 0.8524 - auc_1: 0.9307 - precision_4: 0.8574 - recall_4: 0.8803 - val_loss: 0.3564 - val_accuracy: 0.8661 - val_auc_1: 0.9342 - val_precision_4: 0.9077 - val_recall_4: 0.8460 - lr: 8.6093e-05\n",
      "Epoch 49/100\n",
      "112/112 [==============================] - 15s 133ms/step - loss: 0.3177 - accuracy: 0.8650 - auc_1: 0.9358 - precision_4: 0.8727 - recall_4: 0.8892 - val_loss: 0.3486 - val_accuracy: 0.8482 - val_auc_1: 0.9295 - val_precision_4: 0.8487 - val_recall_4: 0.8860 - lr: 8.6093e-05\n",
      "Epoch 50/100\n",
      "112/112 [==============================] - 16s 146ms/step - loss: 0.3095 - accuracy: 0.8664 - auc_1: 0.9400 - precision_4: 0.8649 - recall_4: 0.8985 - val_loss: 0.3242 - val_accuracy: 0.8594 - val_auc_1: 0.9362 - val_precision_4: 0.8864 - val_recall_4: 0.8580 - lr: 8.6093e-05\n",
      "Epoch 51/100\n",
      "112/112 [==============================] - 16s 147ms/step - loss: 0.3156 - accuracy: 0.8605 - auc_1: 0.9381 - precision_4: 0.8718 - recall_4: 0.8796 - val_loss: 0.3432 - val_accuracy: 0.8594 - val_auc_1: 0.9381 - val_precision_4: 0.8438 - val_recall_4: 0.9180 - lr: 8.6093e-05\n",
      "Epoch 52/100\n",
      "112/112 [==============================] - 16s 146ms/step - loss: 0.3176 - accuracy: 0.8644 - auc_1: 0.9358 - precision_4: 0.8731 - recall_4: 0.8886 - val_loss: 0.3125 - val_accuracy: 0.8717 - val_auc_1: 0.9448 - val_precision_4: 0.8598 - val_recall_4: 0.9200 - lr: 8.6093e-05\n",
      "Epoch 53/100\n",
      "112/112 [==============================] - 15s 132ms/step - loss: 0.3090 - accuracy: 0.8717 - auc_1: 0.9401 - precision_4: 0.8652 - recall_4: 0.9067 - val_loss: 0.3401 - val_accuracy: 0.8672 - val_auc_1: 0.9365 - val_precision_4: 0.8534 - val_recall_4: 0.9200 - lr: 8.6093e-05\n",
      "Epoch 54/100\n",
      "112/112 [==============================] - 15s 132ms/step - loss: 0.3167 - accuracy: 0.8689 - auc_1: 0.9367 - precision_4: 0.8732 - recall_4: 0.8934 - val_loss: 0.3692 - val_accuracy: 0.8326 - val_auc_1: 0.9323 - val_precision_4: 0.7907 - val_recall_4: 0.9520 - lr: 7.7484e-05\n",
      "Epoch 55/100\n",
      "112/112 [==============================] - 15s 132ms/step - loss: 0.3124 - accuracy: 0.8658 - auc_1: 0.9384 - precision_4: 0.8718 - recall_4: 0.8881 - val_loss: 0.3256 - val_accuracy: 0.8694 - val_auc_1: 0.9381 - val_precision_4: 0.8748 - val_recall_4: 0.8940 - lr: 7.7484e-05\n",
      "Epoch 56/100\n",
      "112/112 [==============================] - 15s 133ms/step - loss: 0.3057 - accuracy: 0.8697 - auc_1: 0.9396 - precision_4: 0.8778 - recall_4: 0.8946 - val_loss: 0.4906 - val_accuracy: 0.7600 - val_auc_1: 0.9367 - val_precision_4: 0.9553 - val_recall_4: 0.5980 - lr: 7.7484e-05\n",
      "Epoch 57/100\n",
      "112/112 [==============================] - ETA: 0s - loss: 0.3141 - accuracy: 0.8636 - auc_1: 0.9375 - precision_4: 0.8681 - recall_4: 0.8895\n",
      "Epoch 57: ReduceLROnPlateau reducing learning rate to 1.5496819105464966e-05.\n",
      "112/112 [==============================] - 15s 133ms/step - loss: 0.3141 - accuracy: 0.8636 - auc_1: 0.9375 - precision_4: 0.8681 - recall_4: 0.8895 - val_loss: 0.3462 - val_accuracy: 0.8504 - val_auc_1: 0.9412 - val_precision_4: 0.8134 - val_recall_4: 0.9500 - lr: 8.2356e-06\n",
      "Epoch 58/100\n",
      "112/112 [==============================] - 15s 133ms/step - loss: 0.3086 - accuracy: 0.8714 - auc_1: 0.9398 - precision_4: 0.8747 - recall_4: 0.8946 - val_loss: 0.3170 - val_accuracy: 0.8772 - val_auc_1: 0.9447 - val_precision_4: 0.8794 - val_recall_4: 0.9040 - lr: 8.2356e-06\n",
      "Epoch 59/100\n",
      "112/112 [==============================] - 15s 133ms/step - loss: 0.3044 - accuracy: 0.8677 - auc_1: 0.9413 - precision_4: 0.8724 - recall_4: 0.8950 - val_loss: 0.3668 - val_accuracy: 0.8583 - val_auc_1: 0.9309 - val_precision_4: 0.9011 - val_recall_4: 0.8380 - lr: 8.2356e-06\n",
      "Epoch 60/100\n",
      "112/112 [==============================] - 15s 132ms/step - loss: 0.3080 - accuracy: 0.8664 - auc_1: 0.9404 - precision_4: 0.8683 - recall_4: 0.8925 - val_loss: 6.7289 - val_accuracy: 0.5926 - val_auc_1: 0.5799 - val_precision_4: 0.5812 - val_recall_4: 0.9660 - lr: 8.2356e-06\n",
      "Epoch 61/100\n",
      "112/112 [==============================] - 15s 132ms/step - loss: 0.3217 - accuracy: 0.8585 - auc_1: 0.9346 - precision_4: 0.8645 - recall_4: 0.8808 - val_loss: 0.4269 - val_accuracy: 0.8069 - val_auc_1: 0.9261 - val_precision_4: 0.9314 - val_recall_4: 0.7060 - lr: 8.2356e-06\n",
      "Epoch 62/100\n",
      "112/112 [==============================] - ETA: 0s - loss: 0.3122 - accuracy: 0.8616 - auc_1: 0.9372 - precision_4: 0.8751 - recall_4: 0.8853\n",
      "Epoch 62: ReduceLROnPlateau reducing learning rate to 1.6471289200126195e-06.\n",
      "112/112 [==============================] - 15s 132ms/step - loss: 0.3122 - accuracy: 0.8616 - auc_1: 0.9372 - precision_4: 0.8751 - recall_4: 0.8853 - val_loss: 0.3935 - val_accuracy: 0.8304 - val_auc_1: 0.9348 - val_precision_4: 0.9244 - val_recall_4: 0.7580 - lr: 8.7535e-07\n",
      "Epoch 63/100\n",
      "112/112 [==============================] - 15s 132ms/step - loss: 0.3130 - accuracy: 0.8655 - auc_1: 0.9387 - precision_4: 0.8572 - recall_4: 0.9030 - val_loss: 0.3484 - val_accuracy: 0.8583 - val_auc_1: 0.9393 - val_precision_4: 0.8977 - val_recall_4: 0.8420 - lr: 7.8782e-07\n",
      "Epoch 64/100\n",
      "112/112 [==============================] - 15s 132ms/step - loss: 0.3050 - accuracy: 0.8633 - auc_1: 0.9406 - precision_4: 0.8703 - recall_4: 0.8902 - val_loss: 0.3311 - val_accuracy: 0.8616 - val_auc_1: 0.9365 - val_precision_4: 0.8745 - val_recall_4: 0.8780 - lr: 7.8782e-07\n",
      "Epoch 65/100\n",
      "112/112 [==============================] - 16s 147ms/step - loss: 0.3164 - accuracy: 0.8608 - auc_1: 0.9367 - precision_4: 0.8637 - recall_4: 0.8893 - val_loss: 0.3129 - val_accuracy: 0.8728 - val_auc_1: 0.9467 - val_precision_4: 0.8971 - val_recall_4: 0.8720 - lr: 7.8782e-07\n",
      "Epoch 66/100\n",
      "112/112 [==============================] - 16s 147ms/step - loss: 0.3030 - accuracy: 0.8661 - auc_1: 0.9420 - precision_4: 0.8718 - recall_4: 0.8926 - val_loss: 0.3036 - val_accuracy: 0.8750 - val_auc_1: 0.9501 - val_precision_4: 0.8647 - val_recall_4: 0.9200 - lr: 7.8782e-07\n",
      "Epoch 67/100\n",
      "112/112 [==============================] - 15s 132ms/step - loss: 0.3082 - accuracy: 0.8683 - auc_1: 0.9408 - precision_4: 0.8559 - recall_4: 0.9082 - val_loss: 0.3358 - val_accuracy: 0.8694 - val_auc_1: 0.9375 - val_precision_4: 0.8900 - val_recall_4: 0.8740 - lr: 7.8782e-07\n",
      "Epoch 68/100\n",
      "112/112 [==============================] - 15s 133ms/step - loss: 0.3046 - accuracy: 0.8753 - auc_1: 0.9403 - precision_4: 0.8858 - recall_4: 0.8966 - val_loss: 0.3078 - val_accuracy: 0.8705 - val_auc_1: 0.9477 - val_precision_4: 0.9017 - val_recall_4: 0.8620 - lr: 7.8782e-07\n",
      "Epoch 69/100\n",
      "112/112 [==============================] - 15s 132ms/step - loss: 0.2988 - accuracy: 0.8703 - auc_1: 0.9433 - precision_4: 0.8707 - recall_4: 0.9016 - val_loss: 0.3441 - val_accuracy: 0.8605 - val_auc_1: 0.9333 - val_precision_4: 0.8641 - val_recall_4: 0.8900 - lr: 7.8782e-07\n",
      "Epoch 70/100\n",
      "112/112 [==============================] - 15s 132ms/step - loss: 0.2930 - accuracy: 0.8781 - auc_1: 0.9464 - precision_4: 0.8762 - recall_4: 0.9076 - val_loss: 0.3266 - val_accuracy: 0.8705 - val_auc_1: 0.9371 - val_precision_4: 0.8934 - val_recall_4: 0.8720 - lr: 7.8782e-07\n",
      "Epoch 71/100\n",
      "112/112 [==============================] - 15s 132ms/step - loss: 0.3056 - accuracy: 0.8725 - auc_1: 0.9410 - precision_4: 0.8764 - recall_4: 0.9003 - val_loss: 0.3653 - val_accuracy: 0.8337 - val_auc_1: 0.9444 - val_precision_4: 0.9291 - val_recall_4: 0.7600 - lr: 7.8782e-07\n",
      "Epoch 72/100\n",
      "112/112 [==============================] - 15s 133ms/step - loss: 0.2899 - accuracy: 0.8767 - auc_1: 0.9478 - precision_4: 0.8748 - recall_4: 0.9042 - val_loss: 0.2970 - val_accuracy: 0.8683 - val_auc_1: 0.9464 - val_precision_4: 0.8882 - val_recall_4: 0.8740 - lr: 7.0903e-07\n",
      "Epoch 73/100\n",
      "112/112 [==============================] - 15s 132ms/step - loss: 0.2841 - accuracy: 0.8809 - auc_1: 0.9486 - precision_4: 0.8818 - recall_4: 0.9087 - val_loss: 0.3201 - val_accuracy: 0.8739 - val_auc_1: 0.9412 - val_precision_4: 0.8686 - val_recall_4: 0.9120 - lr: 7.0903e-07\n",
      "Epoch 74/100\n",
      "112/112 [==============================] - 16s 147ms/step - loss: 0.2846 - accuracy: 0.8764 - auc_1: 0.9487 - precision_4: 0.8778 - recall_4: 0.9038 - val_loss: 0.2915 - val_accuracy: 0.8817 - val_auc_1: 0.9514 - val_precision_4: 0.8731 - val_recall_4: 0.9220 - lr: 7.0903e-07\n",
      "Epoch 75/100\n",
      "112/112 [==============================] - 15s 133ms/step - loss: 0.2840 - accuracy: 0.8814 - auc_1: 0.9487 - precision_4: 0.8868 - recall_4: 0.9024 - val_loss: 0.2987 - val_accuracy: 0.8828 - val_auc_1: 0.9504 - val_precision_4: 0.8624 - val_recall_4: 0.9400 - lr: 7.0903e-07\n",
      "Epoch 76/100\n",
      "112/112 [==============================] - 15s 132ms/step - loss: 0.2839 - accuracy: 0.8823 - auc_1: 0.9491 - precision_4: 0.8869 - recall_4: 0.9038 - val_loss: 0.3148 - val_accuracy: 0.8739 - val_auc_1: 0.9446 - val_precision_4: 0.8817 - val_recall_4: 0.8940 - lr: 7.0903e-07\n",
      "Epoch 77/100\n",
      "112/112 [==============================] - 15s 132ms/step - loss: 0.2849 - accuracy: 0.8806 - auc_1: 0.9490 - precision_4: 0.8818 - recall_4: 0.9058 - val_loss: 0.2992 - val_accuracy: 0.8795 - val_auc_1: 0.9506 - val_precision_4: 0.9050 - val_recall_4: 0.8760 - lr: 7.0903e-07\n",
      "Epoch 78/100\n",
      "112/112 [==============================] - 15s 132ms/step - loss: 0.2986 - accuracy: 0.8708 - auc_1: 0.9431 - precision_4: 0.8726 - recall_4: 0.9000 - val_loss: 0.3164 - val_accuracy: 0.8739 - val_auc_1: 0.9438 - val_precision_4: 0.8462 - val_recall_4: 0.9460 - lr: 7.0903e-07\n",
      "Epoch 79/100\n",
      "112/112 [==============================] - 15s 132ms/step - loss: 0.2815 - accuracy: 0.8797 - auc_1: 0.9500 - precision_4: 0.8856 - recall_4: 0.9028 - val_loss: 0.3336 - val_accuracy: 0.8527 - val_auc_1: 0.9436 - val_precision_4: 0.9182 - val_recall_4: 0.8080 - lr: 7.0903e-07\n",
      "Epoch 80/100\n",
      "112/112 [==============================] - 16s 146ms/step - loss: 0.2614 - accuracy: 0.8926 - auc_1: 0.9575 - precision_4: 0.8862 - recall_4: 0.9211 - val_loss: 0.2835 - val_accuracy: 0.8750 - val_auc_1: 0.9539 - val_precision_4: 0.9059 - val_recall_4: 0.8660 - lr: 7.0903e-07\n",
      "Epoch 81/100\n",
      "112/112 [==============================] - 17s 148ms/step - loss: 0.2836 - accuracy: 0.8814 - auc_1: 0.9490 - precision_4: 0.8882 - recall_4: 0.9061 - val_loss: 0.2754 - val_accuracy: 0.8828 - val_auc_1: 0.9573 - val_precision_4: 0.8624 - val_recall_4: 0.9400 - lr: 6.3813e-07\n",
      "Epoch 82/100\n",
      "112/112 [==============================] - 15s 134ms/step - loss: 0.2831 - accuracy: 0.8792 - auc_1: 0.9495 - precision_4: 0.8782 - recall_4: 0.9070 - val_loss: 0.2952 - val_accuracy: 0.8839 - val_auc_1: 0.9493 - val_precision_4: 0.8680 - val_recall_4: 0.9340 - lr: 6.3813e-07\n",
      "Epoch 83/100\n",
      "112/112 [==============================] - 15s 133ms/step - loss: 0.2720 - accuracy: 0.8842 - auc_1: 0.9535 - precision_4: 0.8851 - recall_4: 0.9079 - val_loss: 0.3085 - val_accuracy: 0.8638 - val_auc_1: 0.9511 - val_precision_4: 0.9219 - val_recall_4: 0.8260 - lr: 6.3813e-07\n",
      "Epoch 84/100\n",
      "112/112 [==============================] - 15s 133ms/step - loss: 0.2802 - accuracy: 0.8811 - auc_1: 0.9508 - precision_4: 0.8828 - recall_4: 0.9043 - val_loss: 0.2971 - val_accuracy: 0.8717 - val_auc_1: 0.9551 - val_precision_4: 0.9176 - val_recall_4: 0.8460 - lr: 6.3813e-07\n",
      "Epoch 85/100\n",
      "112/112 [==============================] - 15s 134ms/step - loss: 0.2745 - accuracy: 0.8792 - auc_1: 0.9524 - precision_4: 0.8804 - recall_4: 0.9056 - val_loss: 0.3417 - val_accuracy: 0.8504 - val_auc_1: 0.9369 - val_precision_4: 0.8112 - val_recall_4: 0.9540 - lr: 6.3813e-07\n",
      "Epoch 86/100\n",
      "112/112 [==============================] - 15s 134ms/step - loss: 0.2714 - accuracy: 0.8811 - auc_1: 0.9536 - precision_4: 0.8914 - recall_4: 0.9001 - val_loss: 0.3009 - val_accuracy: 0.8817 - val_auc_1: 0.9498 - val_precision_4: 0.8505 - val_recall_4: 0.9560 - lr: 6.3813e-07\n",
      "Epoch 87/100\n",
      "112/112 [==============================] - 15s 134ms/step - loss: 0.2567 - accuracy: 0.8962 - auc_1: 0.9587 - precision_4: 0.8978 - recall_4: 0.9159 - val_loss: 0.3256 - val_accuracy: 0.8605 - val_auc_1: 0.9552 - val_precision_4: 0.9475 - val_recall_4: 0.7940 - lr: 6.3813e-07\n",
      "Epoch 88/100\n",
      "112/112 [==============================] - 15s 133ms/step - loss: 0.2897 - accuracy: 0.8739 - auc_1: 0.9470 - precision_4: 0.8740 - recall_4: 0.9045 - val_loss: 0.3273 - val_accuracy: 0.8549 - val_auc_1: 0.9519 - val_precision_4: 0.8073 - val_recall_4: 0.9720 - lr: 6.3813e-07\n",
      "Epoch 89/100\n",
      "112/112 [==============================] - 15s 132ms/step - loss: 0.2670 - accuracy: 0.8873 - auc_1: 0.9542 - precision_4: 0.8900 - recall_4: 0.9129 - val_loss: 0.3387 - val_accuracy: 0.8739 - val_auc_1: 0.9465 - val_precision_4: 0.8862 - val_recall_4: 0.8880 - lr: 6.3813e-07\n",
      "Epoch 90/100\n",
      "112/112 [==============================] - 15s 133ms/step - loss: 0.2893 - accuracy: 0.8725 - auc_1: 0.9478 - precision_4: 0.8750 - recall_4: 0.8951 - val_loss: 0.3050 - val_accuracy: 0.8795 - val_auc_1: 0.9526 - val_precision_4: 0.8403 - val_recall_4: 0.9680 - lr: 5.7432e-07\n",
      "Epoch 91/100\n",
      "112/112 [==============================] - 15s 133ms/step - loss: 0.2700 - accuracy: 0.8859 - auc_1: 0.9543 - precision_4: 0.8910 - recall_4: 0.9074 - val_loss: 0.3006 - val_accuracy: 0.8705 - val_auc_1: 0.9548 - val_precision_4: 0.9229 - val_recall_4: 0.8380 - lr: 5.7432e-07\n",
      "Epoch 92/100\n",
      "112/112 [==============================] - 15s 133ms/step - loss: 0.2699 - accuracy: 0.8839 - auc_1: 0.9539 - precision_4: 0.8912 - recall_4: 0.9011 - val_loss: 0.2879 - val_accuracy: 0.8895 - val_auc_1: 0.9535 - val_precision_4: 0.8720 - val_recall_4: 0.9400 - lr: 5.7432e-07\n",
      "Epoch 93/100\n",
      "112/112 [==============================] - 15s 137ms/step - loss: 0.2725 - accuracy: 0.8836 - auc_1: 0.9538 - precision_4: 0.8872 - recall_4: 0.9071 - val_loss: 0.2841 - val_accuracy: 0.8873 - val_auc_1: 0.9543 - val_precision_4: 0.8889 - val_recall_4: 0.9120 - lr: 5.7432e-07\n",
      "Epoch 94/100\n",
      "112/112 [==============================] - 15s 136ms/step - loss: 0.2750 - accuracy: 0.8856 - auc_1: 0.9522 - precision_4: 0.8866 - recall_4: 0.9090 - val_loss: 0.2796 - val_accuracy: 0.8806 - val_auc_1: 0.9541 - val_precision_4: 0.8922 - val_recall_4: 0.8940 - lr: 5.7432e-07\n",
      "Epoch 95/100\n",
      "112/112 [==============================] - 15s 133ms/step - loss: 0.2609 - accuracy: 0.8898 - auc_1: 0.9568 - precision_4: 0.8981 - recall_4: 0.9074 - val_loss: 0.3044 - val_accuracy: 0.8605 - val_auc_1: 0.9566 - val_precision_4: 0.9271 - val_recall_4: 0.8140 - lr: 5.7432e-07\n",
      "Epoch 96/100\n",
      "112/112 [==============================] - 15s 133ms/step - loss: 0.2514 - accuracy: 0.8973 - auc_1: 0.9604 - precision_4: 0.8935 - recall_4: 0.9234 - val_loss: 0.3528 - val_accuracy: 0.8415 - val_auc_1: 0.9555 - val_precision_4: 0.9520 - val_recall_4: 0.7540 - lr: 5.7432e-07\n",
      "Epoch 97/100\n",
      "112/112 [==============================] - 15s 133ms/step - loss: 0.2672 - accuracy: 0.8881 - auc_1: 0.9541 - precision_4: 0.8896 - recall_4: 0.9131 - val_loss: 0.3076 - val_accuracy: 0.8638 - val_auc_1: 0.9528 - val_precision_4: 0.8236 - val_recall_4: 0.9620 - lr: 5.7432e-07\n",
      "Epoch 98/100\n",
      "112/112 [==============================] - 17s 148ms/step - loss: 0.2533 - accuracy: 0.8954 - auc_1: 0.9595 - precision_4: 0.9036 - recall_4: 0.9112 - val_loss: 0.2602 - val_accuracy: 0.8917 - val_auc_1: 0.9599 - val_precision_4: 0.8882 - val_recall_4: 0.9220 - lr: 5.7432e-07\n",
      "Epoch 99/100\n",
      "112/112 [==============================] - 15s 134ms/step - loss: 0.2654 - accuracy: 0.8920 - auc_1: 0.9562 - precision_4: 0.8884 - recall_4: 0.9171 - val_loss: 0.3014 - val_accuracy: 0.8739 - val_auc_1: 0.9517 - val_precision_4: 0.8389 - val_recall_4: 0.9580 - lr: 5.1689e-07\n",
      "Epoch 100/100\n",
      "112/112 [==============================] - 16s 147ms/step - loss: 0.2514 - accuracy: 0.8915 - auc_1: 0.9602 - precision_4: 0.8991 - recall_4: 0.9102 - val_loss: 0.2839 - val_accuracy: 0.8717 - val_auc_1: 0.9628 - val_precision_4: 0.9385 - val_recall_4: 0.8240 - lr: 5.1689e-07\n",
      "28/28 [==============================] - 1s 43ms/step - loss: 0.2839 - accuracy: 0.8717 - auc_1: 0.9628 - precision_4: 0.9385 - recall_4: 0.8240\n",
      "Validation Loss: 0.2839\n",
      "Validation Accuracy: 0.8717\n",
      "Validation AUC: 0.9628\n",
      "Validation Precision: 0.9385\n",
      "Validation Recall: 0.8240\n",
      "28/28 [==============================] - 2s 32ms/step\n",
      "Confusion Matrix:\n",
      "[[369  27]\n",
      " [ 88 412]]\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.81      0.93      0.87       396\n",
      "           1       0.94      0.82      0.88       500\n",
      "\n",
      "    accuracy                           0.87       896\n",
      "   macro avg       0.87      0.88      0.87       896\n",
      "weighted avg       0.88      0.87      0.87       896\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\meh91075\\AppData\\Local\\Temp\\ipykernel_14680\\2272295425.py:328: UserWarning: No artists with labels found to put in legend.  Note that artists whose label start with an underscore are ignored when legend() is called with no argument.\n",
      "  plt.legend()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Visualizations have been saved as PNG files.\n",
      "Predictions have been saved in 'predictions.csv'.\n",
      "Model: \"model_5\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_11 (InputLayer)          [(None, 224, 224, 3  0           []                               \n",
      "                                )]                                                                \n",
      "                                                                                                  \n",
      " conv2d_17 (Conv2D)             (None, 224, 224, 64  1792        ['input_11[0][0]']               \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " max_pooling2d_17 (MaxPooling2D  (None, 112, 112, 64  0          ['conv2d_17[0][0]']              \n",
      " )                              )                                                                 \n",
      "                                                                                                  \n",
      " conv2d_18 (Conv2D)             (None, 112, 112, 12  73856       ['max_pooling2d_17[0][0]']       \n",
      "                                8)                                                                \n",
      "                                                                                                  \n",
      " max_pooling2d_18 (MaxPooling2D  (None, 56, 56, 128)  0          ['conv2d_18[0][0]']              \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " conv2d_19 (Conv2D)             (None, 56, 56, 256)  295168      ['max_pooling2d_18[0][0]']       \n",
      "                                                                                                  \n",
      " max_pooling2d_19 (MaxPooling2D  (None, 28, 28, 256)  0          ['conv2d_19[0][0]']              \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " conv2d_20 (Conv2D)             (None, 28, 28, 512)  1180160     ['max_pooling2d_19[0][0]']       \n",
      "                                                                                                  \n",
      " max_pooling2d_20 (MaxPooling2D  (None, 14, 14, 512)  0          ['conv2d_20[0][0]']              \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " flatten_2 (Flatten)            (None, 100352)       0           ['max_pooling2d_20[0][0]']       \n",
      "                                                                                                  \n",
      " dense_19 (Dense)               (None, 512)          51380736    ['flatten_2[0][0]']              \n",
      "                                                                                                  \n",
      " batch_normalization_23 (BatchN  (None, 512)         2048        ['dense_19[0][0]']               \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " dropout_17 (Dropout)           (None, 512)          0           ['batch_normalization_23[0][0]'] \n",
      "                                                                                                  \n",
      " input_12 (InputLayer)          [(None, 31)]         0           []                               \n",
      "                                                                                                  \n",
      " dense_20 (Dense)               (None, 256)          131328      ['dropout_17[0][0]']             \n",
      "                                                                                                  \n",
      " dense_21 (Dense)               (None, 64)           2048        ['input_12[0][0]']               \n",
      "                                                                                                  \n",
      " batch_normalization_24 (BatchN  (None, 256)         1024        ['dense_20[0][0]']               \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " batch_normalization_25 (BatchN  (None, 64)          256         ['dense_21[0][0]']               \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " dropout_18 (Dropout)           (None, 256)          0           ['batch_normalization_24[0][0]'] \n",
      "                                                                                                  \n",
      " dropout_19 (Dropout)           (None, 64)           0           ['batch_normalization_25[0][0]'] \n",
      "                                                                                                  \n",
      " concatenate_5 (Concatenate)    (None, 320)          0           ['dropout_18[0][0]',             \n",
      "                                                                  'dropout_19[0][0]']             \n",
      "                                                                                                  \n",
      " dense_22 (Dense)               (None, 128)          41088       ['concatenate_5[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_26 (BatchN  (None, 128)         512         ['dense_22[0][0]']               \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " dropout_20 (Dropout)           (None, 128)          0           ['batch_normalization_26[0][0]'] \n",
      "                                                                                                  \n",
      " dense_23 (Dense)               (None, 1)            129         ['dropout_20[0][0]']             \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 53,110,145\n",
      "Trainable params: 53,108,225\n",
      "Non-trainable params: 1,920\n",
      "__________________________________________________________________________________________________\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "module 'pydot' has no attribute 'InvocationException'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "File \u001b[1;32m~\\.conda\\envs\\tf-gpu\\lib\\site-packages\\pydot\\core.py:1851\u001b[0m, in \u001b[0;36mDot.create\u001b[1;34m(self, prog, format, encoding)\u001b[0m\n\u001b[0;32m   1850\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1851\u001b[0m     stdout_data, stderr_data, process \u001b[38;5;241m=\u001b[39m \u001b[43mcall_graphviz\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1852\u001b[0m \u001b[43m        \u001b[49m\u001b[43mprogram\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprog\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1853\u001b[0m \u001b[43m        \u001b[49m\u001b[43marguments\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43marguments\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1854\u001b[0m \u001b[43m        \u001b[49m\u001b[43mworking_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtmp_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1855\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1856\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[1;32m~\\.conda\\envs\\tf-gpu\\lib\\site-packages\\pydot\\core.py:211\u001b[0m, in \u001b[0;36mcall_graphviz\u001b[1;34m(program, arguments, working_dir, **kwargs)\u001b[0m\n\u001b[0;32m    209\u001b[0m program_with_args \u001b[38;5;241m=\u001b[39m [program] \u001b[38;5;241m+\u001b[39m arguments\n\u001b[1;32m--> 211\u001b[0m process \u001b[38;5;241m=\u001b[39m subprocess\u001b[38;5;241m.\u001b[39mPopen(\n\u001b[0;32m    212\u001b[0m     program_with_args,\n\u001b[0;32m    213\u001b[0m     env\u001b[38;5;241m=\u001b[39menv,\n\u001b[0;32m    214\u001b[0m     cwd\u001b[38;5;241m=\u001b[39mworking_dir,\n\u001b[0;32m    215\u001b[0m     shell\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    216\u001b[0m     stderr\u001b[38;5;241m=\u001b[39msubprocess\u001b[38;5;241m.\u001b[39mPIPE,\n\u001b[0;32m    217\u001b[0m     stdout\u001b[38;5;241m=\u001b[39msubprocess\u001b[38;5;241m.\u001b[39mPIPE,\n\u001b[0;32m    218\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m    219\u001b[0m )\n\u001b[0;32m    220\u001b[0m stdout_data, stderr_data \u001b[38;5;241m=\u001b[39m process\u001b[38;5;241m.\u001b[39mcommunicate()\n",
      "File \u001b[1;32m~\\.conda\\envs\\tf-gpu\\lib\\subprocess.py:951\u001b[0m, in \u001b[0;36mPopen.__init__\u001b[1;34m(self, args, bufsize, executable, stdin, stdout, stderr, preexec_fn, close_fds, shell, cwd, env, universal_newlines, startupinfo, creationflags, restore_signals, start_new_session, pass_fds, user, group, extra_groups, encoding, errors, text, umask)\u001b[0m\n\u001b[0;32m    948\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstderr \u001b[38;5;241m=\u001b[39m io\u001b[38;5;241m.\u001b[39mTextIOWrapper(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstderr,\n\u001b[0;32m    949\u001b[0m                     encoding\u001b[38;5;241m=\u001b[39mencoding, errors\u001b[38;5;241m=\u001b[39merrors)\n\u001b[1;32m--> 951\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execute_child\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexecutable\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpreexec_fn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclose_fds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    952\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mpass_fds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcwd\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    953\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mstartupinfo\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreationflags\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshell\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    954\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mp2cread\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mp2cwrite\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    955\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mc2pread\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mc2pwrite\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    956\u001b[0m \u001b[43m                        \u001b[49m\u001b[43merrread\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merrwrite\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    957\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mrestore_signals\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    958\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mgid\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muid\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mumask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    959\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mstart_new_session\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    960\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m:\n\u001b[0;32m    961\u001b[0m     \u001b[38;5;66;03m# Cleanup if the child failed starting.\u001b[39;00m\n",
      "File \u001b[1;32m~\\.conda\\envs\\tf-gpu\\lib\\subprocess.py:1436\u001b[0m, in \u001b[0;36mPopen._execute_child\u001b[1;34m(self, args, executable, preexec_fn, close_fds, pass_fds, cwd, env, startupinfo, creationflags, shell, p2cread, p2cwrite, c2pread, c2pwrite, errread, errwrite, unused_restore_signals, unused_gid, unused_gids, unused_uid, unused_umask, unused_start_new_session)\u001b[0m\n\u001b[0;32m   1435\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1436\u001b[0m     hp, ht, pid, tid \u001b[38;5;241m=\u001b[39m \u001b[43m_winapi\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mCreateProcess\u001b[49m\u001b[43m(\u001b[49m\u001b[43mexecutable\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1437\u001b[0m \u001b[43m                             \u001b[49m\u001b[38;5;66;43;03m# no special security\u001b[39;49;00m\n\u001b[0;32m   1438\u001b[0m \u001b[43m                             \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m   1439\u001b[0m \u001b[43m                             \u001b[49m\u001b[38;5;28;43mint\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mclose_fds\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1440\u001b[0m \u001b[43m                             \u001b[49m\u001b[43mcreationflags\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1441\u001b[0m \u001b[43m                             \u001b[49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1442\u001b[0m \u001b[43m                             \u001b[49m\u001b[43mcwd\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1443\u001b[0m \u001b[43m                             \u001b[49m\u001b[43mstartupinfo\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1444\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m   1445\u001b[0m     \u001b[38;5;66;03m# Child is launched. Close the parent's copy of those pipe\u001b[39;00m\n\u001b[0;32m   1446\u001b[0m     \u001b[38;5;66;03m# handles that only the child should have open.  You need\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1449\u001b[0m     \u001b[38;5;66;03m# pipe will not close when the child process exits and the\u001b[39;00m\n\u001b[0;32m   1450\u001b[0m     \u001b[38;5;66;03m# ReadFile will hang.\u001b[39;00m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [WinError 2] The system cannot find the file specified",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "File \u001b[1;32m~\\.conda\\envs\\tf-gpu\\lib\\site-packages\\keras\\utils\\vis_utils.py:57\u001b[0m, in \u001b[0;36mcheck_graphviz\u001b[1;34m()\u001b[0m\n\u001b[0;32m     54\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     55\u001b[0m     \u001b[38;5;66;03m# Attempt to create an image of a blank graph\u001b[39;00m\n\u001b[0;32m     56\u001b[0m     \u001b[38;5;66;03m# to check the pydot/graphviz installation.\u001b[39;00m\n\u001b[1;32m---> 57\u001b[0m     \u001b[43mpydot\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mDot\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpydot\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mDot\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     58\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[1;32m~\\.conda\\envs\\tf-gpu\\lib\\site-packages\\pydot\\core.py:1860\u001b[0m, in \u001b[0;36mDot.create\u001b[1;34m(self, prog, format, encoding)\u001b[0m\n\u001b[0;32m   1859\u001b[0m     args[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mprog\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m not found in path.\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m-> 1860\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m(\u001b[38;5;241m*\u001b[39margs)\n\u001b[0;32m   1861\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [WinError 2] \"dot\" not found in path.",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[30], line 384\u001b[0m\n\u001b[0;32m    381\u001b[0m model\u001b[38;5;241m.\u001b[39msummary()\n\u001b[0;32m    383\u001b[0m \u001b[38;5;66;03m# Save model architecture as image\u001b[39;00m\n\u001b[1;32m--> 384\u001b[0m \u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkeras\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mutils\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mplot_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mto_file\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmodel_architecture.png\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshow_shapes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshow_layer_names\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m    385\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mModel architecture has been saved as \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmodel_architecture.png\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    387\u001b[0m \u001b[38;5;66;03m# Calculate and output additional metrics\u001b[39;00m\n\u001b[0;32m    388\u001b[0m \u001b[38;5;66;03m# ROC AUC\u001b[39;00m\n",
      "File \u001b[1;32m~\\.conda\\envs\\tf-gpu\\lib\\site-packages\\keras\\utils\\vis_utils.py:436\u001b[0m, in \u001b[0;36mplot_model\u001b[1;34m(model, to_file, show_shapes, show_dtype, show_layer_names, rankdir, expand_nested, dpi, layer_range, show_layer_activations)\u001b[0m\n\u001b[0;32m    429\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m model\u001b[38;5;241m.\u001b[39mbuilt:\n\u001b[0;32m    430\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    431\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThis model has not yet been built. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    432\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBuild the model first by calling `build()` or by calling \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    433\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mthe model on a batch of data.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    434\u001b[0m     )\n\u001b[1;32m--> 436\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[43mcheck_graphviz\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[0;32m    437\u001b[0m     message \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    438\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou must install pydot (`pip install pydot`) \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    439\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mand install graphviz \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    440\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m(see instructions at https://graphviz.gitlab.io/download/) \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    441\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfor plot_model to work.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    442\u001b[0m     )\n\u001b[0;32m    443\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIPython.core.magics.namespace\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m sys\u001b[38;5;241m.\u001b[39mmodules:\n\u001b[0;32m    444\u001b[0m         \u001b[38;5;66;03m# We don't raise an exception here in order to avoid crashing\u001b[39;00m\n\u001b[0;32m    445\u001b[0m         \u001b[38;5;66;03m# notebook tests where graphviz is not available.\u001b[39;00m\n",
      "File \u001b[1;32m~\\.conda\\envs\\tf-gpu\\lib\\site-packages\\keras\\utils\\vis_utils.py:59\u001b[0m, in \u001b[0;36mcheck_graphviz\u001b[1;34m()\u001b[0m\n\u001b[0;32m     57\u001b[0m     pydot\u001b[38;5;241m.\u001b[39mDot\u001b[38;5;241m.\u001b[39mcreate(pydot\u001b[38;5;241m.\u001b[39mDot())\n\u001b[0;32m     58\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m---> 59\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mOSError\u001b[39;00m, \u001b[43mpydot\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mInvocationException\u001b[49m):\n\u001b[0;32m     60\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "\u001b[1;31mAttributeError\u001b[0m: module 'pydot' has no attribute 'InvocationException'"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models, Input, mixed_precision\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import confusion_matrix, classification_report, f1_score, roc_auc_score, precision_recall_curve, average_precision_score, accuracy_score, roc_curve\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Load the DataFrame\n",
    "file_path = r\"C:\\Users\\meh91075\\Downloads\\AutoSplit_dataset_09092024.csv\"\n",
    "#C:\\Users\\meh91075\\Downloads\\merged_features_filtered.csv\"\n",
    "df = pd.read_csv(file_path)\n",
    "print(f\"Original CSV data loaded. Number of rows: {len(df)}\")\n",
    "print(f\"Original distribution of labels:\")\n",
    "print(df['label_majority'].value_counts())\n",
    "\n",
    "# Optimize GPU configuration\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    try:\n",
    "        for gpu in gpus:\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)\n",
    "        logical_gpus = tf.config.experimental.list_logical_devices('GPU')\n",
    "        print(f\"{len(gpus)} Physical GPUs, {len(logical_gpus)} Logical GPUs\")\n",
    "    except RuntimeError as e:\n",
    "        print(e)\n",
    "\n",
    "# Activate Mixed Precision\n",
    "if tf.test.is_built_with_cuda():\n",
    "    policy = mixed_precision.Policy('mixed_float16')\n",
    "    mixed_precision.set_global_policy(policy)\n",
    "    print(\"Mixed precision policy set to mixed_float16\")\n",
    "else:\n",
    "    print(\"CUDA not available, using default precision\")\n",
    "\n",
    "\n",
    "# Pfade und Konfiguration\n",
    "base_dir = \"C:/Users/meh91075/Desktop/02_AutoSplit/02_AutoSplit/10_Datasets/01_Autosplit_dataset\"\n",
    "filtered_dir = os.path.join(base_dir, 'New_filtered_data_label4')\n",
    "non_filtered_dir = os.path.join(base_dir, 'New_non_filtered_data')\n",
    "\n",
    "# # Paths and Configuration\n",
    "# base_dir = \"C:/Users/meh91075/Desktop/02_AutoSplit/02_AutoSplit/10_Datasets/01_Autosplit_dataset\"\n",
    "# base_dir1 = \"C:/Users/meh91075/Downloads/04_Standardparts\"\n",
    "# filtered_dir = os.path.join(base_dir1, 'png')\n",
    "# non_filtered_dir = os.path.join(base_dir, 'New_non_filtered_data')\n",
    "\n",
    "img_height, img_width = 224, 224\n",
    "batch_size = 32\n",
    "\n",
    "def get_image_paths(directory):\n",
    "    return [os.path.join(root, file) for root, _, files in os.walk(directory) \n",
    "            for file in files if file.lower().endswith(('.png', '.jpg', '.jpeg'))]\n",
    "\n",
    "filtered_images = get_image_paths(filtered_dir)\n",
    "non_filtered_images = get_image_paths(non_filtered_dir)\n",
    "print(f\"Found images: Filtered: {len(filtered_images)}, Not filtered: {len(non_filtered_images)}\")\n",
    "\n",
    "def extract_filename(path):\n",
    "    basename = os.path.basename(path)\n",
    "    return basename.replace('_ISO', '').replace('_ISOAlt', '').split('.')[0]\n",
    "\n",
    "image_df = pd.DataFrame({\n",
    "    'image_path': filtered_images + non_filtered_images,\n",
    "    'label': [1] * len(filtered_images) + [0] * len(non_filtered_images),\n",
    "    'filename': [extract_filename(path) for path in filtered_images + non_filtered_images]\n",
    "})\n",
    "\n",
    "print(\"Sample of extracted filenames:\")\n",
    "print(image_df['filename'].head())\n",
    "\n",
    "# Adjusted merge process\n",
    "merged_df = pd.merge(df, image_df[['filename', 'image_path']], on='filename', how='left')\n",
    "merged_df['label'] = merged_df['label_majority'].map({1: 0, 2: 0, 3: 0, 4: 1})\n",
    "merged_df['has_image'] = merged_df['image_path'].notna()\n",
    "\n",
    "# Filter only entries with associated images\n",
    "merged_df_with_images = merged_df[merged_df['has_image']]\n",
    "\n",
    "# Undersampling of the majority class\n",
    "majority_class = merged_df_with_images[merged_df_with_images['label'] == 0]\n",
    "minority_class = merged_df_with_images[merged_df_with_images['label'] == 1]\n",
    "\n",
    "majority_downsampled = majority_class.sample(n=2000, random_state=42)\n",
    "balanced_df = pd.concat([majority_downsampled, minority_class])\n",
    "\n",
    "print(f\"After undersampling: Number of rows: {len(balanced_df)}\")\n",
    "print(\"Distribution of labels after undersampling:\")\n",
    "print(balanced_df['label'].value_counts())\n",
    "\n",
    "# Train-Test-Split\n",
    "train_df, val_df = train_test_split(balanced_df, test_size=0.2, stratify=balanced_df['label'], random_state=42)\n",
    "print(f\"Training data: {len(train_df)}, Validation data: {len(val_df)}\")\n",
    "\n",
    "# Determine the numeric columns\n",
    "numeric_columns = df.select_dtypes(include=[np.number]).columns.tolist()\n",
    "numeric_columns = [col for col in numeric_columns if col not in ['label_majority', 'label']]\n",
    "\n",
    "# Normalization of numeric data\n",
    "scaler = StandardScaler()\n",
    "train_df[numeric_columns] = scaler.fit_transform(train_df[numeric_columns])\n",
    "val_df[numeric_columns] = scaler.transform(val_df[numeric_columns])\n",
    "\n",
    "# Optimized data generator function with data augmentation\n",
    "def create_dataset(dataframe, is_training=True):\n",
    "    def parse_function(filename, label, *numeric_data):\n",
    "        img = tf.io.read_file(filename)\n",
    "        img = tf.image.decode_jpeg(img, channels=3)\n",
    "        img = tf.image.resize(img, [img_height, img_width])\n",
    "        img = tf.cast(img, tf.float32) / 255.0\n",
    "        if is_training:\n",
    "            img = tf.image.random_flip_left_right(img)\n",
    "            img = tf.image.random_brightness(img, max_delta=0.2)\n",
    "            img = tf.image.random_contrast(img, lower=0.8, upper=1.2)\n",
    "            img = tf.image.random_saturation(img, lower=0.8, upper=1.2)\n",
    "            img = tf.image.random_hue(img, max_delta=0.1)\n",
    "        \n",
    "        numeric_data = tf.convert_to_tensor(numeric_data, dtype=tf.float32)\n",
    "        numeric_data = tf.squeeze(numeric_data)\n",
    "        return (img, numeric_data), label\n",
    "\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((\n",
    "        dataframe['image_path'].values,\n",
    "        dataframe['label'].values,\n",
    "        dataframe[numeric_columns].values\n",
    "    ))\n",
    "    \n",
    "    dataset = dataset.map(parse_function, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "    \n",
    "    if is_training:\n",
    "        dataset = dataset.shuffle(buffer_size=len(dataframe)).repeat()\n",
    "    else:\n",
    "        dataset = dataset.repeat()\n",
    "    \n",
    "    dataset = dataset.batch(batch_size)\n",
    "    dataset = dataset.prefetch(tf.data.AUTOTUNE)\n",
    "    return dataset\n",
    "\n",
    "# Create the datasets\n",
    "train_dataset = create_dataset(train_df)\n",
    "val_dataset = create_dataset(val_df, is_training=False)\n",
    "\n",
    "# Calculate steps per epoch\n",
    "steps_per_epoch = len(train_df) // batch_size\n",
    "validation_steps = len(val_df) // batch_size\n",
    "\n",
    "# Updated optimized model definition\n",
    "def create_optimized_model(img_height, img_width, num_numeric_features):\n",
    "    # Image processing branch (CNN)\n",
    "    img_input = Input(shape=(img_height, img_width, 3))\n",
    "    \n",
    "    # 1Blue-1Orange\n",
    "    x = layers.Conv2D(64, 3, activation='relu', padding='same')(img_input)\n",
    "    x = layers.MaxPooling2D()(x)\n",
    "    \n",
    "    # 1Blue-1Orange\n",
    "    x = layers.Conv2D(128, 3, activation='relu', padding='same')(x)\n",
    "    x = layers.MaxPooling2D()(x)\n",
    "    \n",
    "    # 1Blue-1Orange\n",
    "    x = layers.Conv2D(256, 3, activation='relu', padding='same')(x)\n",
    "    x = layers.MaxPooling2D()(x)\n",
    "    \n",
    "    # 1Blue-1Orange\n",
    "    x = layers.Conv2D(512, 3, activation='relu', padding='same')(x)\n",
    "    x = layers.MaxPooling2D()(x)\n",
    "    \n",
    "    # 1Yellow (Flatten)\n",
    "    x = layers.Flatten()(x)\n",
    "    \n",
    "    # 1Green\n",
    "    x = layers.Dense(512, activation='relu')(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.Dropout(0.5)(x)\n",
    "    \n",
    "    # 1Green\n",
    "    x = layers.Dense(256, activation='relu')(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.Dropout(0.5)(x)\n",
    "    \n",
    "    # Numerical data branch (MLP)\n",
    "    num_input = Input(shape=(num_numeric_features,))\n",
    "    y = layers.Dense(64, activation='relu')(num_input)\n",
    "    y = layers.BatchNormalization()(y)\n",
    "    y = layers.Dropout(0.3)(y)\n",
    "    \n",
    "    # Combining the branches\n",
    "    combined = layers.concatenate([x, y])\n",
    "    \n",
    "    # 1 fully connected Green\n",
    "    z = layers.Dense(128, activation='relu')(combined)\n",
    "    z = layers.BatchNormalization()(z)\n",
    "    z = layers.Dropout(0.5)(z)\n",
    "    \n",
    "    # Output layer\n",
    "    output = layers.Dense(1, activation='sigmoid')(z)\n",
    "    \n",
    "    model = models.Model(inputs=[img_input, num_input], outputs=output)\n",
    "    return model\n",
    "\n",
    "# Learning Rate Logger Callback\n",
    "class LearningRateLogger(tf.keras.callbacks.Callback):\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        if logs is not None:\n",
    "            lr = self.model.optimizer.learning_rate\n",
    "            if isinstance(lr, tf.keras.optimizers.schedules.LearningRateSchedule):\n",
    "                lr = lr(self.model.optimizer.iterations)\n",
    "            logs['lr'] = tf.keras.backend.get_value(lr)\n",
    "\n",
    "# Custom ReduceLROnPlateau Callback\n",
    "class CustomReduceLROnPlateau(tf.keras.callbacks.Callback):\n",
    "    def __init__(self, factor=0.1, patience=10, min_lr=0, monitor='val_loss', mode='min'):\n",
    "        super(CustomReduceLROnPlateau, self).__init__()\n",
    "        self.factor = factor\n",
    "        self.patience = patience\n",
    "        self.min_lr = min_lr\n",
    "        self.monitor = monitor\n",
    "        self.mode = mode\n",
    "        self.best = np.Inf if mode == 'min' else -np.Inf\n",
    "        self.wait = 0\n",
    "        self.best_weights = None\n",
    "\n",
    "    def on_train_begin(self, logs=None):\n",
    "        self.best = np.Inf if self.mode == 'min' else -np.Inf\n",
    "        self.wait = 0\n",
    "        self.best_weights = None\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        logs = logs or {}\n",
    "        current = logs.get(self.monitor)\n",
    "        if current is None:\n",
    "            return\n",
    "        \n",
    "        if self.mode == 'min' and current < self.best:\n",
    "            self.best = current\n",
    "            self.wait = 0\n",
    "            self.best_weights = self.model.get_weights()\n",
    "        elif self.mode == 'max' and current > self.best:\n",
    "            self.best = current\n",
    "            self.wait = 0\n",
    "            self.best_weights = self.model.get_weights()\n",
    "        else:\n",
    "            self.wait += 1\n",
    "        \n",
    "        if self.wait >= self.patience:\n",
    "            current_lr = self.model.optimizer.learning_rate\n",
    "            if isinstance(current_lr, tf.keras.optimizers.schedules.LearningRateSchedule):\n",
    "                current_lr = current_lr(self.model.optimizer.iterations)\n",
    "            current_lr = float(current_lr.numpy())\n",
    "            \n",
    "            if current_lr > self.min_lr:\n",
    "                new_lr = max(current_lr * self.factor, self.min_lr)\n",
    "                new_lr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(\n",
    "                    initial_learning_rate=new_lr,\n",
    "                    decay_steps=1000,\n",
    "                    decay_rate=0.9,\n",
    "                    staircase=True\n",
    "                )\n",
    "                self.model.optimizer.learning_rate = new_lr_schedule\n",
    "                print(f'\\nEpoch {epoch+1}: ReduceLROnPlateau reducing learning rate to {new_lr}.')\n",
    "                self.wait = 0\n",
    "                self.model.set_weights(self.best_weights)\n",
    "\n",
    "# Compile and train\n",
    "initial_learning_rate = 1e-3\n",
    "lr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(\n",
    "    initial_learning_rate, decay_steps=1000, decay_rate=0.9, staircase=True\n",
    ")\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=lr_schedule)\n",
    "\n",
    "# Multi-GPU Strategy\n",
    "strategy = tf.distribute.MirroredStrategy()\n",
    "with strategy.scope():\n",
    "    model = create_optimized_model(img_height, img_width, len(numeric_columns))\n",
    "    model.compile(\n",
    "        optimizer=optimizer,\n",
    "        loss='binary_crossentropy',\n",
    "        metrics=['accuracy', tf.keras.metrics.AUC(name='auc_1'), tf.keras.metrics.Precision(), tf.keras.metrics.Recall()]\n",
    "    )\n",
    "\n",
    "# Callbacks\n",
    "reduce_lr = CustomReduceLROnPlateau(monitor='val_auc_1', factor=0.2, patience=5, min_lr=1e-6, mode='max')\n",
    "model_checkpoint = tf.keras.callbacks.ModelCheckpoint('best_model.h5', save_best_only=True, monitor='val_auc_1', mode='max')\n",
    "lr_logger = LearningRateLogger()\n",
    "\n",
    "# Training\n",
    "history = model.fit(\n",
    "    train_dataset,\n",
    "    epochs=100,\n",
    "    steps_per_epoch=steps_per_epoch,\n",
    "    validation_data=val_dataset,\n",
    "    validation_steps=validation_steps,\n",
    "    callbacks=[reduce_lr, model_checkpoint, lr_logger]\n",
    ")\n",
    "\n",
    "# Save model\n",
    "model.save('hybrid_model_optimized1.h5')\n",
    "\n",
    "# Evaluation\n",
    "val_loss, val_accuracy, val_auc, val_precision, val_recall = model.evaluate(val_dataset, steps=validation_steps)\n",
    "print(f\"Validation Loss: {val_loss:.4f}\")\n",
    "print(f\"Validation Accuracy: {val_accuracy:.4f}\")\n",
    "print(f\"Validation AUC: {val_auc:.4f}\")\n",
    "print(f\"Validation Precision: {val_precision:.4f}\")\n",
    "print(f\"Validation Recall: {val_recall:.4f}\")\n",
    "\n",
    "# Predictions for the validation dataset\n",
    "y_pred = model.predict(val_dataset, steps=validation_steps)\n",
    "y_pred_classes = (y_pred > 0.5).astype(\"int32\")\n",
    "y_true = np.concatenate([y for _, y in val_dataset.take(validation_steps)], axis=0)\n",
    "\n",
    "# Confusion matrix and classification report\n",
    "conf_matrix = confusion_matrix(y_true, y_pred_classes)\n",
    "class_report = classification_report(y_true, y_pred_classes)\n",
    "\n",
    "print(\"Confusion Matrix:\")\n",
    "print(conf_matrix)\n",
    "print(\"\\nClassification Report:\")\n",
    "print(class_report)\n",
    "\n",
    "# Visualization of training history (continued)\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(1, 3, 2)\n",
    "plt.plot(history.history['loss'], label='Training Loss')\n",
    "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "plt.title('Model Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(1, 3, 3)\n",
    "plt.plot(history.history['auc_1'], label='Training AUC')\n",
    "plt.plot(history.history['val_auc_1'], label='Validation AUC')\n",
    "plt.title('Model AUC')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('AUC')\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('training_history.png')\n",
    "plt.close()\n",
    "\n",
    "# Visualization of learning rate\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(history.history['lr'])\n",
    "plt.title('Learning Rate over Epochs')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Learning Rate')\n",
    "plt.yscale('log')\n",
    "plt.savefig('learning_rate.png')\n",
    "plt.close()\n",
    "\n",
    "# Visualization of confusion matrix\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.xlabel('Predicted Label')\n",
    "plt.ylabel('True Label')\n",
    "plt.savefig('confusion_matrix.png')\n",
    "plt.close()\n",
    "\n",
    "print(\"Visualizations have been saved as PNG files.\")\n",
    "\n",
    "# Save predictions and true labels\n",
    "predictions_df = pd.DataFrame({\n",
    "    'True_Label': y_true,\n",
    "    'Predicted_Probability': y_pred.flatten(),\n",
    "    'Predicted_Class': y_pred_classes.flatten()\n",
    "})\n",
    "predictions_df.to_csv('predictions.csv', index=False)\n",
    "print(\"Predictions have been saved in 'predictions.csv'.\")\n",
    "\n",
    "# Model summary\n",
    "model.summary()\n",
    "\n",
    "# Save model architecture as image\n",
    "tf.keras.utils.plot_model(model, to_file='model_architecture.png', show_shapes=True, show_layer_names=True)\n",
    "print(\"Model architecture has been saved as 'model_architecture.png'.\")\n",
    "\n",
    "# Calculate and output additional metrics\n",
    "# ROC AUC\n",
    "roc_auc = roc_auc_score(y_true, y_pred)\n",
    "print(f\"ROC AUC: {roc_auc:.4f}\")\n",
    "\n",
    "# Precision-Recall AUC\n",
    "pr_auc = average_precision_score(y_true, y_pred)\n",
    "print(f\"Precision-Recall AUC: {pr_auc:.4f}\")\n",
    "\n",
    "# F1-Score at various thresholds\n",
    "thresholds = np.arange(0, 1.01, 0.01)\n",
    "f1_scores = [f1_score(y_true, y_pred > threshold) for threshold in thresholds]\n",
    "best_threshold = thresholds[np.argmax(f1_scores)]\n",
    "best_f1 = max(f1_scores)\n",
    "print(f\"Best F1-Score: {best_f1:.4f} at threshold {best_threshold:.2f}\")\n",
    "\n",
    "# Visualization of Precision-Recall curve\n",
    "precision, recall, _ = precision_recall_curve(y_true, y_pred)\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(recall, precision, marker='.')\n",
    "plt.title('Precision-Recall Curve')\n",
    "plt.xlabel('Recall')\n",
    "plt.ylabel('Precision')\n",
    "plt.savefig('precision_recall_curve.png')\n",
    "plt.close()\n",
    "print(\"Precision-Recall curve has been saved as 'precision_recall_curve.png'.\")\n",
    "\n",
    "# Save training history\n",
    "with open('training_history.txt', 'w') as f:\n",
    "    for key, values in history.history.items():\n",
    "        f.write(f\"{key}: {values}\\n\")\n",
    "print(\"Training history has been saved in 'training_history.txt'.\")\n",
    "\n",
    "print(\"Model training and evaluation completed.\")\n",
    "\n",
    "# Visualization of ROC curve\n",
    "fpr, tpr, _ = roc_curve(y_true, y_pred)\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(fpr, tpr, label=f'ROC Curve (AUC = {roc_auc:.2f})')\n",
    "plt.plot([0, 1], [0, 1], linestyle='--', label='Random Classifier')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver Operating Characteristic (ROC) Curve')\n",
    "plt.legend()\n",
    "plt.savefig('roc_curve.png')\n",
    "plt.close()\n",
    "print(\"ROC curve has been saved as 'roc_curve.png'.\")\n",
    "\n",
    "# Analyze misclassified examples\n",
    "misclassified = np.where(y_pred_classes.flatten() != y_true)[0]\n",
    "print(f\"\\nNumber of misclassified examples: {len(misclassified)}\")\n",
    "\n",
    "if len(misclassified) > 0:\n",
    "    print(\"\\nDetails of some misclassified examples:\")\n",
    "    for i in misclassified[:5]:  # Show details for the first 5 misclassified examples\n",
    "        true_label = y_true[i]\n",
    "        pred_label = y_pred_classes[i][0]\n",
    "        pred_prob = y_pred[i][0]\n",
    "        print(f\"Example {i}: True Label: {true_label}, Predicted Label: {pred_label}, Predicted Probability: {pred_prob:.4f}\")\n",
    "\n",
    "# Model performance across different thresholds\n",
    "accuracies = [accuracy_score(y_true, y_pred > threshold) for threshold in thresholds]\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(thresholds, accuracies, label='Accuracy')\n",
    "plt.plot(thresholds, f1_scores, label='F1-Score')\n",
    "plt.axvline(x=0.5, color='r', linestyle='--', label='Default Threshold (0.5)')\n",
    "plt.xlabel('Threshold')\n",
    "plt.ylabel('Score')\n",
    "plt.title('Model Performance vs. Classification Threshold')\n",
    "plt.legend()\n",
    "plt.savefig('performance_vs_threshold.png')\n",
    "plt.close()\n",
    "print(\"Performance vs. Threshold plot has been saved as 'performance_vs_threshold.png'.\")\n",
    "\n",
    "# Optional: SHAP Analysis\n",
    "try:\n",
    "    import shap\n",
    "    \n",
    "    # Select a subset of validation data for SHAP analysis\n",
    "    num_explain = 100\n",
    "    explain_dataset = val_dataset.take(num_explain).unbatch()\n",
    "    background_dataset = train_dataset.take(num_explain).unbatch()\n",
    "    \n",
    "    # Create an Explainer\n",
    "    explainer = shap.DeepExplainer(model, background_dataset)\n",
    "    \n",
    "    # Calculate SHAP values\n",
    "    shap_values = explainer.shap_values(explain_dataset)\n",
    "    \n",
    "    # Visualize SHAP values\n",
    "    shap.summary_plot(shap_values[0], plot_type=\"bar\", feature_names=numeric_columns, show=False)\n",
    "    plt.savefig('shap_summary.png')\n",
    "    plt.close()\n",
    "    print(\"SHAP summary has been saved as 'shap_summary.png'.\")\n",
    "except ImportError:\n",
    "    print(\"SHAP is not installed. Skipping SHAP analysis.\")\n",
    "\n",
    "print(\"\\nAll analyses and visualizations are complete.\")\n",
    "\n",
    "# Summary of key metrics\n",
    "print(\"\\nSummary of key metrics:\")\n",
    "print(f\"Validation Accuracy: {val_accuracy:.4f}\")\n",
    "print(f\"Validation AUC: {val_auc:.4f}\")\n",
    "print(f\"Validation Precision: {val_precision:.4f}\")\n",
    "print(f\"Validation Recall: {val_recall:.4f}\")\n",
    "print(f\"ROC AUC: {roc_auc:.4f}\")\n",
    "print(f\"Precision-Recall AUC: {pr_auc:.4f}\")\n",
    "print(f\"Best F1-Score: {best_f1:.4f} at threshold {best_threshold:.2f}\")\n",
    "\n",
    "# Save summary to a text file\n",
    "with open('model_summary.txt', 'w') as f:\n",
    "    f.write(\"Summary of key metrics:\\n\")\n",
    "    f.write(f\"Validation Accuracy: {val_accuracy:.4f}\\n\")\n",
    "    f.write(f\"Validation AUC: {val_auc:.4f}\\n\")\n",
    "    f.write(f\"Validation Precision: {val_precision:.4f}\\n\")\n",
    "    f.write(f\"Validation Recall: {val_recall:.4f}\\n\")\n",
    "    f.write(f\"ROC AUC: {roc_auc:.4f}\\n\")\n",
    "    f.write(f\"Precision-Recall AUC: {pr_auc:.4f}\\n\")\n",
    "    f.write(f\"Best F1-Score: {best_f1:.4f} at threshold {best_threshold:.2f}\\n\")\n",
    "\n",
    "print(\"\\nA summary of key metrics has been saved in 'model_summary.txt'.\")\n",
    "print(\"\\nThe script has been executed successfully. All results and visualizations have been saved.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccf975de-b14e-4d35-8d07-267afdd6acd8",
   "metadata": {},
   "source": [
    "## final_model.h5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "dfd5a4b9-d31b-4a09-bd7c-bb627362ecb8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using MirroredStrategy with devices ('/job:localhost/replica:0/task:0/device:GPU:0',)\n",
      "\n",
      "Starting training...\n",
      "Epoch 1/100\n",
      "126/126 [==============================] - ETA: 0s - loss: 0.7532 - accuracy: 0.6196 - auc: 0.6683 - precision: 0.6149 - recall: 0.6398\n",
      "Epoch 1: val_auc improved from -inf to 0.85281, saving model to best_model.h5\n",
      "126/126 [==============================] - 26s 136ms/step - loss: 0.7532 - accuracy: 0.6196 - auc: 0.6683 - precision: 0.6149 - recall: 0.6398 - val_loss: 0.6876 - val_accuracy: 0.5508 - val_auc: 0.8528 - val_precision: 0.5268 - val_recall: 0.9980 - lr: 0.0010\n",
      "Epoch 2/100\n",
      "126/126 [==============================] - ETA: 0s - loss: 0.5773 - accuracy: 0.7192 - auc: 0.7947 - precision: 0.7134 - recall: 0.7329\n",
      "Epoch 2: val_auc did not improve from 0.85281\n",
      "126/126 [==============================] - 18s 123ms/step - loss: 0.5773 - accuracy: 0.7192 - auc: 0.7947 - precision: 0.7134 - recall: 0.7329 - val_loss: 0.5644 - val_accuracy: 0.6982 - val_auc: 0.8322 - val_precision: 0.6361 - val_recall: 0.9263 - lr: 0.0010\n",
      "Epoch 3/100\n",
      "126/126 [==============================] - ETA: 0s - loss: 0.5310 - accuracy: 0.7494 - auc: 0.8243 - precision: 0.7403 - recall: 0.7683\n",
      "Epoch 3: val_auc improved from 0.85281 to 0.87655, saving model to best_model.h5\n",
      "126/126 [==============================] - 18s 124ms/step - loss: 0.5310 - accuracy: 0.7494 - auc: 0.8243 - precision: 0.7403 - recall: 0.7683 - val_loss: 0.5708 - val_accuracy: 0.6723 - val_auc: 0.8765 - val_precision: 0.6061 - val_recall: 0.9841 - lr: 0.0010\n",
      "Epoch 4/100\n",
      "126/126 [==============================] - ETA: 0s - loss: 0.4844 - accuracy: 0.7720 - auc: 0.8523 - precision: 0.7671 - recall: 0.7813\n",
      "Epoch 4: val_auc improved from 0.87655 to 0.88782, saving model to best_model.h5\n",
      "126/126 [==============================] - 18s 124ms/step - loss: 0.4844 - accuracy: 0.7720 - auc: 0.8523 - precision: 0.7671 - recall: 0.7813 - val_loss: 0.4579 - val_accuracy: 0.7849 - val_auc: 0.8878 - val_precision: 0.7329 - val_recall: 0.8964 - lr: 0.0010\n",
      "Epoch 5/100\n",
      "126/126 [==============================] - ETA: 0s - loss: 0.4750 - accuracy: 0.7733 - auc: 0.8565 - precision: 0.7588 - recall: 0.8012\n",
      "Epoch 5: val_auc improved from 0.88782 to 0.89368, saving model to best_model.h5\n",
      "126/126 [==============================] - 19s 124ms/step - loss: 0.4750 - accuracy: 0.7733 - auc: 0.8565 - precision: 0.7588 - recall: 0.8012 - val_loss: 0.5032 - val_accuracy: 0.7271 - val_auc: 0.8937 - val_precision: 0.9222 - val_recall: 0.4960 - lr: 0.0010\n",
      "Epoch 6/100\n",
      "126/126 [==============================] - ETA: 0s - loss: 0.4500 - accuracy: 0.7870 - auc: 0.8707 - precision: 0.7775 - recall: 0.8042\n",
      "Epoch 6: val_auc did not improve from 0.89368\n",
      "126/126 [==============================] - 18s 123ms/step - loss: 0.4500 - accuracy: 0.7870 - auc: 0.8707 - precision: 0.7775 - recall: 0.8042 - val_loss: 0.8056 - val_accuracy: 0.6145 - val_auc: 0.8823 - val_precision: 0.9675 - val_recall: 0.2371 - lr: 0.0010\n",
      "Epoch 7/100\n",
      "126/126 [==============================] - ETA: 0s - loss: 0.4300 - accuracy: 0.7987 - auc: 0.8815 - precision: 0.7906 - recall: 0.8127\n",
      "Epoch 7: val_auc did not improve from 0.89368\n",
      "126/126 [==============================] - 18s 123ms/step - loss: 0.4300 - accuracy: 0.7987 - auc: 0.8815 - precision: 0.7906 - recall: 0.8127 - val_loss: 0.4453 - val_accuracy: 0.7998 - val_auc: 0.8813 - val_precision: 0.7581 - val_recall: 0.8805 - lr: 0.0010\n",
      "Epoch 8/100\n",
      "126/126 [==============================] - ETA: 0s - loss: 0.4199 - accuracy: 0.8079 - auc: 0.8880 - precision: 0.8012 - recall: 0.8191\n",
      "Epoch 8: val_auc improved from 0.89368 to 0.91234, saving model to best_model.h5\n",
      "126/126 [==============================] - 18s 124ms/step - loss: 0.4199 - accuracy: 0.8079 - auc: 0.8880 - precision: 0.8012 - recall: 0.8191 - val_loss: 0.4195 - val_accuracy: 0.8157 - val_auc: 0.9123 - val_precision: 0.8856 - val_recall: 0.7251 - lr: 0.0010\n",
      "Epoch 9/100\n",
      "126/126 [==============================] - ETA: 0s - loss: 0.4075 - accuracy: 0.8084 - auc: 0.8945 - precision: 0.8028 - recall: 0.8176\n",
      "Epoch 9: val_auc improved from 0.91234 to 0.91873, saving model to best_model.h5\n",
      "126/126 [==============================] - 18s 124ms/step - loss: 0.4075 - accuracy: 0.8084 - auc: 0.8945 - precision: 0.8028 - recall: 0.8176 - val_loss: 0.3959 - val_accuracy: 0.8277 - val_auc: 0.9187 - val_precision: 0.7831 - val_recall: 0.9064 - lr: 0.0010\n",
      "Epoch 10/100\n",
      "126/126 [==============================] - ETA: 0s - loss: 0.3968 - accuracy: 0.8154 - auc: 0.9005 - precision: 0.8082 - recall: 0.8271\n",
      "Epoch 10: val_auc improved from 0.91873 to 0.92304, saving model to best_model.h5\n",
      "126/126 [==============================] - 18s 124ms/step - loss: 0.3968 - accuracy: 0.8154 - auc: 0.9005 - precision: 0.8082 - recall: 0.8271 - val_loss: 0.3892 - val_accuracy: 0.8307 - val_auc: 0.9230 - val_precision: 0.8952 - val_recall: 0.7490 - lr: 0.0010\n",
      "Epoch 11/100\n",
      "126/126 [==============================] - ETA: 0s - loss: 0.3904 - accuracy: 0.8259 - auc: 0.9042 - precision: 0.8147 - recall: 0.8435\n",
      "Epoch 11: val_auc did not improve from 0.92304\n",
      "126/126 [==============================] - 18s 123ms/step - loss: 0.3904 - accuracy: 0.8259 - auc: 0.9042 - precision: 0.8147 - recall: 0.8435 - val_loss: 0.4881 - val_accuracy: 0.7490 - val_auc: 0.9127 - val_precision: 0.9340 - val_recall: 0.5359 - lr: 0.0010\n",
      "Epoch 12/100\n",
      "126/126 [==============================] - ETA: 0s - loss: 0.3767 - accuracy: 0.8293 - auc: 0.9106 - precision: 0.8199 - recall: 0.8440\n",
      "Epoch 12: val_auc did not improve from 0.92304\n",
      "126/126 [==============================] - 18s 123ms/step - loss: 0.3767 - accuracy: 0.8293 - auc: 0.9106 - precision: 0.8199 - recall: 0.8440 - val_loss: 0.5433 - val_accuracy: 0.7291 - val_auc: 0.9197 - val_precision: 0.6521 - val_recall: 0.9821 - lr: 0.0010\n",
      "Epoch 13/100\n",
      "126/126 [==============================] - ETA: 0s - loss: 0.3806 - accuracy: 0.8254 - auc: 0.9090 - precision: 0.8223 - recall: 0.8301\n",
      "Epoch 13: val_auc improved from 0.92304 to 0.93215, saving model to best_model.h5\n",
      "126/126 [==============================] - 18s 125ms/step - loss: 0.3806 - accuracy: 0.8254 - auc: 0.9090 - precision: 0.8223 - recall: 0.8301 - val_loss: 0.7799 - val_accuracy: 0.6185 - val_auc: 0.9322 - val_precision: 0.9685 - val_recall: 0.2450 - lr: 0.0010\n",
      "Epoch 14/100\n",
      "126/126 [==============================] - ETA: 0s - loss: 0.3661 - accuracy: 0.8326 - auc: 0.9172 - precision: 0.8274 - recall: 0.8406\n",
      "Epoch 14: val_auc did not improve from 0.93215\n",
      "126/126 [==============================] - 18s 124ms/step - loss: 0.3661 - accuracy: 0.8326 - auc: 0.9172 - precision: 0.8274 - recall: 0.8406 - val_loss: 0.3646 - val_accuracy: 0.8446 - val_auc: 0.9269 - val_precision: 0.8046 - val_recall: 0.9104 - lr: 0.0010\n",
      "Epoch 15/100\n",
      "126/126 [==============================] - ETA: 0s - loss: 0.3687 - accuracy: 0.8353 - auc: 0.9147 - precision: 0.8302 - recall: 0.8430\n",
      "Epoch 15: val_auc did not improve from 0.93215\n",
      "126/126 [==============================] - 18s 124ms/step - loss: 0.3687 - accuracy: 0.8353 - auc: 0.9147 - precision: 0.8302 - recall: 0.8430 - val_loss: 0.3606 - val_accuracy: 0.8376 - val_auc: 0.9264 - val_precision: 0.8792 - val_recall: 0.7829 - lr: 0.0010\n",
      "Epoch 16/100\n",
      "126/126 [==============================] - ETA: 0s - loss: 0.3659 - accuracy: 0.8363 - auc: 0.9169 - precision: 0.8332 - recall: 0.8411\n",
      "Epoch 16: val_auc did not improve from 0.93215\n",
      "126/126 [==============================] - 18s 124ms/step - loss: 0.3659 - accuracy: 0.8363 - auc: 0.9169 - precision: 0.8332 - recall: 0.8411 - val_loss: 2.3629 - val_accuracy: 0.5060 - val_auc: 0.7923 - val_precision: 0.8750 - val_recall: 0.0139 - lr: 0.0010\n",
      "Epoch 17/100\n",
      "126/126 [==============================] - ETA: 0s - loss: 0.3643 - accuracy: 0.8351 - auc: 0.9165 - precision: 0.8266 - recall: 0.8480\n",
      "Epoch 17: val_auc did not improve from 0.93215\n",
      "126/126 [==============================] - 19s 124ms/step - loss: 0.3643 - accuracy: 0.8351 - auc: 0.9165 - precision: 0.8266 - recall: 0.8480 - val_loss: 0.4668 - val_accuracy: 0.7859 - val_auc: 0.9113 - val_precision: 0.9184 - val_recall: 0.6275 - lr: 0.0010\n",
      "Epoch 18/100\n",
      "126/126 [==============================] - ETA: 0s - loss: 0.3531 - accuracy: 0.8391 - auc: 0.9220 - precision: 0.8347 - recall: 0.8455\n",
      "Epoch 18: ReduceLROnPlateau reducing learning rate to 0.00020000000949949026.\n",
      "\n",
      "Epoch 18: val_auc did not improve from 0.93215\n",
      "126/126 [==============================] - 18s 124ms/step - loss: 0.3531 - accuracy: 0.8391 - auc: 0.9220 - precision: 0.8347 - recall: 0.8455 - val_loss: 0.3770 - val_accuracy: 0.8357 - val_auc: 0.9271 - val_precision: 0.8983 - val_recall: 0.7570 - lr: 0.0010\n",
      "Epoch 19/100\n",
      "126/126 [==============================] - ETA: 0s - loss: 0.3449 - accuracy: 0.8545 - auc: 0.9259 - precision: 0.8416 - recall: 0.8734\n",
      "Epoch 19: val_auc improved from 0.93215 to 0.93742, saving model to best_model.h5\n",
      "126/126 [==============================] - 19s 125ms/step - loss: 0.3449 - accuracy: 0.8545 - auc: 0.9259 - precision: 0.8416 - recall: 0.8734 - val_loss: 0.3321 - val_accuracy: 0.8695 - val_auc: 0.9374 - val_precision: 0.8809 - val_recall: 0.8546 - lr: 2.0000e-04\n",
      "Epoch 20/100\n",
      "126/126 [==============================] - ETA: 0s - loss: 0.3449 - accuracy: 0.8483 - auc: 0.9262 - precision: 0.8410 - recall: 0.8590\n",
      "Epoch 20: val_auc improved from 0.93742 to 0.94226, saving model to best_model.h5\n",
      "126/126 [==============================] - 19s 125ms/step - loss: 0.3449 - accuracy: 0.8483 - auc: 0.9262 - precision: 0.8410 - recall: 0.8590 - val_loss: 0.3218 - val_accuracy: 0.8735 - val_auc: 0.9423 - val_precision: 0.8655 - val_recall: 0.8845 - lr: 2.0000e-04\n",
      "Epoch 21/100\n",
      "126/126 [==============================] - ETA: 0s - loss: 0.3430 - accuracy: 0.8495 - auc: 0.9272 - precision: 0.8414 - recall: 0.8615\n",
      "Epoch 21: val_auc did not improve from 0.94226\n",
      "126/126 [==============================] - 18s 124ms/step - loss: 0.3430 - accuracy: 0.8495 - auc: 0.9272 - precision: 0.8414 - recall: 0.8615 - val_loss: 0.3329 - val_accuracy: 0.8725 - val_auc: 0.9392 - val_precision: 0.8388 - val_recall: 0.9223 - lr: 2.0000e-04\n",
      "Epoch 22/100\n",
      "126/126 [==============================] - ETA: 0s - loss: 0.3353 - accuracy: 0.8478 - auc: 0.9300 - precision: 0.8435 - recall: 0.8540\n",
      "Epoch 22: val_auc improved from 0.94226 to 0.94233, saving model to best_model.h5\n",
      "126/126 [==============================] - 19s 124ms/step - loss: 0.3353 - accuracy: 0.8478 - auc: 0.9300 - precision: 0.8435 - recall: 0.8540 - val_loss: 0.3216 - val_accuracy: 0.8715 - val_auc: 0.9423 - val_precision: 0.8943 - val_recall: 0.8426 - lr: 2.0000e-04\n",
      "Epoch 23/100\n",
      "126/126 [==============================] - ETA: 0s - loss: 0.3429 - accuracy: 0.8433 - auc: 0.9269 - precision: 0.8424 - recall: 0.8445\n",
      "Epoch 23: val_auc did not improve from 0.94233\n",
      "126/126 [==============================] - 19s 123ms/step - loss: 0.3429 - accuracy: 0.8433 - auc: 0.9269 - precision: 0.8424 - recall: 0.8445 - val_loss: 0.3240 - val_accuracy: 0.8765 - val_auc: 0.9405 - val_precision: 0.8566 - val_recall: 0.9044 - lr: 2.0000e-04\n",
      "Epoch 24/100\n",
      "126/126 [==============================] - ETA: 0s - loss: 0.3406 - accuracy: 0.8418 - auc: 0.9278 - precision: 0.8376 - recall: 0.8480\n",
      "Epoch 24: val_auc improved from 0.94233 to 0.94457, saving model to best_model.h5\n",
      "126/126 [==============================] - 19s 124ms/step - loss: 0.3406 - accuracy: 0.8418 - auc: 0.9278 - precision: 0.8376 - recall: 0.8480 - val_loss: 0.3152 - val_accuracy: 0.8825 - val_auc: 0.9446 - val_precision: 0.8810 - val_recall: 0.8845 - lr: 2.0000e-04\n",
      "Epoch 25/100\n",
      "126/126 [==============================] - ETA: 0s - loss: 0.3301 - accuracy: 0.8585 - auc: 0.9324 - precision: 0.8536 - recall: 0.8655\n",
      "Epoch 25: val_auc did not improve from 0.94457\n",
      "126/126 [==============================] - 19s 123ms/step - loss: 0.3301 - accuracy: 0.8585 - auc: 0.9324 - precision: 0.8536 - recall: 0.8655 - val_loss: 0.3195 - val_accuracy: 0.8695 - val_auc: 0.9420 - val_precision: 0.8905 - val_recall: 0.8426 - lr: 2.0000e-04\n",
      "Epoch 26/100\n",
      "126/126 [==============================] - ETA: 0s - loss: 0.3273 - accuracy: 0.8565 - auc: 0.9335 - precision: 0.8485 - recall: 0.8680\n",
      "Epoch 26: val_auc did not improve from 0.94457\n",
      "126/126 [==============================] - 19s 123ms/step - loss: 0.3273 - accuracy: 0.8565 - auc: 0.9335 - precision: 0.8485 - recall: 0.8680 - val_loss: 0.3179 - val_accuracy: 0.8735 - val_auc: 0.9426 - val_precision: 0.8834 - val_recall: 0.8606 - lr: 2.0000e-04\n",
      "Epoch 27/100\n",
      "126/126 [==============================] - ETA: 0s - loss: 0.3396 - accuracy: 0.8540 - auc: 0.9281 - precision: 0.8481 - recall: 0.8625\n",
      "Epoch 27: val_auc did not improve from 0.94457\n",
      "126/126 [==============================] - 19s 124ms/step - loss: 0.3396 - accuracy: 0.8540 - auc: 0.9281 - precision: 0.8481 - recall: 0.8625 - val_loss: 0.3344 - val_accuracy: 0.8625 - val_auc: 0.9412 - val_precision: 0.8238 - val_recall: 0.9223 - lr: 2.0000e-04\n",
      "Epoch 28/100\n",
      "126/126 [==============================] - ETA: 0s - loss: 0.3374 - accuracy: 0.8528 - auc: 0.9297 - precision: 0.8457 - recall: 0.8630\n",
      "Epoch 28: val_auc did not improve from 0.94457\n",
      "126/126 [==============================] - 19s 123ms/step - loss: 0.3374 - accuracy: 0.8528 - auc: 0.9297 - precision: 0.8457 - recall: 0.8630 - val_loss: 0.3119 - val_accuracy: 0.8785 - val_auc: 0.9439 - val_precision: 0.8711 - val_recall: 0.8884 - lr: 2.0000e-04\n",
      "Epoch 29/100\n",
      "126/126 [==============================] - ETA: 0s - loss: 0.3289 - accuracy: 0.8572 - auc: 0.9331 - precision: 0.8494 - recall: 0.8685\n",
      "Epoch 29: ReduceLROnPlateau reducing learning rate to 4.0000001899898055e-05.\n",
      "\n",
      "Epoch 29: val_auc did not improve from 0.94457\n",
      "126/126 [==============================] - 19s 123ms/step - loss: 0.3289 - accuracy: 0.8572 - auc: 0.9331 - precision: 0.8494 - recall: 0.8685 - val_loss: 0.3121 - val_accuracy: 0.8785 - val_auc: 0.9442 - val_precision: 0.8725 - val_recall: 0.8865 - lr: 2.0000e-04\n",
      "Epoch 30/100\n",
      "126/126 [==============================] - ETA: 0s - loss: 0.3241 - accuracy: 0.8587 - auc: 0.9352 - precision: 0.8571 - recall: 0.8610\n",
      "Epoch 30: val_auc did not improve from 0.94457\n",
      "126/126 [==============================] - 19s 123ms/step - loss: 0.3241 - accuracy: 0.8587 - auc: 0.9352 - precision: 0.8571 - recall: 0.8610 - val_loss: 0.3128 - val_accuracy: 0.8765 - val_auc: 0.9443 - val_precision: 0.8677 - val_recall: 0.8884 - lr: 4.0000e-05\n",
      "Epoch 31/100\n",
      "126/126 [==============================] - ETA: 0s - loss: 0.3357 - accuracy: 0.8577 - auc: 0.9303 - precision: 0.8496 - recall: 0.8695\n",
      "Epoch 31: val_auc did not improve from 0.94457\n",
      "126/126 [==============================] - 19s 123ms/step - loss: 0.3357 - accuracy: 0.8577 - auc: 0.9303 - precision: 0.8496 - recall: 0.8695 - val_loss: 0.3136 - val_accuracy: 0.8785 - val_auc: 0.9441 - val_precision: 0.8740 - val_recall: 0.8845 - lr: 4.0000e-05\n",
      "Epoch 32/100\n",
      "126/126 [==============================] - ETA: 0s - loss: 0.3305 - accuracy: 0.8495 - auc: 0.9322 - precision: 0.8461 - recall: 0.8545\n",
      "Epoch 32: val_auc did not improve from 0.94457\n",
      "126/126 [==============================] - 19s 123ms/step - loss: 0.3305 - accuracy: 0.8495 - auc: 0.9322 - precision: 0.8461 - recall: 0.8545 - val_loss: 0.3120 - val_accuracy: 0.8785 - val_auc: 0.9442 - val_precision: 0.8770 - val_recall: 0.8805 - lr: 4.0000e-05\n",
      "Epoch 33/100\n",
      "126/126 [==============================] - ETA: 0s - loss: 0.3318 - accuracy: 0.8533 - auc: 0.9318 - precision: 0.8435 - recall: 0.8675\n",
      "Epoch 33: val_auc did not improve from 0.94457\n",
      "126/126 [==============================] - 19s 123ms/step - loss: 0.3318 - accuracy: 0.8533 - auc: 0.9318 - precision: 0.8435 - recall: 0.8675 - val_loss: 0.3121 - val_accuracy: 0.8805 - val_auc: 0.9445 - val_precision: 0.8805 - val_recall: 0.8805 - lr: 4.0000e-05\n",
      "Epoch 34/100\n",
      "126/126 [==============================] - ETA: 0s - loss: 0.3232 - accuracy: 0.8660 - auc: 0.9355 - precision: 0.8588 - recall: 0.8759Restoring model weights from the end of the best epoch: 24.\n",
      "\n",
      "Epoch 34: ReduceLROnPlateau reducing learning rate to 8.000000525498762e-06.\n",
      "\n",
      "Epoch 34: val_auc did not improve from 0.94457\n",
      "126/126 [==============================] - 19s 124ms/step - loss: 0.3232 - accuracy: 0.8660 - auc: 0.9355 - precision: 0.8588 - recall: 0.8759 - val_loss: 0.3167 - val_accuracy: 0.8775 - val_auc: 0.9437 - val_precision: 0.8555 - val_recall: 0.9084 - lr: 4.0000e-05\n",
      "Epoch 34: early stopping\n",
      "32/32 [==============================] - 1s 37ms/step - loss: 0.3152 - accuracy: 0.8825 - auc: 0.9446 - precision: 0.8810 - recall: 0.8845\n",
      "\n",
      "Validation Metrics:\n",
      "loss: 0.3152\n",
      "accuracy: 0.8825\n",
      "auc: 0.9446\n",
      "precision: 0.8810\n",
      "recall: 0.8845\n",
      "32/32 [==============================] - 1s 31ms/step\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.88      0.88      0.88       502\n",
      "           1       0.88      0.88      0.88       502\n",
      "\n",
      "    accuracy                           0.88      1004\n",
      "   macro avg       0.88      0.88      0.88      1004\n",
      "weighted avg       0.88      0.88      0.88      1004\n",
      "\n",
      "ROC AUC: 0.9446\n",
      "\n",
      "Training completed. Model and visualizations saved.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models, Input, mixed_precision\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import confusion_matrix, classification_report, f1_score, roc_auc_score\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Konstanten\n",
    "IMG_HEIGHT, IMG_WIDTH = 224, 224\n",
    "BATCH_SIZE = 32\n",
    "EPOCHS = 100\n",
    "\n",
    "# GPU Konfiguration\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    try:\n",
    "        for gpu in gpus:\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)\n",
    "    except RuntimeError as e:\n",
    "        print(e)\n",
    "\n",
    "# Mixed Precision\n",
    "if tf.test.is_built_with_cuda():\n",
    "    policy = mixed_precision.Policy('mixed_float16')\n",
    "    mixed_precision.set_global_policy(policy)\n",
    "\n",
    "def create_dataset(dataframe, numeric_columns, is_training=True):\n",
    "    def parse_function(filename, label, numeric_data):\n",
    "        # Bild verarbeiten\n",
    "        img = tf.io.read_file(filename)\n",
    "        img = tf.image.decode_jpeg(img, channels=3)\n",
    "        img = tf.image.resize(img, [IMG_HEIGHT, IMG_WIDTH])\n",
    "        img = tf.cast(img, tf.float32) / 255.0\n",
    "        \n",
    "        if is_training:\n",
    "            img = tf.image.random_flip_left_right(img)\n",
    "            img = tf.image.random_brightness(img, max_delta=0.2)\n",
    "            img = tf.image.random_contrast(img, lower=0.8, upper=1.2)\n",
    "        \n",
    "        return (img, numeric_data), label\n",
    "\n",
    "    # Dataset erstellen\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((\n",
    "        dataframe['image_path'].values,\n",
    "        dataframe['label'].values,\n",
    "        dataframe[numeric_columns].values.astype(np.float32)\n",
    "    ))\n",
    "    \n",
    "    dataset = dataset.map(parse_function, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "    if is_training:\n",
    "        dataset = dataset.shuffle(buffer_size=len(dataframe))\n",
    "    \n",
    "    dataset = dataset.batch(BATCH_SIZE)\n",
    "    dataset = dataset.prefetch(tf.data.AUTOTUNE)\n",
    "    return dataset\n",
    "\n",
    "def create_model(num_numeric_features):\n",
    "    # Image processing branch\n",
    "    img_input = Input(shape=(IMG_HEIGHT, IMG_WIDTH, 3))\n",
    "    \n",
    "    x = layers.Conv2D(64, 3, activation='relu', padding='same')(img_input)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.MaxPooling2D()(x)\n",
    "    \n",
    "    x = layers.Conv2D(128, 3, activation='relu', padding='same')(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.MaxPooling2D()(x)\n",
    "    \n",
    "    x = layers.Conv2D(256, 3, activation='relu', padding='same')(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.MaxPooling2D()(x)\n",
    "    \n",
    "    x = layers.GlobalAveragePooling2D()(x)\n",
    "    x = layers.Dropout(0.5)(x)\n",
    "    \n",
    "    # Numeric data branch\n",
    "    num_input = Input(shape=(num_numeric_features,))\n",
    "    y = layers.Dense(64, activation='relu')(num_input)\n",
    "    y = layers.BatchNormalization()(y)\n",
    "    y = layers.Dropout(0.3)(y)\n",
    "    \n",
    "    # Combine branches\n",
    "    combined = layers.concatenate([x, y])\n",
    "    \n",
    "    z = layers.Dense(128, activation='relu')(combined)\n",
    "    z = layers.BatchNormalization()(z)\n",
    "    z = layers.Dropout(0.5)(z)\n",
    "    \n",
    "    output = layers.Dense(1, activation='sigmoid')(z)\n",
    "    \n",
    "    return models.Model(inputs=[img_input, num_input], outputs=output)\n",
    "\n",
    "def main():\n",
    "    # Daten laden\n",
    "    file_path = r\"C:\\Users\\meh91075\\Downloads\\AutoSplit_dataset_09092024.csv\"\n",
    "    df = pd.read_csv(file_path)\n",
    "    \n",
    "    # Bilder laden\n",
    "    filtered_dir = os.path.join(\"C:/Users/meh91075/Desktop/02_AutoSplit/02_AutoSplit/10_Datasets/01_Autosplit_dataset\", 'New_filtered_data_label4')\n",
    "    non_filtered_dir = os.path.join(\"C:/Users/meh91075/Desktop/02_AutoSplit/02_AutoSplit/10_Datasets/01_Autosplit_dataset\", 'New_non_filtered_data')\n",
    "    \n",
    "    def get_image_paths(directory):\n",
    "        return [os.path.join(root, file) for root, _, files in os.walk(directory) \n",
    "                for file in files if file.lower().endswith(('.png', '.jpg', '.jpeg'))]\n",
    "\n",
    "    filtered_images = get_image_paths(filtered_dir)\n",
    "    non_filtered_images = get_image_paths(non_filtered_dir)\n",
    "    \n",
    "    def extract_filename(path):\n",
    "        return os.path.basename(path).replace('_ISO', '').replace('_ISOAlt', '').split('.')[0]\n",
    "\n",
    "    image_df = pd.DataFrame({\n",
    "        'image_path': filtered_images + non_filtered_images,\n",
    "        'label': [1] * len(filtered_images) + [0] * len(non_filtered_images),\n",
    "        'filename': [extract_filename(path) for path in filtered_images + non_filtered_images]\n",
    "    })\n",
    "    \n",
    "    # Daten vorbereiten\n",
    "    merged_df = pd.merge(df, image_df[['filename', 'image_path']], on='filename', how='left')\n",
    "    merged_df['label'] = merged_df['label_majority'].map({1: 0, 2: 0, 3: 0, 4: 1})\n",
    "    merged_df = merged_df[merged_df['image_path'].notna()]\n",
    "    \n",
    "    # Daten balancieren\n",
    "    minority_class = merged_df[merged_df['label'] == 1]\n",
    "    majority_class = merged_df[merged_df['label'] == 0].sample(n=len(minority_class), random_state=42)\n",
    "    balanced_df = pd.concat([majority_class, minority_class])\n",
    "    \n",
    "    # Train-Test-Split\n",
    "    train_df, val_df = train_test_split(balanced_df, test_size=0.2, stratify=balanced_df['label'], random_state=42)\n",
    "    \n",
    "    # Numerische Features\n",
    "    numeric_columns = df.select_dtypes(include=[np.number]).columns.tolist()\n",
    "    numeric_columns = [col for col in numeric_columns if col not in ['label_majority', 'label']]\n",
    "    \n",
    "    # Normalisierung\n",
    "    scaler = StandardScaler()\n",
    "    train_df[numeric_columns] = scaler.fit_transform(train_df[numeric_columns])\n",
    "    val_df[numeric_columns] = scaler.transform(val_df[numeric_columns])\n",
    "    \n",
    "    # Datasets erstellen\n",
    "    train_dataset = create_dataset(train_df, numeric_columns)\n",
    "    val_dataset = create_dataset(val_df, numeric_columns, is_training=False)\n",
    "    \n",
    "    # Model erstellen\n",
    "    strategy = tf.distribute.MirroredStrategy()\n",
    "    with strategy.scope():\n",
    "        model = create_model(len(numeric_columns))\n",
    "        \n",
    "        optimizer = tf.keras.optimizers.Adam(learning_rate=1e-3)\n",
    "        \n",
    "        model.compile(\n",
    "            optimizer=optimizer,\n",
    "            loss='binary_crossentropy',\n",
    "            metrics=['accuracy', \n",
    "                    tf.keras.metrics.AUC(name='auc'), \n",
    "                    tf.keras.metrics.Precision(name='precision'),\n",
    "                    tf.keras.metrics.Recall(name='recall')]\n",
    "        )\n",
    "    \n",
    "    # Callbacks\n",
    "    callbacks = [\n",
    "        tf.keras.callbacks.EarlyStopping(\n",
    "            monitor='val_auc',\n",
    "            patience=10,\n",
    "            mode='max',\n",
    "            restore_best_weights=True,\n",
    "            verbose=1\n",
    "        ),\n",
    "        tf.keras.callbacks.ReduceLROnPlateau(\n",
    "            monitor='val_auc',\n",
    "            factor=0.2,\n",
    "            patience=5,\n",
    "            mode='max',\n",
    "            min_lr=1e-6,\n",
    "            verbose=1\n",
    "        ),\n",
    "        tf.keras.callbacks.ModelCheckpoint(\n",
    "            'best_model.h5',\n",
    "            monitor='val_auc',\n",
    "            mode='max',\n",
    "            save_best_only=True,\n",
    "            verbose=1\n",
    "        )\n",
    "    ]\n",
    "    \n",
    "    # Training\n",
    "    print(\"\\nStarting training...\")\n",
    "    history = model.fit(\n",
    "        train_dataset,\n",
    "        epochs=EPOCHS,\n",
    "        validation_data=val_dataset,\n",
    "        callbacks=callbacks,\n",
    "        verbose=1\n",
    "    )\n",
    "    \n",
    "    # Evaluierung\n",
    "    val_metrics = model.evaluate(val_dataset)\n",
    "    print(\"\\nValidation Metrics:\")\n",
    "    for name, value in zip(model.metrics_names, val_metrics):\n",
    "        print(f\"{name}: {value:.4f}\")\n",
    "    \n",
    "    # Vorhersagen\n",
    "    y_pred = model.predict(val_dataset)\n",
    "    y_pred_classes = (y_pred > 0.5).astype(\"int32\")\n",
    "    y_true = val_df['label'].values\n",
    "    \n",
    "    # Metriken\n",
    "    print(\"\\nClassification Report:\")\n",
    "    print(classification_report(y_true, y_pred_classes))\n",
    "    \n",
    "    # ROC AUC\n",
    "    roc_auc = roc_auc_score(y_true, y_pred)\n",
    "    print(f\"ROC AUC: {roc_auc:.4f}\")\n",
    "    \n",
    "    # Confusion Matrix\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.heatmap(confusion_matrix(y_true, y_pred_classes), \n",
    "                annot=True, fmt='d', cmap='Blues')\n",
    "    plt.title('Confusion Matrix')\n",
    "    plt.xlabel('Predicted Label')\n",
    "    plt.ylabel('True Label')\n",
    "    plt.savefig('confusion_matrix.png')\n",
    "    plt.close()\n",
    "    \n",
    "    # Training History\n",
    "    plt.figure(figsize=(15, 5))\n",
    "    \n",
    "    plt.subplot(1, 3, 1)\n",
    "    plt.plot(history.history['accuracy'], label='Training')\n",
    "    plt.plot(history.history['val_accuracy'], label='Validation')\n",
    "    plt.title('Model Accuracy')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.legend()\n",
    "    \n",
    "    plt.subplot(1, 3, 2)\n",
    "    plt.plot(history.history['loss'], label='Training')\n",
    "    plt.plot(history.history['val_loss'], label='Validation')\n",
    "    plt.title('Model Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    \n",
    "    plt.subplot(1, 3, 3)\n",
    "    plt.plot(history.history['auc'], label='Training')\n",
    "    plt.plot(history.history['val_auc'], label='Validation')\n",
    "    plt.title('Model AUC')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('AUC')\n",
    "    plt.legend()\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('training_history.png')\n",
    "    plt.close()\n",
    "    \n",
    "    # Model speichern\n",
    "    model.save('final_model.h5')\n",
    "    print(\"\\nTraining completed. Model and visualizations saved.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d27f44d-ed92-4a9f-84ae-8f265adc5472",
   "metadata": {},
   "source": [
    "## hybrid_model_optimized_final_new.h5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ede66ba9-0d18-4a74-9571-44454310aa28",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original CSV data loaded. Number of rows: 19274\n",
      "Original distribution of labels:\n",
      "label_majority\n",
      "2    10850\n",
      "1     3485\n",
      "4     2745\n",
      "3     2194\n",
      "Name: count, dtype: int64\n",
      "1 Physical GPUs, 1 Logical GPUs\n",
      "WARNING:tensorflow:Mixed precision compatibility check (mixed_float16): WARNING\n",
      "Your GPU may run slowly with dtype policy mixed_float16 because it does not have compute capability of at least 7.0. Your GPU:\n",
      "  Quadro P5000, compute capability 6.1\n",
      "See https://developer.nvidia.com/cuda-gpus for a list of GPUs and their compute capabilities.\n",
      "If you will use compatible GPU(s) not attached to this host, e.g. by running a multi-worker model, you can ignore this warning. This message will only be logged once\n",
      "Mixed precision policy set to mixed_float16\n",
      "Found images: Filtered: 5664, Not filtered: 27994\n",
      "Sample of extracted filenames:\n",
      "0       00042065\n",
      "1    00042065Alt\n",
      "2       00043749\n",
      "3    00043749Alt\n",
      "4       00080967\n",
      "Name: filename, dtype: object\n",
      "After undersampling: Number of rows: 4499\n",
      "Distribution of labels after undersampling:\n",
      "label\n",
      "1    2499\n",
      "0    2000\n",
      "Name: count, dtype: int64\n",
      "Training data: 3599, Validation data: 900\n",
      "INFO:tensorflow:Using MirroredStrategy with devices ('/job:localhost/replica:0/task:0/device:GPU:0',)\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "Epoch 1/100\n",
      "112/112 [==============================] - 33s 130ms/step - loss: 0.6487 - accuracy: 0.7246 - auc: 0.7877 - precision: 0.7556 - recall: 0.7461 - val_loss: 0.3985 - val_accuracy: 0.8270 - val_auc: 0.9012 - val_precision: 0.8101 - val_recall: 0.8996 - lr: 0.0010\n",
      "Epoch 2/100\n",
      "112/112 [==============================] - 12s 110ms/step - loss: 0.4695 - accuracy: 0.7905 - auc: 0.8731 - precision: 0.8082 - recall: 0.8159 - val_loss: 0.3641 - val_accuracy: 0.8371 - val_auc: 0.9199 - val_precision: 0.8200 - val_recall: 0.9056 - lr: 0.0010\n",
      "Epoch 3/100\n",
      "112/112 [==============================] - 11s 102ms/step - loss: 0.4147 - accuracy: 0.8181 - auc: 0.8962 - precision: 0.8225 - recall: 0.8587 - val_loss: 0.3730 - val_accuracy: 0.8415 - val_auc: 0.9045 - val_precision: 0.8309 - val_recall: 0.8976 - lr: 0.0010\n",
      "Epoch 4/100\n",
      "112/112 [==============================] - 12s 110ms/step - loss: 0.3894 - accuracy: 0.8287 - auc: 0.9061 - precision: 0.8330 - recall: 0.8633 - val_loss: 0.3496 - val_accuracy: 0.8560 - val_auc: 0.9210 - val_precision: 0.8596 - val_recall: 0.8855 - lr: 0.0010\n",
      "Epoch 5/100\n",
      "112/112 [==============================] - 11s 102ms/step - loss: 0.3483 - accuracy: 0.8502 - auc: 0.9226 - precision: 0.8524 - recall: 0.8837 - val_loss: 0.3503 - val_accuracy: 0.8549 - val_auc: 0.9196 - val_precision: 0.8695 - val_recall: 0.8695 - lr: 0.0010\n",
      "Epoch 6/100\n",
      "112/112 [==============================] - 12s 110ms/step - loss: 0.3379 - accuracy: 0.8552 - auc: 0.9267 - precision: 0.8534 - recall: 0.8917 - val_loss: 0.3259 - val_accuracy: 0.8683 - val_auc: 0.9307 - val_precision: 0.8571 - val_recall: 0.9157 - lr: 0.0010\n",
      "Epoch 7/100\n",
      "112/112 [==============================] - 12s 109ms/step - loss: 0.3217 - accuracy: 0.8697 - auc: 0.9333 - precision: 0.8700 - recall: 0.9001 - val_loss: 0.3178 - val_accuracy: 0.8661 - val_auc: 0.9360 - val_precision: 0.8663 - val_recall: 0.8976 - lr: 0.0010\n",
      "Epoch 8/100\n",
      "112/112 [==============================] - 11s 101ms/step - loss: 0.3059 - accuracy: 0.8677 - auc: 0.9412 - precision: 0.8717 - recall: 0.8928 - val_loss: 0.3452 - val_accuracy: 0.8460 - val_auc: 0.9256 - val_precision: 0.8719 - val_recall: 0.8474 - lr: 0.0010\n",
      "Epoch 9/100\n",
      "112/112 [==============================] - 11s 101ms/step - loss: 0.2986 - accuracy: 0.8756 - auc: 0.9425 - precision: 0.8683 - recall: 0.9160 - val_loss: 0.3438 - val_accuracy: 0.8248 - val_auc: 0.9310 - val_precision: 0.8797 - val_recall: 0.7932 - lr: 9.0000e-04\n",
      "Epoch 10/100\n",
      "112/112 [==============================] - 11s 101ms/step - loss: 0.2945 - accuracy: 0.8736 - auc: 0.9450 - precision: 0.8763 - recall: 0.8980 - val_loss: 0.3203 - val_accuracy: 0.8638 - val_auc: 0.9343 - val_precision: 0.8715 - val_recall: 0.8855 - lr: 9.0000e-04\n",
      "Epoch 11/100\n",
      "112/112 [==============================] - 12s 109ms/step - loss: 0.2883 - accuracy: 0.8783 - auc: 0.9463 - precision: 0.8782 - recall: 0.9088 - val_loss: 0.3149 - val_accuracy: 0.8627 - val_auc: 0.9387 - val_precision: 0.8758 - val_recall: 0.8775 - lr: 9.0000e-04\n",
      "Epoch 12/100\n",
      "112/112 [==============================] - 12s 109ms/step - loss: 0.2721 - accuracy: 0.8912 - auc: 0.9519 - precision: 0.8889 - recall: 0.9185 - val_loss: 0.3087 - val_accuracy: 0.8650 - val_auc: 0.9425 - val_precision: 0.8793 - val_recall: 0.8775 - lr: 9.0000e-04\n",
      "Epoch 13/100\n",
      "112/112 [==============================] - 11s 101ms/step - loss: 0.2616 - accuracy: 0.8929 - auc: 0.9558 - precision: 0.8865 - recall: 0.9249 - val_loss: 0.3098 - val_accuracy: 0.8627 - val_auc: 0.9406 - val_precision: 0.8378 - val_recall: 0.9337 - lr: 9.0000e-04\n",
      "Epoch 14/100\n",
      "112/112 [==============================] - 11s 101ms/step - loss: 0.2622 - accuracy: 0.8898 - auc: 0.9558 - precision: 0.8874 - recall: 0.9173 - val_loss: 0.3074 - val_accuracy: 0.8650 - val_auc: 0.9409 - val_precision: 0.8733 - val_recall: 0.8855 - lr: 9.0000e-04\n",
      "Epoch 15/100\n",
      "112/112 [==============================] - 12s 109ms/step - loss: 0.2611 - accuracy: 0.8937 - auc: 0.9550 - precision: 0.8904 - recall: 0.9237 - val_loss: 0.2995 - val_accuracy: 0.8728 - val_auc: 0.9462 - val_precision: 0.8466 - val_recall: 0.9418 - lr: 9.0000e-04\n",
      "Epoch 16/100\n",
      "112/112 [==============================] - 11s 101ms/step - loss: 0.2467 - accuracy: 0.8990 - auc: 0.9607 - precision: 0.8949 - recall: 0.9249 - val_loss: 0.2941 - val_accuracy: 0.8839 - val_auc: 0.9445 - val_precision: 0.8833 - val_recall: 0.9116 - lr: 9.0000e-04\n",
      "Epoch 17/100\n",
      "112/112 [==============================] - 12s 109ms/step - loss: 0.2623 - accuracy: 0.8895 - auc: 0.9558 - precision: 0.8908 - recall: 0.9148 - val_loss: 0.2872 - val_accuracy: 0.8806 - val_auc: 0.9473 - val_precision: 0.8826 - val_recall: 0.9056 - lr: 9.0000e-04\n",
      "Epoch 18/100\n",
      "112/112 [==============================] - 12s 108ms/step - loss: 0.2412 - accuracy: 0.9004 - auc: 0.9623 - precision: 0.8968 - recall: 0.9270 - val_loss: 0.2784 - val_accuracy: 0.8873 - val_auc: 0.9512 - val_precision: 0.8738 - val_recall: 0.9317 - lr: 8.1000e-04\n",
      "Epoch 19/100\n",
      "112/112 [==============================] - 11s 100ms/step - loss: 0.2385 - accuracy: 0.9065 - auc: 0.9634 - precision: 0.9090 - recall: 0.9240 - val_loss: 0.2834 - val_accuracy: 0.8828 - val_auc: 0.9510 - val_precision: 0.8816 - val_recall: 0.9116 - lr: 8.1000e-04\n",
      "Epoch 20/100\n",
      "112/112 [==============================] - 11s 100ms/step - loss: 0.2368 - accuracy: 0.9065 - auc: 0.9632 - precision: 0.9034 - recall: 0.9311 - val_loss: 0.2812 - val_accuracy: 0.8839 - val_auc: 0.9507 - val_precision: 0.8788 - val_recall: 0.9177 - lr: 8.1000e-04\n",
      "Epoch 21/100\n",
      "112/112 [==============================] - 11s 100ms/step - loss: 0.2405 - accuracy: 0.9035 - auc: 0.9625 - precision: 0.9023 - recall: 0.9268 - val_loss: 0.2973 - val_accuracy: 0.8750 - val_auc: 0.9460 - val_precision: 0.8814 - val_recall: 0.8956 - lr: 8.1000e-04\n",
      "Epoch 22/100\n",
      "112/112 [==============================] - 11s 100ms/step - loss: 0.2266 - accuracy: 0.9085 - auc: 0.9670 - precision: 0.9069 - recall: 0.9330 - val_loss: 0.3040 - val_accuracy: 0.8616 - val_auc: 0.9438 - val_precision: 0.8740 - val_recall: 0.8775 - lr: 8.1000e-04\n",
      "Epoch 23/100\n",
      "112/112 [==============================] - ETA: 0s - loss: 0.2243 - accuracy: 0.9110 - auc: 0.9672 - precision: 0.9125 - recall: 0.9277\n",
      "Epoch 23: ReduceLROnPlateau reducing learning rate to 0.0001619999995455146.\n",
      "112/112 [==============================] - 11s 102ms/step - loss: 0.2243 - accuracy: 0.9110 - auc: 0.9672 - precision: 0.9125 - recall: 0.9277 - val_loss: 0.3127 - val_accuracy: 0.8672 - val_auc: 0.9440 - val_precision: 0.8366 - val_recall: 0.9458 - lr: 1.3122e-04\n",
      "Epoch 24/100\n",
      "112/112 [==============================] - 11s 100ms/step - loss: 0.2306 - accuracy: 0.9074 - auc: 0.9658 - precision: 0.9049 - recall: 0.9296 - val_loss: 0.2879 - val_accuracy: 0.8828 - val_auc: 0.9483 - val_precision: 0.8772 - val_recall: 0.9177 - lr: 1.3122e-04\n",
      "Epoch 25/100\n",
      "112/112 [==============================] - 11s 99ms/step - loss: 0.2340 - accuracy: 0.9035 - auc: 0.9639 - precision: 0.9027 - recall: 0.9278 - val_loss: 0.2855 - val_accuracy: 0.8806 - val_auc: 0.9500 - val_precision: 0.8668 - val_recall: 0.9277 - lr: 1.3122e-04\n",
      "Epoch 26/100\n",
      "112/112 [==============================] - 12s 107ms/step - loss: 0.2444 - accuracy: 0.9029 - auc: 0.9608 - precision: 0.8992 - recall: 0.9275 - val_loss: 0.2799 - val_accuracy: 0.8839 - val_auc: 0.9517 - val_precision: 0.8848 - val_recall: 0.9096 - lr: 1.3122e-04\n",
      "Epoch 27/100\n",
      "112/112 [==============================] - 11s 99ms/step - loss: 0.2334 - accuracy: 0.9093 - auc: 0.9650 - precision: 0.9095 - recall: 0.9291 - val_loss: 0.2839 - val_accuracy: 0.8862 - val_auc: 0.9509 - val_precision: 0.8708 - val_recall: 0.9337 - lr: 1.1810e-04\n",
      "Epoch 28/100\n",
      "112/112 [==============================] - 12s 108ms/step - loss: 0.2181 - accuracy: 0.9124 - auc: 0.9679 - precision: 0.9113 - recall: 0.9349 - val_loss: 0.2755 - val_accuracy: 0.8895 - val_auc: 0.9527 - val_precision: 0.8844 - val_recall: 0.9217 - lr: 1.1810e-04\n",
      "Epoch 29/100\n",
      "112/112 [==============================] - 11s 100ms/step - loss: 0.2393 - accuracy: 0.9021 - auc: 0.9628 - precision: 0.8977 - recall: 0.9281 - val_loss: 0.2913 - val_accuracy: 0.8817 - val_auc: 0.9494 - val_precision: 0.8828 - val_recall: 0.9076 - lr: 1.1810e-04\n",
      "Epoch 30/100\n",
      "112/112 [==============================] - 12s 108ms/step - loss: 0.2068 - accuracy: 0.9146 - auc: 0.9717 - precision: 0.9061 - recall: 0.9443 - val_loss: 0.2676 - val_accuracy: 0.8862 - val_auc: 0.9562 - val_precision: 0.8944 - val_recall: 0.9016 - lr: 1.1810e-04\n",
      "Epoch 31/100\n",
      "112/112 [==============================] - 11s 100ms/step - loss: 0.2110 - accuracy: 0.9093 - auc: 0.9719 - precision: 0.9165 - recall: 0.9215 - val_loss: 0.2844 - val_accuracy: 0.8850 - val_auc: 0.9498 - val_precision: 0.8776 - val_recall: 0.9217 - lr: 1.1810e-04\n",
      "Epoch 32/100\n",
      "112/112 [==============================] - 12s 108ms/step - loss: 0.2125 - accuracy: 0.9141 - auc: 0.9700 - precision: 0.9067 - recall: 0.9407 - val_loss: 0.2709 - val_accuracy: 0.8917 - val_auc: 0.9568 - val_precision: 0.8748 - val_recall: 0.9398 - lr: 1.1810e-04\n",
      "Epoch 33/100\n",
      "112/112 [==============================] - 12s 108ms/step - loss: 0.2060 - accuracy: 0.9230 - auc: 0.9716 - precision: 0.9232 - recall: 0.9415 - val_loss: 0.2657 - val_accuracy: 0.8895 - val_auc: 0.9573 - val_precision: 0.8919 - val_recall: 0.9116 - lr: 1.1810e-04\n",
      "Epoch 34/100\n",
      "112/112 [==============================] - 11s 100ms/step - loss: 0.2080 - accuracy: 0.9104 - auc: 0.9728 - precision: 0.9079 - recall: 0.9314 - val_loss: 0.2869 - val_accuracy: 0.8761 - val_auc: 0.9530 - val_precision: 0.8564 - val_recall: 0.9337 - lr: 1.1810e-04\n",
      "Epoch 35/100\n",
      "112/112 [==============================] - 11s 100ms/step - loss: 0.2073 - accuracy: 0.9202 - auc: 0.9710 - precision: 0.9202 - recall: 0.9394 - val_loss: 0.2765 - val_accuracy: 0.8795 - val_auc: 0.9544 - val_precision: 0.8963 - val_recall: 0.8855 - lr: 1.1810e-04\n",
      "Epoch 36/100\n",
      "112/112 [==============================] - 11s 100ms/step - loss: 0.1948 - accuracy: 0.9222 - auc: 0.9754 - precision: 0.9241 - recall: 0.9367 - val_loss: 0.2728 - val_accuracy: 0.8906 - val_auc: 0.9538 - val_precision: 0.8774 - val_recall: 0.9337 - lr: 1.0629e-04\n",
      "Epoch 37/100\n",
      "112/112 [==============================] - 11s 100ms/step - loss: 0.1936 - accuracy: 0.9241 - auc: 0.9757 - precision: 0.9221 - recall: 0.9425 - val_loss: 0.2949 - val_accuracy: 0.8817 - val_auc: 0.9491 - val_precision: 0.8784 - val_recall: 0.9137 - lr: 1.0629e-04\n",
      "Epoch 38/100\n",
      "112/112 [==============================] - ETA: 0s - loss: 0.1807 - accuracy: 0.9294 - auc: 0.9775 - precision: 0.9263 - recall: 0.9482\n",
      "Epoch 38: ReduceLROnPlateau reducing learning rate to 2.125763421645388e-05.\n",
      "112/112 [==============================] - 11s 102ms/step - loss: 0.1807 - accuracy: 0.9294 - auc: 0.9775 - precision: 0.9263 - recall: 0.9482 - val_loss: 0.2829 - val_accuracy: 0.8839 - val_auc: 0.9542 - val_precision: 0.8788 - val_recall: 0.9177 - lr: 1.3947e-05\n",
      "Epoch 39/100\n",
      "112/112 [==============================] - 11s 100ms/step - loss: 0.2127 - accuracy: 0.9149 - auc: 0.9707 - precision: 0.9148 - recall: 0.9320 - val_loss: 0.2831 - val_accuracy: 0.8795 - val_auc: 0.9534 - val_precision: 0.8585 - val_recall: 0.9378 - lr: 1.3947e-05\n",
      "Epoch 40/100\n",
      "112/112 [==============================] - 11s 99ms/step - loss: 0.1939 - accuracy: 0.9252 - auc: 0.9749 - precision: 0.9204 - recall: 0.9480 - val_loss: 0.2791 - val_accuracy: 0.8839 - val_auc: 0.9549 - val_precision: 0.8689 - val_recall: 0.9317 - lr: 1.3947e-05\n",
      "Epoch 41/100\n",
      "112/112 [==============================] - 11s 100ms/step - loss: 0.1901 - accuracy: 0.9244 - auc: 0.9761 - precision: 0.9209 - recall: 0.9454 - val_loss: 0.3081 - val_accuracy: 0.8739 - val_auc: 0.9491 - val_precision: 0.9053 - val_recall: 0.8635 - lr: 1.3947e-05\n",
      "Epoch 42/100\n",
      "112/112 [==============================] - 11s 100ms/step - loss: 0.1981 - accuracy: 0.9227 - auc: 0.9739 - precision: 0.9241 - recall: 0.9376 - val_loss: 0.2800 - val_accuracy: 0.8817 - val_auc: 0.9542 - val_precision: 0.8657 - val_recall: 0.9317 - lr: 1.3947e-05\n",
      "Epoch 43/100\n",
      "112/112 [==============================] - ETA: 0s - loss: 0.1953 - accuracy: 0.9205 - auc: 0.9755 - precision: 0.9168 - recall: 0.9414\n",
      "Epoch 43: ReduceLROnPlateau reducing learning rate to 2.7894260711036626e-06.\n",
      "112/112 [==============================] - 11s 102ms/step - loss: 0.1953 - accuracy: 0.9205 - auc: 0.9755 - precision: 0.9168 - recall: 0.9414 - val_loss: 0.2784 - val_accuracy: 0.8783 - val_auc: 0.9551 - val_precision: 0.8691 - val_recall: 0.9197 - lr: 1.8301e-06\n",
      "Epoch 44/100\n",
      "112/112 [==============================] - 11s 99ms/step - loss: 0.1963 - accuracy: 0.9227 - auc: 0.9746 - precision: 0.9203 - recall: 0.9445 - val_loss: 0.2780 - val_accuracy: 0.8817 - val_auc: 0.9528 - val_precision: 0.8798 - val_recall: 0.9116 - lr: 1.8301e-06\n",
      "Epoch 45/100\n",
      "112/112 [==============================] - 11s 100ms/step - loss: 0.1931 - accuracy: 0.9272 - auc: 0.9748 - precision: 0.9196 - recall: 0.9501 - val_loss: 0.2722 - val_accuracy: 0.8973 - val_auc: 0.9557 - val_precision: 0.9044 - val_recall: 0.9116 - lr: 1.6471e-06\n",
      "Epoch 46/100\n",
      "112/112 [==============================] - 11s 100ms/step - loss: 0.1829 - accuracy: 0.9302 - auc: 0.9782 - precision: 0.9315 - recall: 0.9454 - val_loss: 0.2965 - val_accuracy: 0.8873 - val_auc: 0.9496 - val_precision: 0.8825 - val_recall: 0.9197 - lr: 1.6471e-06\n",
      "Epoch 47/100\n",
      "112/112 [==============================] - 11s 100ms/step - loss: 0.1882 - accuracy: 0.9300 - auc: 0.9770 - precision: 0.9305 - recall: 0.9441 - val_loss: 0.2966 - val_accuracy: 0.8906 - val_auc: 0.9492 - val_precision: 0.8984 - val_recall: 0.9056 - lr: 1.6471e-06\n",
      "Epoch 48/100\n",
      "112/112 [==============================] - ETA: 0s - loss: 0.1891 - accuracy: 0.9255 - auc: 0.9769 - precision: 0.9256 - recall: 0.9410\n",
      "Epoch 48: ReduceLROnPlateau reducing learning rate to 1e-06.\n",
      "112/112 [==============================] - 11s 102ms/step - loss: 0.1891 - accuracy: 0.9255 - auc: 0.9769 - precision: 0.9256 - recall: 0.9410 - val_loss: 0.2879 - val_accuracy: 0.8817 - val_auc: 0.9512 - val_precision: 0.8769 - val_recall: 0.9157 - lr: 5.9049e-07\n",
      "Epoch 49/100\n",
      "112/112 [==============================] - 11s 100ms/step - loss: 0.1971 - accuracy: 0.9205 - auc: 0.9745 - precision: 0.9171 - recall: 0.9424 - val_loss: 0.2910 - val_accuracy: 0.8795 - val_auc: 0.9490 - val_precision: 0.8869 - val_recall: 0.8976 - lr: 5.9049e-07\n",
      "Epoch 50/100\n",
      "112/112 [==============================] - 11s 100ms/step - loss: 0.1982 - accuracy: 0.9258 - auc: 0.9741 - precision: 0.9269 - recall: 0.9416 - val_loss: 0.2764 - val_accuracy: 0.8839 - val_auc: 0.9533 - val_precision: 0.8803 - val_recall: 0.9157 - lr: 5.9049e-07\n",
      "Epoch 51/100\n",
      "112/112 [==============================] - 11s 100ms/step - loss: 0.2032 - accuracy: 0.9219 - auc: 0.9724 - precision: 0.9153 - recall: 0.9466 - val_loss: 0.2860 - val_accuracy: 0.8828 - val_auc: 0.9522 - val_precision: 0.8786 - val_recall: 0.9157 - lr: 5.9049e-07\n",
      "Epoch 52/100\n",
      "112/112 [==============================] - 11s 100ms/step - loss: 0.1837 - accuracy: 0.9277 - auc: 0.9778 - precision: 0.9233 - recall: 0.9464 - val_loss: 0.3009 - val_accuracy: 0.8795 - val_auc: 0.9471 - val_precision: 0.8611 - val_recall: 0.9337 - lr: 5.9049e-07\n",
      "Epoch 53/100\n",
      "112/112 [==============================] - 11s 100ms/step - loss: 0.1809 - accuracy: 0.9266 - auc: 0.9783 - precision: 0.9238 - recall: 0.9464 - val_loss: 0.2942 - val_accuracy: 0.8806 - val_auc: 0.9532 - val_precision: 0.8587 - val_recall: 0.9398 - lr: 5.9049e-07\n",
      "Epoch 54/100\n",
      "112/112 [==============================] - 11s 100ms/step - loss: 0.1807 - accuracy: 0.9319 - auc: 0.9782 - precision: 0.9308 - recall: 0.9476 - val_loss: 0.2922 - val_accuracy: 0.8850 - val_auc: 0.9524 - val_precision: 0.8911 - val_recall: 0.9036 - lr: 5.3144e-07\n",
      "Epoch 55/100\n",
      "112/112 [==============================] - 11s 100ms/step - loss: 0.1794 - accuracy: 0.9297 - auc: 0.9786 - precision: 0.9319 - recall: 0.9447 - val_loss: 0.3108 - val_accuracy: 0.8817 - val_auc: 0.9476 - val_precision: 0.8643 - val_recall: 0.9337 - lr: 5.3144e-07\n",
      "Epoch 56/100\n",
      "112/112 [==============================] - 11s 100ms/step - loss: 0.1852 - accuracy: 0.9222 - auc: 0.9773 - precision: 0.9161 - recall: 0.9453 - val_loss: 0.2891 - val_accuracy: 0.8862 - val_auc: 0.9498 - val_precision: 0.8852 - val_recall: 0.9137 - lr: 5.3144e-07\n",
      "Epoch 57/100\n",
      "112/112 [==============================] - 11s 100ms/step - loss: 0.1838 - accuracy: 0.9272 - auc: 0.9781 - precision: 0.9297 - recall: 0.9413 - val_loss: 0.2971 - val_accuracy: 0.8783 - val_auc: 0.9516 - val_precision: 0.8636 - val_recall: 0.9277 - lr: 5.3144e-07\n",
      "Epoch 58/100\n",
      "112/112 [==============================] - 11s 100ms/step - loss: 0.1702 - accuracy: 0.9350 - auc: 0.9805 - precision: 0.9274 - recall: 0.9554 - val_loss: 0.2954 - val_accuracy: 0.8884 - val_auc: 0.9544 - val_precision: 0.8798 - val_recall: 0.9257 - lr: 5.3144e-07\n",
      "Epoch 59/100\n",
      "112/112 [==============================] - 11s 100ms/step - loss: 0.1643 - accuracy: 0.9375 - auc: 0.9823 - precision: 0.9349 - recall: 0.9553 - val_loss: 0.2877 - val_accuracy: 0.8828 - val_auc: 0.9539 - val_precision: 0.8743 - val_recall: 0.9217 - lr: 5.3144e-07\n",
      "Epoch 60/100\n",
      "112/112 [==============================] - 11s 100ms/step - loss: 0.1678 - accuracy: 0.9358 - auc: 0.9816 - precision: 0.9346 - recall: 0.9506 - val_loss: 0.3043 - val_accuracy: 0.8839 - val_auc: 0.9490 - val_precision: 0.8818 - val_recall: 0.9137 - lr: 5.3144e-07\n",
      "Epoch 61/100\n",
      "112/112 [==============================] - 11s 100ms/step - loss: 0.1725 - accuracy: 0.9314 - auc: 0.9808 - precision: 0.9304 - recall: 0.9473 - val_loss: 0.2988 - val_accuracy: 0.8862 - val_auc: 0.9508 - val_precision: 0.8822 - val_recall: 0.9177 - lr: 5.3144e-07\n",
      "Epoch 62/100\n",
      "112/112 [==============================] - 11s 100ms/step - loss: 0.1734 - accuracy: 0.9372 - auc: 0.9797 - precision: 0.9363 - recall: 0.9518 - val_loss: 0.2870 - val_accuracy: 0.8951 - val_auc: 0.9518 - val_precision: 0.9024 - val_recall: 0.9096 - lr: 5.3144e-07\n",
      "Epoch 63/100\n",
      "112/112 [==============================] - 11s 101ms/step - loss: 0.1665 - accuracy: 0.9369 - auc: 0.9801 - precision: 0.9300 - recall: 0.9582 - val_loss: 0.3050 - val_accuracy: 0.8828 - val_auc: 0.9485 - val_precision: 0.8876 - val_recall: 0.9036 - lr: 4.7830e-07\n",
      "Epoch 64/100\n",
      "112/112 [==============================] - 11s 101ms/step - loss: 0.1588 - accuracy: 0.9411 - auc: 0.9831 - precision: 0.9415 - recall: 0.9533 - val_loss: 0.2895 - val_accuracy: 0.8917 - val_auc: 0.9518 - val_precision: 0.8908 - val_recall: 0.9177 - lr: 4.7830e-07\n",
      "Epoch 65/100\n",
      "112/112 [==============================] - 11s 101ms/step - loss: 0.1716 - accuracy: 0.9347 - auc: 0.9801 - precision: 0.9332 - recall: 0.9501 - val_loss: 0.3010 - val_accuracy: 0.8850 - val_auc: 0.9484 - val_precision: 0.8958 - val_recall: 0.8976 - lr: 4.7830e-07\n",
      "Epoch 66/100\n",
      "112/112 [==============================] - 11s 101ms/step - loss: 0.1597 - accuracy: 0.9381 - auc: 0.9819 - precision: 0.9385 - recall: 0.9525 - val_loss: 0.2959 - val_accuracy: 0.8739 - val_auc: 0.9532 - val_precision: 0.8545 - val_recall: 0.9317 - lr: 4.7830e-07\n",
      "Epoch 67/100\n",
      "112/112 [==============================] - 11s 101ms/step - loss: 0.1461 - accuracy: 0.9408 - auc: 0.9867 - precision: 0.9355 - recall: 0.9575 - val_loss: 0.3063 - val_accuracy: 0.8828 - val_auc: 0.9508 - val_precision: 0.8816 - val_recall: 0.9116 - lr: 4.7830e-07\n",
      "Epoch 68/100\n",
      "112/112 [==============================] - 11s 101ms/step - loss: 0.1582 - accuracy: 0.9422 - auc: 0.9829 - precision: 0.9467 - recall: 0.9500 - val_loss: 0.3047 - val_accuracy: 0.8761 - val_auc: 0.9538 - val_precision: 0.8550 - val_recall: 0.9357 - lr: 4.7830e-07\n",
      "Epoch 69/100\n",
      "112/112 [==============================] - 11s 101ms/step - loss: 0.1489 - accuracy: 0.9425 - auc: 0.9852 - precision: 0.9392 - recall: 0.9590 - val_loss: 0.3172 - val_accuracy: 0.8772 - val_auc: 0.9501 - val_precision: 0.8619 - val_recall: 0.9277 - lr: 4.7830e-07\n",
      "Epoch 70/100\n",
      "112/112 [==============================] - 11s 101ms/step - loss: 0.1571 - accuracy: 0.9381 - auc: 0.9836 - precision: 0.9342 - recall: 0.9558 - val_loss: 0.2895 - val_accuracy: 0.8884 - val_auc: 0.9550 - val_precision: 0.8827 - val_recall: 0.9217 - lr: 4.7830e-07\n",
      "Epoch 71/100\n",
      "112/112 [==============================] - 11s 100ms/step - loss: 0.1436 - accuracy: 0.9434 - auc: 0.9863 - precision: 0.9473 - recall: 0.9516 - val_loss: 0.3101 - val_accuracy: 0.8873 - val_auc: 0.9510 - val_precision: 0.8738 - val_recall: 0.9317 - lr: 4.7830e-07\n",
      "Epoch 72/100\n",
      "112/112 [==============================] - 11s 101ms/step - loss: 0.1465 - accuracy: 0.9442 - auc: 0.9849 - precision: 0.9348 - recall: 0.9661 - val_loss: 0.3076 - val_accuracy: 0.8873 - val_auc: 0.9513 - val_precision: 0.8869 - val_recall: 0.9137 - lr: 4.3047e-07\n",
      "Epoch 73/100\n",
      "112/112 [==============================] - 11s 101ms/step - loss: 0.1463 - accuracy: 0.9467 - auc: 0.9851 - precision: 0.9478 - recall: 0.9568 - val_loss: 0.3182 - val_accuracy: 0.8828 - val_auc: 0.9511 - val_precision: 0.8786 - val_recall: 0.9157 - lr: 4.3047e-07\n",
      "Epoch 74/100\n",
      "112/112 [==============================] - 11s 101ms/step - loss: 0.1504 - accuracy: 0.9445 - auc: 0.9848 - precision: 0.9463 - recall: 0.9552 - val_loss: 0.3327 - val_accuracy: 0.8806 - val_auc: 0.9453 - val_precision: 0.8738 - val_recall: 0.9177 - lr: 4.3047e-07\n",
      "Epoch 75/100\n",
      "112/112 [==============================] - 11s 100ms/step - loss: 0.1398 - accuracy: 0.9467 - auc: 0.9861 - precision: 0.9364 - recall: 0.9684 - val_loss: 0.3432 - val_accuracy: 0.8728 - val_auc: 0.9459 - val_precision: 0.8794 - val_recall: 0.8936 - lr: 4.3047e-07\n",
      "Epoch 76/100\n",
      "112/112 [==============================] - 11s 100ms/step - loss: 0.1579 - accuracy: 0.9434 - auc: 0.9825 - precision: 0.9360 - recall: 0.9646 - val_loss: 0.3271 - val_accuracy: 0.8694 - val_auc: 0.9488 - val_precision: 0.8629 - val_recall: 0.9096 - lr: 4.3047e-07\n",
      "Epoch 77/100\n",
      "112/112 [==============================] - 11s 100ms/step - loss: 0.1491 - accuracy: 0.9473 - auc: 0.9843 - precision: 0.9455 - recall: 0.9603 - val_loss: 0.3364 - val_accuracy: 0.8683 - val_auc: 0.9454 - val_precision: 0.8831 - val_recall: 0.8795 - lr: 4.3047e-07\n",
      "Epoch 78/100\n",
      "112/112 [==============================] - 11s 100ms/step - loss: 0.1474 - accuracy: 0.9461 - auc: 0.9850 - precision: 0.9445 - recall: 0.9601 - val_loss: 0.3202 - val_accuracy: 0.8728 - val_auc: 0.9497 - val_precision: 0.8664 - val_recall: 0.9116 - lr: 4.3047e-07\n",
      "Epoch 79/100\n",
      "112/112 [==============================] - 11s 100ms/step - loss: 0.1385 - accuracy: 0.9481 - auc: 0.9872 - precision: 0.9502 - recall: 0.9568 - val_loss: 0.3379 - val_accuracy: 0.8761 - val_auc: 0.9434 - val_precision: 0.8817 - val_recall: 0.8976 - lr: 4.3047e-07\n",
      "Epoch 80/100\n",
      "112/112 [==============================] - 11s 100ms/step - loss: 0.1596 - accuracy: 0.9386 - auc: 0.9830 - precision: 0.9346 - recall: 0.9554 - val_loss: 0.3138 - val_accuracy: 0.8850 - val_auc: 0.9508 - val_precision: 0.8776 - val_recall: 0.9217 - lr: 4.3047e-07\n",
      "Epoch 81/100\n",
      "112/112 [==============================] - 11s 100ms/step - loss: 0.1214 - accuracy: 0.9540 - auc: 0.9900 - precision: 0.9496 - recall: 0.9683 - val_loss: 0.3216 - val_accuracy: 0.8717 - val_auc: 0.9504 - val_precision: 0.8593 - val_recall: 0.9197 - lr: 3.8742e-07\n",
      "Epoch 82/100\n",
      "112/112 [==============================] - 11s 101ms/step - loss: 0.1369 - accuracy: 0.9484 - auc: 0.9871 - precision: 0.9461 - recall: 0.9626 - val_loss: 0.3347 - val_accuracy: 0.8783 - val_auc: 0.9459 - val_precision: 0.8929 - val_recall: 0.8876 - lr: 3.8742e-07\n",
      "Epoch 83/100\n",
      "112/112 [==============================] - 11s 101ms/step - loss: 0.1324 - accuracy: 0.9520 - auc: 0.9876 - precision: 0.9505 - recall: 0.9639 - val_loss: 0.3348 - val_accuracy: 0.8672 - val_auc: 0.9461 - val_precision: 0.8694 - val_recall: 0.8956 - lr: 3.8742e-07\n",
      "Epoch 84/100\n",
      "112/112 [==============================] - 11s 100ms/step - loss: 0.1363 - accuracy: 0.9492 - auc: 0.9871 - precision: 0.9463 - recall: 0.9617 - val_loss: 0.3250 - val_accuracy: 0.8862 - val_auc: 0.9495 - val_precision: 0.8867 - val_recall: 0.9116 - lr: 3.8742e-07\n",
      "Epoch 85/100\n",
      "112/112 [==============================] - 11s 100ms/step - loss: 0.1412 - accuracy: 0.9467 - auc: 0.9865 - precision: 0.9458 - recall: 0.9603 - val_loss: 0.3302 - val_accuracy: 0.8750 - val_auc: 0.9472 - val_precision: 0.8876 - val_recall: 0.8876 - lr: 3.8742e-07\n",
      "Epoch 86/100\n",
      "112/112 [==============================] - 11s 100ms/step - loss: 0.1368 - accuracy: 0.9506 - auc: 0.9873 - precision: 0.9483 - recall: 0.9631 - val_loss: 0.3348 - val_accuracy: 0.8806 - val_auc: 0.9458 - val_precision: 0.8811 - val_recall: 0.9076 - lr: 3.8742e-07\n",
      "Epoch 87/100\n",
      "112/112 [==============================] - 11s 100ms/step - loss: 0.1445 - accuracy: 0.9445 - auc: 0.9855 - precision: 0.9430 - recall: 0.9585 - val_loss: 0.3345 - val_accuracy: 0.8839 - val_auc: 0.9479 - val_precision: 0.8878 - val_recall: 0.9056 - lr: 3.8742e-07\n",
      "Epoch 88/100\n",
      "112/112 [==============================] - 11s 100ms/step - loss: 0.1196 - accuracy: 0.9545 - auc: 0.9905 - precision: 0.9475 - recall: 0.9711 - val_loss: 0.3270 - val_accuracy: 0.8873 - val_auc: 0.9472 - val_precision: 0.8946 - val_recall: 0.9036 - lr: 3.8742e-07\n",
      "Epoch 89/100\n",
      "112/112 [==============================] - 11s 100ms/step - loss: 0.1211 - accuracy: 0.9542 - auc: 0.9904 - precision: 0.9528 - recall: 0.9653 - val_loss: 0.3500 - val_accuracy: 0.8873 - val_auc: 0.9460 - val_precision: 0.8946 - val_recall: 0.9036 - lr: 3.8742e-07\n",
      "Epoch 90/100\n",
      "112/112 [==============================] - 11s 100ms/step - loss: 0.1289 - accuracy: 0.9517 - auc: 0.9883 - precision: 0.9529 - recall: 0.9627 - val_loss: 0.3281 - val_accuracy: 0.8862 - val_auc: 0.9491 - val_precision: 0.8852 - val_recall: 0.9137 - lr: 3.4868e-07\n",
      "Epoch 91/100\n",
      "112/112 [==============================] - 11s 100ms/step - loss: 0.1095 - accuracy: 0.9568 - auc: 0.9917 - precision: 0.9525 - recall: 0.9695 - val_loss: 0.3510 - val_accuracy: 0.8850 - val_auc: 0.9443 - val_precision: 0.8748 - val_recall: 0.9257 - lr: 3.4868e-07\n",
      "Epoch 92/100\n",
      "112/112 [==============================] - 11s 101ms/step - loss: 0.1313 - accuracy: 0.9487 - auc: 0.9886 - precision: 0.9521 - recall: 0.9560 - val_loss: 0.3317 - val_accuracy: 0.8806 - val_auc: 0.9478 - val_precision: 0.8752 - val_recall: 0.9157 - lr: 3.4868e-07\n",
      "Epoch 93/100\n",
      "112/112 [==============================] - 11s 100ms/step - loss: 0.1165 - accuracy: 0.9556 - auc: 0.9909 - precision: 0.9535 - recall: 0.9674 - val_loss: 0.3341 - val_accuracy: 0.8862 - val_auc: 0.9507 - val_precision: 0.8867 - val_recall: 0.9116 - lr: 3.4868e-07\n",
      "Epoch 94/100\n",
      "112/112 [==============================] - 11s 100ms/step - loss: 0.1183 - accuracy: 0.9598 - auc: 0.9893 - precision: 0.9577 - recall: 0.9703 - val_loss: 0.3397 - val_accuracy: 0.8806 - val_auc: 0.9473 - val_precision: 0.8752 - val_recall: 0.9157 - lr: 3.4868e-07\n",
      "Epoch 95/100\n",
      "112/112 [==============================] - 11s 100ms/step - loss: 0.1228 - accuracy: 0.9551 - auc: 0.9897 - precision: 0.9562 - recall: 0.9643 - val_loss: 0.3335 - val_accuracy: 0.8839 - val_auc: 0.9494 - val_precision: 0.8833 - val_recall: 0.9116 - lr: 3.4868e-07\n",
      "Epoch 96/100\n",
      "112/112 [==============================] - 11s 100ms/step - loss: 0.1293 - accuracy: 0.9495 - auc: 0.9892 - precision: 0.9437 - recall: 0.9653 - val_loss: 0.3366 - val_accuracy: 0.8839 - val_auc: 0.9475 - val_precision: 0.8924 - val_recall: 0.8996 - lr: 3.4868e-07\n",
      "Epoch 97/100\n",
      "112/112 [==============================] - 11s 100ms/step - loss: 0.1249 - accuracy: 0.9551 - auc: 0.9890 - precision: 0.9502 - recall: 0.9682 - val_loss: 0.3474 - val_accuracy: 0.8817 - val_auc: 0.9471 - val_precision: 0.8858 - val_recall: 0.9036 - lr: 3.4868e-07\n",
      "Epoch 98/100\n",
      "112/112 [==============================] - 11s 100ms/step - loss: 0.1190 - accuracy: 0.9531 - auc: 0.9909 - precision: 0.9604 - recall: 0.9576 - val_loss: 0.3353 - val_accuracy: 0.8873 - val_auc: 0.9483 - val_precision: 0.8795 - val_recall: 0.9237 - lr: 3.4868e-07\n",
      "Epoch 99/100\n",
      "112/112 [==============================] - 11s 100ms/step - loss: 0.1091 - accuracy: 0.9579 - auc: 0.9918 - precision: 0.9492 - recall: 0.9749 - val_loss: 0.3379 - val_accuracy: 0.8929 - val_auc: 0.9498 - val_precision: 0.8957 - val_recall: 0.9137 - lr: 3.1381e-07\n",
      "Epoch 100/100\n",
      "112/112 [==============================] - 11s 100ms/step - loss: 0.1095 - accuracy: 0.9568 - auc: 0.9918 - precision: 0.9574 - recall: 0.9656 - val_loss: 0.3589 - val_accuracy: 0.8884 - val_auc: 0.9417 - val_precision: 0.8902 - val_recall: 0.9116 - lr: 3.1381e-07\n",
      "28/28 [==============================] - 2s 72ms/step - loss: 0.3589 - accuracy: 0.8884 - auc: 0.9417 - precision: 0.8902 - recall: 0.9116\n",
      "Validation Loss: 0.3589\n",
      "Validation Accuracy: 0.8884\n",
      "Validation AUC: 0.9417\n",
      "Validation Precision: 0.8902\n",
      "Validation Recall: 0.9116\n",
      "28/28 [==============================] - 4s 64ms/step\n",
      "Confusion Matrix:\n",
      "[[342  56]\n",
      " [ 44 454]]\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.89      0.86      0.87       398\n",
      "           1       0.89      0.91      0.90       498\n",
      "\n",
      "    accuracy                           0.89       896\n",
      "   macro avg       0.89      0.89      0.89       896\n",
      "weighted avg       0.89      0.89      0.89       896\n",
      "\n",
      "Visualizations have been saved as PNG files.\n",
      "Predictions have been saved in 'predictions.csv'.\n",
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_1 (InputLayer)           [(None, 224, 224, 3  0           []                               \n",
      "                                )]                                                                \n",
      "                                                                                                  \n",
      " conv1_pad (ZeroPadding2D)      (None, 230, 230, 3)  0           ['input_1[0][0]']                \n",
      "                                                                                                  \n",
      " conv1_conv (Conv2D)            (None, 112, 112, 64  9472        ['conv1_pad[0][0]']              \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " pool1_pad (ZeroPadding2D)      (None, 114, 114, 64  0           ['conv1_conv[0][0]']             \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " pool1_pool (MaxPooling2D)      (None, 56, 56, 64)   0           ['pool1_pad[0][0]']              \n",
      "                                                                                                  \n",
      " conv2_block1_preact_bn (BatchN  (None, 56, 56, 64)  256         ['pool1_pool[0][0]']             \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " conv2_block1_preact_relu (Acti  (None, 56, 56, 64)  0           ['conv2_block1_preact_bn[0][0]'] \n",
      " vation)                                                                                          \n",
      "                                                                                                  \n",
      " conv2_block1_1_conv (Conv2D)   (None, 56, 56, 64)   4096        ['conv2_block1_preact_relu[0][0]'\n",
      "                                                                 ]                                \n",
      "                                                                                                  \n",
      " conv2_block1_1_bn (BatchNormal  (None, 56, 56, 64)  256         ['conv2_block1_1_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv2_block1_1_relu (Activatio  (None, 56, 56, 64)  0           ['conv2_block1_1_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv2_block1_2_pad (ZeroPaddin  (None, 58, 58, 64)  0           ['conv2_block1_1_relu[0][0]']    \n",
      " g2D)                                                                                             \n",
      "                                                                                                  \n",
      " conv2_block1_2_conv (Conv2D)   (None, 56, 56, 64)   36864       ['conv2_block1_2_pad[0][0]']     \n",
      "                                                                                                  \n",
      " conv2_block1_2_bn (BatchNormal  (None, 56, 56, 64)  256         ['conv2_block1_2_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv2_block1_2_relu (Activatio  (None, 56, 56, 64)  0           ['conv2_block1_2_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv2_block1_0_conv (Conv2D)   (None, 56, 56, 256)  16640       ['conv2_block1_preact_relu[0][0]'\n",
      "                                                                 ]                                \n",
      "                                                                                                  \n",
      " conv2_block1_3_conv (Conv2D)   (None, 56, 56, 256)  16640       ['conv2_block1_2_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv2_block1_out (Add)         (None, 56, 56, 256)  0           ['conv2_block1_0_conv[0][0]',    \n",
      "                                                                  'conv2_block1_3_conv[0][0]']    \n",
      "                                                                                                  \n",
      " conv2_block2_preact_bn (BatchN  (None, 56, 56, 256)  1024       ['conv2_block1_out[0][0]']       \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " conv2_block2_preact_relu (Acti  (None, 56, 56, 256)  0          ['conv2_block2_preact_bn[0][0]'] \n",
      " vation)                                                                                          \n",
      "                                                                                                  \n",
      " conv2_block2_1_conv (Conv2D)   (None, 56, 56, 64)   16384       ['conv2_block2_preact_relu[0][0]'\n",
      "                                                                 ]                                \n",
      "                                                                                                  \n",
      " conv2_block2_1_bn (BatchNormal  (None, 56, 56, 64)  256         ['conv2_block2_1_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv2_block2_1_relu (Activatio  (None, 56, 56, 64)  0           ['conv2_block2_1_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv2_block2_2_pad (ZeroPaddin  (None, 58, 58, 64)  0           ['conv2_block2_1_relu[0][0]']    \n",
      " g2D)                                                                                             \n",
      "                                                                                                  \n",
      " conv2_block2_2_conv (Conv2D)   (None, 56, 56, 64)   36864       ['conv2_block2_2_pad[0][0]']     \n",
      "                                                                                                  \n",
      " conv2_block2_2_bn (BatchNormal  (None, 56, 56, 64)  256         ['conv2_block2_2_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv2_block2_2_relu (Activatio  (None, 56, 56, 64)  0           ['conv2_block2_2_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv2_block2_3_conv (Conv2D)   (None, 56, 56, 256)  16640       ['conv2_block2_2_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv2_block2_out (Add)         (None, 56, 56, 256)  0           ['conv2_block1_out[0][0]',       \n",
      "                                                                  'conv2_block2_3_conv[0][0]']    \n",
      "                                                                                                  \n",
      " conv2_block3_preact_bn (BatchN  (None, 56, 56, 256)  1024       ['conv2_block2_out[0][0]']       \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " conv2_block3_preact_relu (Acti  (None, 56, 56, 256)  0          ['conv2_block3_preact_bn[0][0]'] \n",
      " vation)                                                                                          \n",
      "                                                                                                  \n",
      " conv2_block3_1_conv (Conv2D)   (None, 56, 56, 64)   16384       ['conv2_block3_preact_relu[0][0]'\n",
      "                                                                 ]                                \n",
      "                                                                                                  \n",
      " conv2_block3_1_bn (BatchNormal  (None, 56, 56, 64)  256         ['conv2_block3_1_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv2_block3_1_relu (Activatio  (None, 56, 56, 64)  0           ['conv2_block3_1_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv2_block3_2_pad (ZeroPaddin  (None, 58, 58, 64)  0           ['conv2_block3_1_relu[0][0]']    \n",
      " g2D)                                                                                             \n",
      "                                                                                                  \n",
      " conv2_block3_2_conv (Conv2D)   (None, 28, 28, 64)   36864       ['conv2_block3_2_pad[0][0]']     \n",
      "                                                                                                  \n",
      " conv2_block3_2_bn (BatchNormal  (None, 28, 28, 64)  256         ['conv2_block3_2_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv2_block3_2_relu (Activatio  (None, 28, 28, 64)  0           ['conv2_block3_2_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " max_pooling2d (MaxPooling2D)   (None, 28, 28, 256)  0           ['conv2_block2_out[0][0]']       \n",
      "                                                                                                  \n",
      " conv2_block3_3_conv (Conv2D)   (None, 28, 28, 256)  16640       ['conv2_block3_2_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv2_block3_out (Add)         (None, 28, 28, 256)  0           ['max_pooling2d[0][0]',          \n",
      "                                                                  'conv2_block3_3_conv[0][0]']    \n",
      "                                                                                                  \n",
      " conv3_block1_preact_bn (BatchN  (None, 28, 28, 256)  1024       ['conv2_block3_out[0][0]']       \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " conv3_block1_preact_relu (Acti  (None, 28, 28, 256)  0          ['conv3_block1_preact_bn[0][0]'] \n",
      " vation)                                                                                          \n",
      "                                                                                                  \n",
      " conv3_block1_1_conv (Conv2D)   (None, 28, 28, 128)  32768       ['conv3_block1_preact_relu[0][0]'\n",
      "                                                                 ]                                \n",
      "                                                                                                  \n",
      " conv3_block1_1_bn (BatchNormal  (None, 28, 28, 128)  512        ['conv3_block1_1_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv3_block1_1_relu (Activatio  (None, 28, 28, 128)  0          ['conv3_block1_1_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv3_block1_2_pad (ZeroPaddin  (None, 30, 30, 128)  0          ['conv3_block1_1_relu[0][0]']    \n",
      " g2D)                                                                                             \n",
      "                                                                                                  \n",
      " conv3_block1_2_conv (Conv2D)   (None, 28, 28, 128)  147456      ['conv3_block1_2_pad[0][0]']     \n",
      "                                                                                                  \n",
      " conv3_block1_2_bn (BatchNormal  (None, 28, 28, 128)  512        ['conv3_block1_2_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv3_block1_2_relu (Activatio  (None, 28, 28, 128)  0          ['conv3_block1_2_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv3_block1_0_conv (Conv2D)   (None, 28, 28, 512)  131584      ['conv3_block1_preact_relu[0][0]'\n",
      "                                                                 ]                                \n",
      "                                                                                                  \n",
      " conv3_block1_3_conv (Conv2D)   (None, 28, 28, 512)  66048       ['conv3_block1_2_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv3_block1_out (Add)         (None, 28, 28, 512)  0           ['conv3_block1_0_conv[0][0]',    \n",
      "                                                                  'conv3_block1_3_conv[0][0]']    \n",
      "                                                                                                  \n",
      " conv3_block2_preact_bn (BatchN  (None, 28, 28, 512)  2048       ['conv3_block1_out[0][0]']       \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " conv3_block2_preact_relu (Acti  (None, 28, 28, 512)  0          ['conv3_block2_preact_bn[0][0]'] \n",
      " vation)                                                                                          \n",
      "                                                                                                  \n",
      " conv3_block2_1_conv (Conv2D)   (None, 28, 28, 128)  65536       ['conv3_block2_preact_relu[0][0]'\n",
      "                                                                 ]                                \n",
      "                                                                                                  \n",
      " conv3_block2_1_bn (BatchNormal  (None, 28, 28, 128)  512        ['conv3_block2_1_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv3_block2_1_relu (Activatio  (None, 28, 28, 128)  0          ['conv3_block2_1_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv3_block2_2_pad (ZeroPaddin  (None, 30, 30, 128)  0          ['conv3_block2_1_relu[0][0]']    \n",
      " g2D)                                                                                             \n",
      "                                                                                                  \n",
      " conv3_block2_2_conv (Conv2D)   (None, 28, 28, 128)  147456      ['conv3_block2_2_pad[0][0]']     \n",
      "                                                                                                  \n",
      " conv3_block2_2_bn (BatchNormal  (None, 28, 28, 128)  512        ['conv3_block2_2_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv3_block2_2_relu (Activatio  (None, 28, 28, 128)  0          ['conv3_block2_2_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv3_block2_3_conv (Conv2D)   (None, 28, 28, 512)  66048       ['conv3_block2_2_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv3_block2_out (Add)         (None, 28, 28, 512)  0           ['conv3_block1_out[0][0]',       \n",
      "                                                                  'conv3_block2_3_conv[0][0]']    \n",
      "                                                                                                  \n",
      " conv3_block3_preact_bn (BatchN  (None, 28, 28, 512)  2048       ['conv3_block2_out[0][0]']       \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " conv3_block3_preact_relu (Acti  (None, 28, 28, 512)  0          ['conv3_block3_preact_bn[0][0]'] \n",
      " vation)                                                                                          \n",
      "                                                                                                  \n",
      " conv3_block3_1_conv (Conv2D)   (None, 28, 28, 128)  65536       ['conv3_block3_preact_relu[0][0]'\n",
      "                                                                 ]                                \n",
      "                                                                                                  \n",
      " conv3_block3_1_bn (BatchNormal  (None, 28, 28, 128)  512        ['conv3_block3_1_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv3_block3_1_relu (Activatio  (None, 28, 28, 128)  0          ['conv3_block3_1_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv3_block3_2_pad (ZeroPaddin  (None, 30, 30, 128)  0          ['conv3_block3_1_relu[0][0]']    \n",
      " g2D)                                                                                             \n",
      "                                                                                                  \n",
      " conv3_block3_2_conv (Conv2D)   (None, 28, 28, 128)  147456      ['conv3_block3_2_pad[0][0]']     \n",
      "                                                                                                  \n",
      " conv3_block3_2_bn (BatchNormal  (None, 28, 28, 128)  512        ['conv3_block3_2_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv3_block3_2_relu (Activatio  (None, 28, 28, 128)  0          ['conv3_block3_2_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv3_block3_3_conv (Conv2D)   (None, 28, 28, 512)  66048       ['conv3_block3_2_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv3_block3_out (Add)         (None, 28, 28, 512)  0           ['conv3_block2_out[0][0]',       \n",
      "                                                                  'conv3_block3_3_conv[0][0]']    \n",
      "                                                                                                  \n",
      " conv3_block4_preact_bn (BatchN  (None, 28, 28, 512)  2048       ['conv3_block3_out[0][0]']       \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " conv3_block4_preact_relu (Acti  (None, 28, 28, 512)  0          ['conv3_block4_preact_bn[0][0]'] \n",
      " vation)                                                                                          \n",
      "                                                                                                  \n",
      " conv3_block4_1_conv (Conv2D)   (None, 28, 28, 128)  65536       ['conv3_block4_preact_relu[0][0]'\n",
      "                                                                 ]                                \n",
      "                                                                                                  \n",
      " conv3_block4_1_bn (BatchNormal  (None, 28, 28, 128)  512        ['conv3_block4_1_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv3_block4_1_relu (Activatio  (None, 28, 28, 128)  0          ['conv3_block4_1_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv3_block4_2_pad (ZeroPaddin  (None, 30, 30, 128)  0          ['conv3_block4_1_relu[0][0]']    \n",
      " g2D)                                                                                             \n",
      "                                                                                                  \n",
      " conv3_block4_2_conv (Conv2D)   (None, 14, 14, 128)  147456      ['conv3_block4_2_pad[0][0]']     \n",
      "                                                                                                  \n",
      " conv3_block4_2_bn (BatchNormal  (None, 14, 14, 128)  512        ['conv3_block4_2_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv3_block4_2_relu (Activatio  (None, 14, 14, 128)  0          ['conv3_block4_2_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " max_pooling2d_1 (MaxPooling2D)  (None, 14, 14, 512)  0          ['conv3_block3_out[0][0]']       \n",
      "                                                                                                  \n",
      " conv3_block4_3_conv (Conv2D)   (None, 14, 14, 512)  66048       ['conv3_block4_2_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv3_block4_out (Add)         (None, 14, 14, 512)  0           ['max_pooling2d_1[0][0]',        \n",
      "                                                                  'conv3_block4_3_conv[0][0]']    \n",
      "                                                                                                  \n",
      " conv4_block1_preact_bn (BatchN  (None, 14, 14, 512)  2048       ['conv3_block4_out[0][0]']       \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " conv4_block1_preact_relu (Acti  (None, 14, 14, 512)  0          ['conv4_block1_preact_bn[0][0]'] \n",
      " vation)                                                                                          \n",
      "                                                                                                  \n",
      " conv4_block1_1_conv (Conv2D)   (None, 14, 14, 256)  131072      ['conv4_block1_preact_relu[0][0]'\n",
      "                                                                 ]                                \n",
      "                                                                                                  \n",
      " conv4_block1_1_bn (BatchNormal  (None, 14, 14, 256)  1024       ['conv4_block1_1_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv4_block1_1_relu (Activatio  (None, 14, 14, 256)  0          ['conv4_block1_1_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv4_block1_2_pad (ZeroPaddin  (None, 16, 16, 256)  0          ['conv4_block1_1_relu[0][0]']    \n",
      " g2D)                                                                                             \n",
      "                                                                                                  \n",
      " conv4_block1_2_conv (Conv2D)   (None, 14, 14, 256)  589824      ['conv4_block1_2_pad[0][0]']     \n",
      "                                                                                                  \n",
      " conv4_block1_2_bn (BatchNormal  (None, 14, 14, 256)  1024       ['conv4_block1_2_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv4_block1_2_relu (Activatio  (None, 14, 14, 256)  0          ['conv4_block1_2_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv4_block1_0_conv (Conv2D)   (None, 14, 14, 1024  525312      ['conv4_block1_preact_relu[0][0]'\n",
      "                                )                                ]                                \n",
      "                                                                                                  \n",
      " conv4_block1_3_conv (Conv2D)   (None, 14, 14, 1024  263168      ['conv4_block1_2_relu[0][0]']    \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " conv4_block1_out (Add)         (None, 14, 14, 1024  0           ['conv4_block1_0_conv[0][0]',    \n",
      "                                )                                 'conv4_block1_3_conv[0][0]']    \n",
      "                                                                                                  \n",
      " conv4_block2_preact_bn (BatchN  (None, 14, 14, 1024  4096       ['conv4_block1_out[0][0]']       \n",
      " ormalization)                  )                                                                 \n",
      "                                                                                                  \n",
      " conv4_block2_preact_relu (Acti  (None, 14, 14, 1024  0          ['conv4_block2_preact_bn[0][0]'] \n",
      " vation)                        )                                                                 \n",
      "                                                                                                  \n",
      " conv4_block2_1_conv (Conv2D)   (None, 14, 14, 256)  262144      ['conv4_block2_preact_relu[0][0]'\n",
      "                                                                 ]                                \n",
      "                                                                                                  \n",
      " conv4_block2_1_bn (BatchNormal  (None, 14, 14, 256)  1024       ['conv4_block2_1_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv4_block2_1_relu (Activatio  (None, 14, 14, 256)  0          ['conv4_block2_1_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv4_block2_2_pad (ZeroPaddin  (None, 16, 16, 256)  0          ['conv4_block2_1_relu[0][0]']    \n",
      " g2D)                                                                                             \n",
      "                                                                                                  \n",
      " conv4_block2_2_conv (Conv2D)   (None, 14, 14, 256)  589824      ['conv4_block2_2_pad[0][0]']     \n",
      "                                                                                                  \n",
      " conv4_block2_2_bn (BatchNormal  (None, 14, 14, 256)  1024       ['conv4_block2_2_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv4_block2_2_relu (Activatio  (None, 14, 14, 256)  0          ['conv4_block2_2_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv4_block2_3_conv (Conv2D)   (None, 14, 14, 1024  263168      ['conv4_block2_2_relu[0][0]']    \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " conv4_block2_out (Add)         (None, 14, 14, 1024  0           ['conv4_block1_out[0][0]',       \n",
      "                                )                                 'conv4_block2_3_conv[0][0]']    \n",
      "                                                                                                  \n",
      " conv4_block3_preact_bn (BatchN  (None, 14, 14, 1024  4096       ['conv4_block2_out[0][0]']       \n",
      " ormalization)                  )                                                                 \n",
      "                                                                                                  \n",
      " conv4_block3_preact_relu (Acti  (None, 14, 14, 1024  0          ['conv4_block3_preact_bn[0][0]'] \n",
      " vation)                        )                                                                 \n",
      "                                                                                                  \n",
      " conv4_block3_1_conv (Conv2D)   (None, 14, 14, 256)  262144      ['conv4_block3_preact_relu[0][0]'\n",
      "                                                                 ]                                \n",
      "                                                                                                  \n",
      " conv4_block3_1_bn (BatchNormal  (None, 14, 14, 256)  1024       ['conv4_block3_1_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv4_block3_1_relu (Activatio  (None, 14, 14, 256)  0          ['conv4_block3_1_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv4_block3_2_pad (ZeroPaddin  (None, 16, 16, 256)  0          ['conv4_block3_1_relu[0][0]']    \n",
      " g2D)                                                                                             \n",
      "                                                                                                  \n",
      " conv4_block3_2_conv (Conv2D)   (None, 14, 14, 256)  589824      ['conv4_block3_2_pad[0][0]']     \n",
      "                                                                                                  \n",
      " conv4_block3_2_bn (BatchNormal  (None, 14, 14, 256)  1024       ['conv4_block3_2_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv4_block3_2_relu (Activatio  (None, 14, 14, 256)  0          ['conv4_block3_2_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv4_block3_3_conv (Conv2D)   (None, 14, 14, 1024  263168      ['conv4_block3_2_relu[0][0]']    \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " conv4_block3_out (Add)         (None, 14, 14, 1024  0           ['conv4_block2_out[0][0]',       \n",
      "                                )                                 'conv4_block3_3_conv[0][0]']    \n",
      "                                                                                                  \n",
      " conv4_block4_preact_bn (BatchN  (None, 14, 14, 1024  4096       ['conv4_block3_out[0][0]']       \n",
      " ormalization)                  )                                                                 \n",
      "                                                                                                  \n",
      " conv4_block4_preact_relu (Acti  (None, 14, 14, 1024  0          ['conv4_block4_preact_bn[0][0]'] \n",
      " vation)                        )                                                                 \n",
      "                                                                                                  \n",
      " conv4_block4_1_conv (Conv2D)   (None, 14, 14, 256)  262144      ['conv4_block4_preact_relu[0][0]'\n",
      "                                                                 ]                                \n",
      "                                                                                                  \n",
      " conv4_block4_1_bn (BatchNormal  (None, 14, 14, 256)  1024       ['conv4_block4_1_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv4_block4_1_relu (Activatio  (None, 14, 14, 256)  0          ['conv4_block4_1_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv4_block4_2_pad (ZeroPaddin  (None, 16, 16, 256)  0          ['conv4_block4_1_relu[0][0]']    \n",
      " g2D)                                                                                             \n",
      "                                                                                                  \n",
      " conv4_block4_2_conv (Conv2D)   (None, 14, 14, 256)  589824      ['conv4_block4_2_pad[0][0]']     \n",
      "                                                                                                  \n",
      " conv4_block4_2_bn (BatchNormal  (None, 14, 14, 256)  1024       ['conv4_block4_2_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv4_block4_2_relu (Activatio  (None, 14, 14, 256)  0          ['conv4_block4_2_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv4_block4_3_conv (Conv2D)   (None, 14, 14, 1024  263168      ['conv4_block4_2_relu[0][0]']    \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " conv4_block4_out (Add)         (None, 14, 14, 1024  0           ['conv4_block3_out[0][0]',       \n",
      "                                )                                 'conv4_block4_3_conv[0][0]']    \n",
      "                                                                                                  \n",
      " conv4_block5_preact_bn (BatchN  (None, 14, 14, 1024  4096       ['conv4_block4_out[0][0]']       \n",
      " ormalization)                  )                                                                 \n",
      "                                                                                                  \n",
      " conv4_block5_preact_relu (Acti  (None, 14, 14, 1024  0          ['conv4_block5_preact_bn[0][0]'] \n",
      " vation)                        )                                                                 \n",
      "                                                                                                  \n",
      " conv4_block5_1_conv (Conv2D)   (None, 14, 14, 256)  262144      ['conv4_block5_preact_relu[0][0]'\n",
      "                                                                 ]                                \n",
      "                                                                                                  \n",
      " conv4_block5_1_bn (BatchNormal  (None, 14, 14, 256)  1024       ['conv4_block5_1_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv4_block5_1_relu (Activatio  (None, 14, 14, 256)  0          ['conv4_block5_1_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv4_block5_2_pad (ZeroPaddin  (None, 16, 16, 256)  0          ['conv4_block5_1_relu[0][0]']    \n",
      " g2D)                                                                                             \n",
      "                                                                                                  \n",
      " conv4_block5_2_conv (Conv2D)   (None, 14, 14, 256)  589824      ['conv4_block5_2_pad[0][0]']     \n",
      "                                                                                                  \n",
      " conv4_block5_2_bn (BatchNormal  (None, 14, 14, 256)  1024       ['conv4_block5_2_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv4_block5_2_relu (Activatio  (None, 14, 14, 256)  0          ['conv4_block5_2_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv4_block5_3_conv (Conv2D)   (None, 14, 14, 1024  263168      ['conv4_block5_2_relu[0][0]']    \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " conv4_block5_out (Add)         (None, 14, 14, 1024  0           ['conv4_block4_out[0][0]',       \n",
      "                                )                                 'conv4_block5_3_conv[0][0]']    \n",
      "                                                                                                  \n",
      " conv4_block6_preact_bn (BatchN  (None, 14, 14, 1024  4096       ['conv4_block5_out[0][0]']       \n",
      " ormalization)                  )                                                                 \n",
      "                                                                                                  \n",
      " conv4_block6_preact_relu (Acti  (None, 14, 14, 1024  0          ['conv4_block6_preact_bn[0][0]'] \n",
      " vation)                        )                                                                 \n",
      "                                                                                                  \n",
      " conv4_block6_1_conv (Conv2D)   (None, 14, 14, 256)  262144      ['conv4_block6_preact_relu[0][0]'\n",
      "                                                                 ]                                \n",
      "                                                                                                  \n",
      " conv4_block6_1_bn (BatchNormal  (None, 14, 14, 256)  1024       ['conv4_block6_1_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv4_block6_1_relu (Activatio  (None, 14, 14, 256)  0          ['conv4_block6_1_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv4_block6_2_pad (ZeroPaddin  (None, 16, 16, 256)  0          ['conv4_block6_1_relu[0][0]']    \n",
      " g2D)                                                                                             \n",
      "                                                                                                  \n",
      " conv4_block6_2_conv (Conv2D)   (None, 7, 7, 256)    589824      ['conv4_block6_2_pad[0][0]']     \n",
      "                                                                                                  \n",
      " conv4_block6_2_bn (BatchNormal  (None, 7, 7, 256)   1024        ['conv4_block6_2_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv4_block6_2_relu (Activatio  (None, 7, 7, 256)   0           ['conv4_block6_2_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " max_pooling2d_2 (MaxPooling2D)  (None, 7, 7, 1024)  0           ['conv4_block5_out[0][0]']       \n",
      "                                                                                                  \n",
      " conv4_block6_3_conv (Conv2D)   (None, 7, 7, 1024)   263168      ['conv4_block6_2_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv4_block6_out (Add)         (None, 7, 7, 1024)   0           ['max_pooling2d_2[0][0]',        \n",
      "                                                                  'conv4_block6_3_conv[0][0]']    \n",
      "                                                                                                  \n",
      " conv5_block1_preact_bn (BatchN  (None, 7, 7, 1024)  4096        ['conv4_block6_out[0][0]']       \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " conv5_block1_preact_relu (Acti  (None, 7, 7, 1024)  0           ['conv5_block1_preact_bn[0][0]'] \n",
      " vation)                                                                                          \n",
      "                                                                                                  \n",
      " conv5_block1_1_conv (Conv2D)   (None, 7, 7, 512)    524288      ['conv5_block1_preact_relu[0][0]'\n",
      "                                                                 ]                                \n",
      "                                                                                                  \n",
      " conv5_block1_1_bn (BatchNormal  (None, 7, 7, 512)   2048        ['conv5_block1_1_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv5_block1_1_relu (Activatio  (None, 7, 7, 512)   0           ['conv5_block1_1_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv5_block1_2_pad (ZeroPaddin  (None, 9, 9, 512)   0           ['conv5_block1_1_relu[0][0]']    \n",
      " g2D)                                                                                             \n",
      "                                                                                                  \n",
      " conv5_block1_2_conv (Conv2D)   (None, 7, 7, 512)    2359296     ['conv5_block1_2_pad[0][0]']     \n",
      "                                                                                                  \n",
      " conv5_block1_2_bn (BatchNormal  (None, 7, 7, 512)   2048        ['conv5_block1_2_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv5_block1_2_relu (Activatio  (None, 7, 7, 512)   0           ['conv5_block1_2_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv5_block1_0_conv (Conv2D)   (None, 7, 7, 2048)   2099200     ['conv5_block1_preact_relu[0][0]'\n",
      "                                                                 ]                                \n",
      "                                                                                                  \n",
      " conv5_block1_3_conv (Conv2D)   (None, 7, 7, 2048)   1050624     ['conv5_block1_2_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv5_block1_out (Add)         (None, 7, 7, 2048)   0           ['conv5_block1_0_conv[0][0]',    \n",
      "                                                                  'conv5_block1_3_conv[0][0]']    \n",
      "                                                                                                  \n",
      " conv5_block2_preact_bn (BatchN  (None, 7, 7, 2048)  8192        ['conv5_block1_out[0][0]']       \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " conv5_block2_preact_relu (Acti  (None, 7, 7, 2048)  0           ['conv5_block2_preact_bn[0][0]'] \n",
      " vation)                                                                                          \n",
      "                                                                                                  \n",
      " conv5_block2_1_conv (Conv2D)   (None, 7, 7, 512)    1048576     ['conv5_block2_preact_relu[0][0]'\n",
      "                                                                 ]                                \n",
      "                                                                                                  \n",
      " conv5_block2_1_bn (BatchNormal  (None, 7, 7, 512)   2048        ['conv5_block2_1_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv5_block2_1_relu (Activatio  (None, 7, 7, 512)   0           ['conv5_block2_1_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv5_block2_2_pad (ZeroPaddin  (None, 9, 9, 512)   0           ['conv5_block2_1_relu[0][0]']    \n",
      " g2D)                                                                                             \n",
      "                                                                                                  \n",
      " conv5_block2_2_conv (Conv2D)   (None, 7, 7, 512)    2359296     ['conv5_block2_2_pad[0][0]']     \n",
      "                                                                                                  \n",
      " conv5_block2_2_bn (BatchNormal  (None, 7, 7, 512)   2048        ['conv5_block2_2_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv5_block2_2_relu (Activatio  (None, 7, 7, 512)   0           ['conv5_block2_2_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv5_block2_3_conv (Conv2D)   (None, 7, 7, 2048)   1050624     ['conv5_block2_2_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv5_block2_out (Add)         (None, 7, 7, 2048)   0           ['conv5_block1_out[0][0]',       \n",
      "                                                                  'conv5_block2_3_conv[0][0]']    \n",
      "                                                                                                  \n",
      " conv5_block3_preact_bn (BatchN  (None, 7, 7, 2048)  8192        ['conv5_block2_out[0][0]']       \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " conv5_block3_preact_relu (Acti  (None, 7, 7, 2048)  0           ['conv5_block3_preact_bn[0][0]'] \n",
      " vation)                                                                                          \n",
      "                                                                                                  \n",
      " conv5_block3_1_conv (Conv2D)   (None, 7, 7, 512)    1048576     ['conv5_block3_preact_relu[0][0]'\n",
      "                                                                 ]                                \n",
      "                                                                                                  \n",
      " conv5_block3_1_bn (BatchNormal  (None, 7, 7, 512)   2048        ['conv5_block3_1_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv5_block3_1_relu (Activatio  (None, 7, 7, 512)   0           ['conv5_block3_1_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv5_block3_2_pad (ZeroPaddin  (None, 9, 9, 512)   0           ['conv5_block3_1_relu[0][0]']    \n",
      " g2D)                                                                                             \n",
      "                                                                                                  \n",
      " conv5_block3_2_conv (Conv2D)   (None, 7, 7, 512)    2359296     ['conv5_block3_2_pad[0][0]']     \n",
      "                                                                                                  \n",
      " conv5_block3_2_bn (BatchNormal  (None, 7, 7, 512)   2048        ['conv5_block3_2_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " input_2 (InputLayer)           [(None, 31)]         0           []                               \n",
      "                                                                                                  \n",
      " conv5_block3_2_relu (Activatio  (None, 7, 7, 512)   0           ['conv5_block3_2_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " dense_1 (Dense)                (None, 128)          4096        ['input_2[0][0]']                \n",
      "                                                                                                  \n",
      " conv5_block3_3_conv (Conv2D)   (None, 7, 7, 2048)   1050624     ['conv5_block3_2_relu[0][0]']    \n",
      "                                                                                                  \n",
      " batch_normalization_1 (BatchNo  (None, 128)         512         ['dense_1[0][0]']                \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " conv5_block3_out (Add)         (None, 7, 7, 2048)   0           ['conv5_block2_out[0][0]',       \n",
      "                                                                  'conv5_block3_3_conv[0][0]']    \n",
      "                                                                                                  \n",
      " dropout_1 (Dropout)            (None, 128)          0           ['batch_normalization_1[0][0]']  \n",
      "                                                                                                  \n",
      " post_bn (BatchNormalization)   (None, 7, 7, 2048)   8192        ['conv5_block3_out[0][0]']       \n",
      "                                                                                                  \n",
      " dense_2 (Dense)                (None, 256)          33024       ['dropout_1[0][0]']              \n",
      "                                                                                                  \n",
      " post_relu (Activation)         (None, 7, 7, 2048)   0           ['post_bn[0][0]']                \n",
      "                                                                                                  \n",
      " batch_normalization_2 (BatchNo  (None, 256)         1024        ['dense_2[0][0]']                \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " global_average_pooling2d (Glob  (None, 2048)        0           ['post_relu[0][0]']              \n",
      " alAveragePooling2D)                                                                              \n",
      "                                                                                                  \n",
      " dropout_2 (Dropout)            (None, 256)          0           ['batch_normalization_2[0][0]']  \n",
      "                                                                                                  \n",
      " dense (Dense)                  (None, 256)          524544      ['global_average_pooling2d[0][0]'\n",
      "                                                                 ]                                \n",
      "                                                                                                  \n",
      " dense_3 (Dense)                (None, 128)          32896       ['dropout_2[0][0]']              \n",
      "                                                                                                  \n",
      " batch_normalization (BatchNorm  (None, 256)         1024        ['dense[0][0]']                  \n",
      " alization)                                                                                       \n",
      "                                                                                                  \n",
      " batch_normalization_3 (BatchNo  (None, 128)         512         ['dense_3[0][0]']                \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " dropout (Dropout)              (None, 256)          0           ['batch_normalization[0][0]']    \n",
      "                                                                                                  \n",
      " dropout_3 (Dropout)            (None, 128)          0           ['batch_normalization_3[0][0]']  \n",
      "                                                                                                  \n",
      " concatenate (Concatenate)      (None, 384)          0           ['dropout[0][0]',                \n",
      "                                                                  'dropout_3[0][0]']              \n",
      "                                                                                                  \n",
      " dense_4 (Dense)                (None, 256)          98560       ['concatenate[0][0]']            \n",
      "                                                                                                  \n",
      " batch_normalization_4 (BatchNo  (None, 256)         1024        ['dense_4[0][0]']                \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " dropout_4 (Dropout)            (None, 256)          0           ['batch_normalization_4[0][0]']  \n",
      "                                                                                                  \n",
      " dense_5 (Dense)                (None, 128)          32896       ['dropout_4[0][0]']              \n",
      "                                                                                                  \n",
      " batch_normalization_5 (BatchNo  (None, 128)         512         ['dense_5[0][0]']                \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " dropout_5 (Dropout)            (None, 128)          0           ['batch_normalization_5[0][0]']  \n",
      "                                                                                                  \n",
      " dense_6 (Dense)                (None, 1)            129         ['dropout_5[0][0]']              \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 24,295,553\n",
      "Trainable params: 728,449\n",
      "Non-trainable params: 23,567,104\n",
      "__________________________________________________________________________________________________\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "module 'pydot' has no attribute 'InvocationException'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "File \u001b[1;32m~\\.conda\\envs\\tf-gpu\\lib\\site-packages\\pydot\\core.py:1851\u001b[0m, in \u001b[0;36mDot.create\u001b[1;34m(self, prog, format, encoding)\u001b[0m\n\u001b[0;32m   1850\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1851\u001b[0m     stdout_data, stderr_data, process \u001b[38;5;241m=\u001b[39m \u001b[43mcall_graphviz\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1852\u001b[0m \u001b[43m        \u001b[49m\u001b[43mprogram\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprog\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1853\u001b[0m \u001b[43m        \u001b[49m\u001b[43marguments\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43marguments\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1854\u001b[0m \u001b[43m        \u001b[49m\u001b[43mworking_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtmp_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1855\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1856\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[1;32m~\\.conda\\envs\\tf-gpu\\lib\\site-packages\\pydot\\core.py:211\u001b[0m, in \u001b[0;36mcall_graphviz\u001b[1;34m(program, arguments, working_dir, **kwargs)\u001b[0m\n\u001b[0;32m    209\u001b[0m program_with_args \u001b[38;5;241m=\u001b[39m [program] \u001b[38;5;241m+\u001b[39m arguments\n\u001b[1;32m--> 211\u001b[0m process \u001b[38;5;241m=\u001b[39m subprocess\u001b[38;5;241m.\u001b[39mPopen(\n\u001b[0;32m    212\u001b[0m     program_with_args,\n\u001b[0;32m    213\u001b[0m     env\u001b[38;5;241m=\u001b[39menv,\n\u001b[0;32m    214\u001b[0m     cwd\u001b[38;5;241m=\u001b[39mworking_dir,\n\u001b[0;32m    215\u001b[0m     shell\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    216\u001b[0m     stderr\u001b[38;5;241m=\u001b[39msubprocess\u001b[38;5;241m.\u001b[39mPIPE,\n\u001b[0;32m    217\u001b[0m     stdout\u001b[38;5;241m=\u001b[39msubprocess\u001b[38;5;241m.\u001b[39mPIPE,\n\u001b[0;32m    218\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m    219\u001b[0m )\n\u001b[0;32m    220\u001b[0m stdout_data, stderr_data \u001b[38;5;241m=\u001b[39m process\u001b[38;5;241m.\u001b[39mcommunicate()\n",
      "File \u001b[1;32m~\\.conda\\envs\\tf-gpu\\lib\\subprocess.py:951\u001b[0m, in \u001b[0;36mPopen.__init__\u001b[1;34m(self, args, bufsize, executable, stdin, stdout, stderr, preexec_fn, close_fds, shell, cwd, env, universal_newlines, startupinfo, creationflags, restore_signals, start_new_session, pass_fds, user, group, extra_groups, encoding, errors, text, umask)\u001b[0m\n\u001b[0;32m    948\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstderr \u001b[38;5;241m=\u001b[39m io\u001b[38;5;241m.\u001b[39mTextIOWrapper(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstderr,\n\u001b[0;32m    949\u001b[0m                     encoding\u001b[38;5;241m=\u001b[39mencoding, errors\u001b[38;5;241m=\u001b[39merrors)\n\u001b[1;32m--> 951\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execute_child\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexecutable\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpreexec_fn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclose_fds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    952\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mpass_fds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcwd\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    953\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mstartupinfo\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreationflags\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshell\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    954\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mp2cread\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mp2cwrite\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    955\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mc2pread\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mc2pwrite\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    956\u001b[0m \u001b[43m                        \u001b[49m\u001b[43merrread\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merrwrite\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    957\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mrestore_signals\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    958\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mgid\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muid\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mumask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    959\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mstart_new_session\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    960\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m:\n\u001b[0;32m    961\u001b[0m     \u001b[38;5;66;03m# Cleanup if the child failed starting.\u001b[39;00m\n",
      "File \u001b[1;32m~\\.conda\\envs\\tf-gpu\\lib\\subprocess.py:1436\u001b[0m, in \u001b[0;36mPopen._execute_child\u001b[1;34m(self, args, executable, preexec_fn, close_fds, pass_fds, cwd, env, startupinfo, creationflags, shell, p2cread, p2cwrite, c2pread, c2pwrite, errread, errwrite, unused_restore_signals, unused_gid, unused_gids, unused_uid, unused_umask, unused_start_new_session)\u001b[0m\n\u001b[0;32m   1435\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1436\u001b[0m     hp, ht, pid, tid \u001b[38;5;241m=\u001b[39m \u001b[43m_winapi\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mCreateProcess\u001b[49m\u001b[43m(\u001b[49m\u001b[43mexecutable\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1437\u001b[0m \u001b[43m                             \u001b[49m\u001b[38;5;66;43;03m# no special security\u001b[39;49;00m\n\u001b[0;32m   1438\u001b[0m \u001b[43m                             \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m   1439\u001b[0m \u001b[43m                             \u001b[49m\u001b[38;5;28;43mint\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mclose_fds\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1440\u001b[0m \u001b[43m                             \u001b[49m\u001b[43mcreationflags\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1441\u001b[0m \u001b[43m                             \u001b[49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1442\u001b[0m \u001b[43m                             \u001b[49m\u001b[43mcwd\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1443\u001b[0m \u001b[43m                             \u001b[49m\u001b[43mstartupinfo\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1444\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m   1445\u001b[0m     \u001b[38;5;66;03m# Child is launched. Close the parent's copy of those pipe\u001b[39;00m\n\u001b[0;32m   1446\u001b[0m     \u001b[38;5;66;03m# handles that only the child should have open.  You need\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1449\u001b[0m     \u001b[38;5;66;03m# pipe will not close when the child process exits and the\u001b[39;00m\n\u001b[0;32m   1450\u001b[0m     \u001b[38;5;66;03m# ReadFile will hang.\u001b[39;00m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [WinError 2] The system cannot find the file specified",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "File \u001b[1;32m~\\.conda\\envs\\tf-gpu\\lib\\site-packages\\keras\\utils\\vis_utils.py:57\u001b[0m, in \u001b[0;36mcheck_graphviz\u001b[1;34m()\u001b[0m\n\u001b[0;32m     54\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     55\u001b[0m     \u001b[38;5;66;03m# Attempt to create an image of a blank graph\u001b[39;00m\n\u001b[0;32m     56\u001b[0m     \u001b[38;5;66;03m# to check the pydot/graphviz installation.\u001b[39;00m\n\u001b[1;32m---> 57\u001b[0m     \u001b[43mpydot\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mDot\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpydot\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mDot\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     58\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[1;32m~\\.conda\\envs\\tf-gpu\\lib\\site-packages\\pydot\\core.py:1860\u001b[0m, in \u001b[0;36mDot.create\u001b[1;34m(self, prog, format, encoding)\u001b[0m\n\u001b[0;32m   1859\u001b[0m     args[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mprog\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m not found in path.\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m-> 1860\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m(\u001b[38;5;241m*\u001b[39margs)\n\u001b[0;32m   1861\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [WinError 2] \"dot\" not found in path.",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 390\u001b[0m\n\u001b[0;32m    387\u001b[0m model\u001b[38;5;241m.\u001b[39msummary()\n\u001b[0;32m    389\u001b[0m \u001b[38;5;66;03m# Save model architecture as image\u001b[39;00m\n\u001b[1;32m--> 390\u001b[0m \u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkeras\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mutils\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mplot_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mto_file\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmodel_architecture.png\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshow_shapes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshow_layer_names\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m    391\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mModel architecture has been saved as \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmodel_architecture.png\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    393\u001b[0m \u001b[38;5;66;03m# Calculate and output additional metrics\u001b[39;00m\n\u001b[0;32m    394\u001b[0m \u001b[38;5;66;03m# ROC AUC\u001b[39;00m\n",
      "File \u001b[1;32m~\\.conda\\envs\\tf-gpu\\lib\\site-packages\\keras\\utils\\vis_utils.py:436\u001b[0m, in \u001b[0;36mplot_model\u001b[1;34m(model, to_file, show_shapes, show_dtype, show_layer_names, rankdir, expand_nested, dpi, layer_range, show_layer_activations)\u001b[0m\n\u001b[0;32m    429\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m model\u001b[38;5;241m.\u001b[39mbuilt:\n\u001b[0;32m    430\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    431\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThis model has not yet been built. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    432\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBuild the model first by calling `build()` or by calling \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    433\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mthe model on a batch of data.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    434\u001b[0m     )\n\u001b[1;32m--> 436\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[43mcheck_graphviz\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[0;32m    437\u001b[0m     message \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    438\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou must install pydot (`pip install pydot`) \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    439\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mand install graphviz \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    440\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m(see instructions at https://graphviz.gitlab.io/download/) \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    441\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfor plot_model to work.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    442\u001b[0m     )\n\u001b[0;32m    443\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIPython.core.magics.namespace\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m sys\u001b[38;5;241m.\u001b[39mmodules:\n\u001b[0;32m    444\u001b[0m         \u001b[38;5;66;03m# We don't raise an exception here in order to avoid crashing\u001b[39;00m\n\u001b[0;32m    445\u001b[0m         \u001b[38;5;66;03m# notebook tests where graphviz is not available.\u001b[39;00m\n",
      "File \u001b[1;32m~\\.conda\\envs\\tf-gpu\\lib\\site-packages\\keras\\utils\\vis_utils.py:59\u001b[0m, in \u001b[0;36mcheck_graphviz\u001b[1;34m()\u001b[0m\n\u001b[0;32m     57\u001b[0m     pydot\u001b[38;5;241m.\u001b[39mDot\u001b[38;5;241m.\u001b[39mcreate(pydot\u001b[38;5;241m.\u001b[39mDot())\n\u001b[0;32m     58\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m---> 59\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mOSError\u001b[39;00m, \u001b[43mpydot\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mInvocationException\u001b[49m):\n\u001b[0;32m     60\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "\u001b[1;31mAttributeError\u001b[0m: module 'pydot' has no attribute 'InvocationException'"
     ]
    }
   ],
   "source": [
    "                                                       ##### Asle jens ine ###########\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models, Input, mixed_precision\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import confusion_matrix, classification_report, f1_score, roc_auc_score, precision_recall_curve, average_precision_score, accuracy_score, roc_curve\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Load the DataFrame\n",
    "file_path = r\"C:\\Users\\meh91075\\Downloads\\AutoSplit_all_in_one_dataset_05112024.csv\"\n",
    "#\"C:\\Users\\meh91075\\Downloads\\merged_features_filtered.csv\"\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# Entferne label_standard\n",
    "df = df.drop('label_standard', axis=1)\n",
    "\n",
    "\n",
    "print(f\"Original CSV data loaded. Number of rows: {len(df)}\")\n",
    "print(f\"Original distribution of labels:\")\n",
    "print(df['label_majority'].value_counts())\n",
    "\n",
    "# Optimize GPU configuration\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    try:\n",
    "        for gpu in gpus:\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)\n",
    "        logical_gpus = tf.config.experimental.list_logical_devices('GPU')\n",
    "        print(f\"{len(gpus)} Physical GPUs, {len(logical_gpus)} Logical GPUs\")\n",
    "    except RuntimeError as e:\n",
    "        print(e)\n",
    "\n",
    "# Activate Mixed Precision\n",
    "if tf.test.is_built_with_cuda():\n",
    "    policy = mixed_precision.Policy('mixed_float16')\n",
    "    mixed_precision.set_global_policy(policy)\n",
    "    print(\"Mixed precision policy set to mixed_float16\")\n",
    "else:\n",
    "    print(\"CUDA not available, using default precision\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Pfade und Konfiguration\n",
    "base_dir =  r\"C:\\Users\\meh91075\\Desktop\\DataSet(S1)\"\n",
    "filtered_dir = os.path.join(base_dir, 'New_filtered_data_label0')\n",
    "non_filtered_dir = os.path.join(base_dir, 'New_non_filtered_data')\n",
    "\n",
    "\n",
    "\n",
    "img_height, img_width = 224, 224\n",
    "batch_size = 32\n",
    "\n",
    "def get_image_paths(directory):\n",
    "    return [os.path.join(root, file) for root, _, files in os.walk(directory) \n",
    "            for file in files if file.lower().endswith(('.png', '.jpg', '.jpeg'))]\n",
    "\n",
    "filtered_images = get_image_paths(filtered_dir)\n",
    "non_filtered_images = get_image_paths(non_filtered_dir)\n",
    "print(f\"Found images: Filtered: {len(filtered_images)}, Not filtered: {len(non_filtered_images)}\")\n",
    "\n",
    "def extract_filename(path):\n",
    "    basename = os.path.basename(path)\n",
    "    return basename.replace('_ISO', '').replace('_ISOAlt', '').split('.')[0]\n",
    "\n",
    "image_df = pd.DataFrame({\n",
    "    'image_path': filtered_images + non_filtered_images,\n",
    "    'label': [1] * len(filtered_images) + [0] * len(non_filtered_images),\n",
    "    'filename': [extract_filename(path) for path in filtered_images + non_filtered_images]\n",
    "})\n",
    "\n",
    "print(\"Sample of extracted filenames:\")\n",
    "print(image_df['filename'].head())\n",
    "\n",
    "# Adjusted merge process\n",
    "merged_df = pd.merge(df, image_df[['filename', 'image_path']], on='filename', how='left')\n",
    "merged_df['label'] = merged_df['label_majority'].map({1: 0, 2: 0, 3: 0, 4: 1})\n",
    "merged_df['has_image'] = merged_df['image_path'].notna()\n",
    "\n",
    "# Filter only entries with associated images\n",
    "merged_df_with_images = merged_df[merged_df['has_image']]\n",
    "\n",
    "# Undersampling of the majority class\n",
    "majority_class = merged_df_with_images[merged_df_with_images['label'] == 0]\n",
    "minority_class = merged_df_with_images[merged_df_with_images['label'] == 1]\n",
    "\n",
    "majority_downsampled = majority_class.sample(n=2000, random_state=42)\n",
    "balanced_df = pd.concat([majority_downsampled, minority_class])\n",
    "\n",
    "print(f\"After undersampling: Number of rows: {len(balanced_df)}\")\n",
    "print(\"Distribution of labels after undersampling:\")\n",
    "print(balanced_df['label'].value_counts())\n",
    "\n",
    "# Train-Test-Split\n",
    "train_df, val_df = train_test_split(balanced_df, test_size=0.2, stratify=balanced_df['label'], random_state=42)\n",
    "print(f\"Training data: {len(train_df)}, Validation data: {len(val_df)}\")\n",
    "\n",
    "# Determine the numeric columns\n",
    "numeric_columns = df.select_dtypes(include=[np.number]).columns.tolist()\n",
    "numeric_columns = [col for col in numeric_columns if col not in ['label_majority', 'label']]\n",
    "\n",
    "# Normalization of numeric data\n",
    "scaler = StandardScaler()\n",
    "train_df[numeric_columns] = scaler.fit_transform(train_df[numeric_columns])\n",
    "val_df[numeric_columns] = scaler.transform(val_df[numeric_columns])\n",
    "\n",
    "# Optimized data generator function with data augmentation\n",
    "def create_dataset(dataframe, is_training=True):\n",
    "    def parse_function(filename, label, *numeric_data):\n",
    "        img = tf.io.read_file(filename)\n",
    "        img = tf.image.decode_jpeg(img, channels=3)\n",
    "        img = tf.image.resize(img, [img_height, img_width])\n",
    "        img = tf.keras.applications.resnet_v2.preprocess_input(img)\n",
    "        \n",
    "        if is_training:\n",
    "            img = tf.image.random_flip_left_right(img)\n",
    "            img = tf.image.random_brightness(img, max_delta=0.2)\n",
    "            img = tf.image.random_contrast(img, lower=0.8, upper=1.2)\n",
    "            img = tf.image.random_saturation(img, lower=0.8, upper=1.2)\n",
    "            img = tf.image.random_hue(img, max_delta=0.1)\n",
    "        \n",
    "        numeric_data = tf.convert_to_tensor(numeric_data, dtype=tf.float32)\n",
    "        numeric_data = tf.squeeze(numeric_data)\n",
    "        return (img, numeric_data), label\n",
    "\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((\n",
    "        dataframe['image_path'].values,\n",
    "        dataframe['label'].values,\n",
    "        dataframe[numeric_columns].values\n",
    "    ))\n",
    "    \n",
    "    dataset = dataset.map(parse_function, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "    \n",
    "    if is_training:\n",
    "        dataset = dataset.shuffle(buffer_size=len(dataframe)).repeat()\n",
    "    else:\n",
    "        dataset = dataset.repeat()\n",
    "    \n",
    "    dataset = dataset.batch(batch_size)\n",
    "    dataset = dataset.prefetch(tf.data.AUTOTUNE)\n",
    "    return dataset\n",
    "\n",
    "# Create the datasets\n",
    "train_dataset = create_dataset(train_df)\n",
    "val_dataset = create_dataset(val_df, is_training=False)\n",
    "\n",
    "# Calculate steps per epoch\n",
    "steps_per_epoch = len(train_df) // batch_size\n",
    "validation_steps = len(val_df) // batch_size\n",
    "\n",
    "# Optimized model definition\n",
    "def create_optimized_hybrid_model(img_height, img_width, num_numeric_features):\n",
    "    # Image processing branch (CNN)\n",
    "    img_input = Input(shape=(img_height, img_width, 3))\n",
    "    \n",
    "    # Use a pre-trained model for feature extraction\n",
    "    base_model = tf.keras.applications.ResNet50V2(include_top=False, weights='imagenet', input_tensor=img_input)\n",
    "    \n",
    "    # Freeze the pre-trained layers\n",
    "    base_model.trainable = False\n",
    "    \n",
    "    x = base_model.output\n",
    "    x = layers.GlobalAveragePooling2D()(x)\n",
    "    x = layers.Dense(256, activation='relu')(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.Dropout(0.5)(x)\n",
    "    \n",
    "    # Numerical data branch (MLP)\n",
    "    num_input = Input(shape=(num_numeric_features,))\n",
    "    y = layers.Dense(128, activation='relu')(num_input)\n",
    "    y = layers.BatchNormalization()(y)\n",
    "    y = layers.Dropout(0.3)(y)\n",
    "    y = layers.Dense(256, activation='relu')(y)\n",
    "    y = layers.BatchNormalization()(y)\n",
    "    y = layers.Dropout(0.4)(y)\n",
    "    y = layers.Dense(128, activation='relu')(y)\n",
    "    y = layers.BatchNormalization()(y)\n",
    "    y = layers.Dropout(0.3)(y)\n",
    "    \n",
    "    # Combining the branches\n",
    "    combined = layers.concatenate([x, y])\n",
    "    \n",
    "    # Final layers\n",
    "    z = layers.Dense(256, activation='relu')(combined)\n",
    "    z = layers.BatchNormalization()(z)\n",
    "    z = layers.Dropout(0.5)(z)\n",
    "    z = layers.Dense(128, activation='relu')(z)\n",
    "    z = layers.BatchNormalization()(z)\n",
    "    z = layers.Dropout(0.5)(z)\n",
    "    \n",
    "    # Output layer\n",
    "    output = layers.Dense(1, activation='sigmoid')(z)\n",
    "    \n",
    "    model = models.Model(inputs=[img_input, num_input], outputs=output)\n",
    "    return model\n",
    "\n",
    "# Learning Rate Logger Callback\n",
    "class LearningRateLogger(tf.keras.callbacks.Callback):\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        if logs is not None:\n",
    "            lr = self.model.optimizer.learning_rate\n",
    "            if isinstance(lr, tf.keras.optimizers.schedules.LearningRateSchedule):\n",
    "                lr = lr(self.model.optimizer.iterations)\n",
    "            logs['lr'] = tf.keras.backend.get_value(lr)\n",
    "\n",
    "# Custom ReduceLROnPlateau Callback\n",
    "class CustomReduceLROnPlateau(tf.keras.callbacks.Callback):\n",
    "    def __init__(self, factor=0.1, patience=10, min_lr=0, monitor='val_loss', mode='min'):\n",
    "        super(CustomReduceLROnPlateau, self).__init__()\n",
    "        self.factor = factor\n",
    "        self.patience = patience\n",
    "        self.min_lr = min_lr\n",
    "        self.monitor = monitor\n",
    "        self.mode = mode\n",
    "        self.best = np.Inf if mode == 'min' else -np.Inf\n",
    "        self.wait = 0\n",
    "        self.best_weights = None\n",
    "\n",
    "    def on_train_begin(self, logs=None):\n",
    "        self.best = np.Inf if self.mode == 'min' else -np.Inf\n",
    "        self.wait = 0\n",
    "        self.best_weights = None\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        logs = logs or {}\n",
    "        current = logs.get(self.monitor)\n",
    "        if current is None:\n",
    "            return\n",
    "        \n",
    "        if self.mode == 'min' and current < self.best:\n",
    "            self.best = current\n",
    "            self.wait = 0\n",
    "            self.best_weights = self.model.get_weights()\n",
    "        elif self.mode == 'max' and current > self.best:\n",
    "            self.best = current\n",
    "            self.wait = 0\n",
    "            self.best_weights = self.model.get_weights()\n",
    "        else:\n",
    "            self.wait += 1\n",
    "        \n",
    "        if self.wait >= self.patience:\n",
    "            current_lr = self.model.optimizer.learning_rate\n",
    "            if isinstance(current_lr, tf.keras.optimizers.schedules.LearningRateSchedule):\n",
    "                current_lr = current_lr(self.model.optimizer.iterations)\n",
    "            current_lr = float(current_lr.numpy())\n",
    "            \n",
    "            if current_lr > self.min_lr:\n",
    "                new_lr = max(current_lr * self.factor, self.min_lr)\n",
    "                new_lr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(\n",
    "                    initial_learning_rate=new_lr,\n",
    "                    decay_steps=1000,\n",
    "                    decay_rate=0.9,\n",
    "                    staircase=True\n",
    "                )\n",
    "                self.model.optimizer.learning_rate = new_lr_schedule\n",
    "                print(f'\\nEpoch {epoch+1}: ReduceLROnPlateau reducing learning rate to {new_lr}.')\n",
    "                self.wait = 0\n",
    "                self.model.set_weights(self.best_weights)\n",
    "\n",
    "# Compile and train\n",
    "initial_learning_rate = 1e-3\n",
    "lr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(\n",
    "    initial_learning_rate, decay_steps=1000, decay_rate=0.9, staircase=True\n",
    ")\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=lr_schedule)\n",
    "\n",
    "# Multi-GPU Strategy\n",
    "strategy = tf.distribute.MirroredStrategy()\n",
    "with strategy.scope():\n",
    "    model = create_optimized_hybrid_model(img_height, img_width, len(numeric_columns))\n",
    "    model.compile(\n",
    "        optimizer=optimizer,\n",
    "        loss='binary_crossentropy',\n",
    "        metrics=['accuracy', tf.keras.metrics.AUC(name='auc'), tf.keras.metrics.Precision(), tf.keras.metrics.Recall()]\n",
    "    )\n",
    "\n",
    "# Callbacks\n",
    "reduce_lr = CustomReduceLROnPlateau(monitor='val_auc', factor=0.2, patience=5, min_lr=1e-6, mode='max')\n",
    "model_checkpoint = tf.keras.callbacks.ModelCheckpoint('best_model.h5', save_best_only=True, monitor='val_auc', mode='max')\n",
    "lr_logger = LearningRateLogger()\n",
    "\n",
    "# Training\n",
    "history = model.fit(\n",
    "    train_dataset,\n",
    "    epochs=100,\n",
    "    steps_per_epoch=steps_per_epoch,\n",
    "    validation_data=val_dataset,\n",
    "    validation_steps=validation_steps,\n",
    "    callbacks=[reduce_lr, model_checkpoint, lr_logger]\n",
    ")\n",
    "\n",
    "# Save model\n",
    "model.save('hybrid_model_optimized_final_new.h5')\n",
    "\n",
    "# Evaluation\n",
    "val_loss, val_accuracy, val_auc, val_precision, val_recall = model.evaluate(val_dataset, steps=validation_steps)\n",
    "print(f\"Validation Loss: {val_loss:.4f}\")\n",
    "print(f\"Validation Accuracy: {val_accuracy:.4f}\")\n",
    "print(f\"Validation AUC: {val_auc:.4f}\")\n",
    "print(f\"Validation Precision: {val_precision:.4f}\")\n",
    "print(f\"Validation Recall: {val_recall:.4f}\")\n",
    "\n",
    "# Predictions for the validation dataset\n",
    "y_pred = model.predict(val_dataset, steps=validation_steps)\n",
    "y_pred_classes = (y_pred > 0.5).astype(\"int32\")\n",
    "y_true = np.concatenate([y for _, y in val_dataset.take(validation_steps)], axis=0)\n",
    "\n",
    "# Confusion matrix and classification report\n",
    "conf_matrix = confusion_matrix(y_true, y_pred_classes)\n",
    "class_report = classification_report(y_true, y_pred_classes)\n",
    "\n",
    "print(\"Confusion Matrix:\")\n",
    "print(conf_matrix)\n",
    "print(\"\\nClassification Report:\")\n",
    "print(class_report)\n",
    "\n",
    "# Visualization of training history\n",
    "plt.figure(figsize=(15, 5))\n",
    "plt.subplot(1, 3, 1)\n",
    "plt.plot(history.history['accuracy'], label='Training Accuracy')\n",
    "plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
    "plt.title('Model Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(1, 3, 2)\n",
    "plt.plot(history.history['loss'], label='Training Loss')\n",
    "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "plt.title('Model Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(1, 3, 3)\n",
    "plt.plot(history.history['auc'], label='Training AUC')\n",
    "plt.plot(history.history['val_auc'], label='Validation AUC')\n",
    "plt.title('Model AUC')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('AUC')\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('training_history.png')\n",
    "plt.close()\n",
    "\n",
    "# Visualization of learning rate\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(history.history['lr'])\n",
    "plt.title('Learning Rate over Epochs')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Learning Rate')\n",
    "plt.yscale('log')\n",
    "plt.savefig('learning_rate.png')\n",
    "plt.close()\n",
    "\n",
    "# Visualization of confusion matrix\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.xlabel('Predicted Label')\n",
    "plt.ylabel('True Label')\n",
    "plt.savefig('confusion_matrix.png')\n",
    "plt.close()\n",
    "\n",
    "print(\"Visualizations have been saved as PNG files.\")\n",
    "\n",
    "# Save predictions and true labels\n",
    "predictions_df = pd.DataFrame({\n",
    "    'True_Label': y_true,\n",
    "    'Predicted_Probability': y_pred.flatten(),\n",
    "    'Predicted_Class': y_pred_classes.flatten()\n",
    "})\n",
    "predictions_df.to_csv('predictions.csv', index=False)\n",
    "print(\"Predictions have been saved in 'predictions.csv'.\")\n",
    "\n",
    "# Model summary\n",
    "model.summary()\n",
    "\n",
    "# Save model architecture as image\n",
    "tf.keras.utils.plot_model(model, to_file='model_architecture.png', show_shapes=True, show_layer_names=True)\n",
    "print(\"Model architecture has been saved as 'model_architecture.png'.\")\n",
    "\n",
    "# Calculate and output additional metrics\n",
    "# ROC AUC\n",
    "roc_auc = roc_auc_score(y_true, y_pred)\n",
    "print(f\"ROC AUC: {roc_auc:.4f}\")\n",
    "\n",
    "# Precision-Recall AUC\n",
    "pr_auc = average_precision_score(y_true, y_pred)\n",
    "print(f\"Precision-Recall AUC: {pr_auc:.4f}\")\n",
    "\n",
    "# F1-Score at various thresholds\n",
    "thresholds = np.arange(0, 1.01, 0.01)\n",
    "f1_scores = [f1_score(y_true, y_pred > threshold) for threshold in thresholds]\n",
    "best_threshold = thresholds[np.argmax(f1_scores)]\n",
    "best_f1 = max(f1_scores)\n",
    "print(f\"Best F1-Score: {best_f1:.4f} at threshold {best_threshold:.2f}\")\n",
    "\n",
    "# Visualization of Precision-Recall curve\n",
    "precision, recall, _ = precision_recall_curve(y_true, y_pred)\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(recall, precision, marker='.')\n",
    "plt.title('Precision-Recall Curve')\n",
    "plt.xlabel('Recall')\n",
    "plt.ylabel('Precision')\n",
    "plt.savefig('precision_recall_curve.png')\n",
    "plt.close()\n",
    "print(\"Precision-Recall curve has been saved as 'precision_recall_curve.png'.\")\n",
    "\n",
    "# Visualization of ROC curve\n",
    "fpr, tpr, _ = roc_curve(y_true, y_pred)\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(fpr, tpr, label=f'ROC Curve (AUC = {roc_auc:.2f})')\n",
    "plt.plot([0, 1], [0, 1], linestyle='--', label='Random Classifier')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver Operating Characteristic (ROC) Curve')\n",
    "plt.legend()\n",
    "plt.savefig('roc_curve.png')\n",
    "plt.close()\n",
    "print(\"ROC curve has been saved as 'roc_curve.png'.\")\n",
    "\n",
    "# Save training history\n",
    "with open('training_history.txt', 'w') as f:\n",
    "    for key, values in history.history.items():\n",
    "        f.write(f\"{key}: {values}\\n\")\n",
    "print(\"Training history has been saved in 'training_history.txt'.\")\n",
    "\n",
    "# Analyze misclassified examples\n",
    "misclassified = np.where(y_pred_classes.flatten() != y_true)[0]\n",
    "print(f\"\\nNumber of misclassified examples: {len(misclassified)}\")\n",
    "\n",
    "if len(misclassified) > 0:\n",
    "    print(\"\\nDetails of some misclassified examples:\")\n",
    "    for i in misclassified[:5]:  # Show details for the first 5 misclassified examples\n",
    "        true_label = y_true[i]\n",
    "        pred_label = y_pred_classes[i][0]\n",
    "        pred_prob = y_pred[i][0]\n",
    "        print(f\"Example {i}: True Label: {true_label}, Predicted Label: {pred_label}, Predicted Probability: {pred_prob:.4f}\")\n",
    "\n",
    "# Model performance across different thresholds\n",
    "accuracies = [accuracy_score(y_true, y_pred > threshold) for threshold in thresholds]\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(thresholds, accuracies, label='Accuracy')\n",
    "plt.plot(thresholds, f1_scores, label='F1-Score')\n",
    "plt.axvline(x=0.5, color='r', linestyle='--', label='Default Threshold (0.5)')\n",
    "plt.xlabel('Threshold')\n",
    "plt.ylabel('Score')\n",
    "plt.title('Model Performance vs. Classification Threshold')\n",
    "plt.legend()\n",
    "plt.savefig('performance_vs_threshold.png')\n",
    "plt.close()\n",
    "print(\"Performance vs. Threshold plot has been saved as 'performance_vs_threshold.png'.\")\n",
    "\n",
    "# Optional: SHAP Analysis\n",
    "try:\n",
    "    import shap\n",
    "    \n",
    "    # Select a subset of validation data for SHAP analysis\n",
    "    num_explain = 100\n",
    "    explain_dataset = val_dataset.take(num_explain).unbatch()\n",
    "    background_dataset = train_dataset.take(num_explain).unbatch()\n",
    "    \n",
    "    # Create an Explainer\n",
    "    explainer = shap.DeepExplainer(model, background_dataset)\n",
    "    \n",
    "    # Calculate SHAP values\n",
    "    shap_values = explainer.shap_values(explain_dataset)\n",
    "    \n",
    "    # Visualize SHAP values\n",
    "    shap.summary_plot(shap_values[0], plot_type=\"bar\", feature_names=numeric_columns, show=False)\n",
    "    plt.savefig('shap_summary.png')\n",
    "    plt.close()\n",
    "    print(\"SHAP summary has been saved as 'shap_summary.png'.\")\n",
    "except ImportError:\n",
    "    print(\"SHAP is not installed. Skipping SHAP analysis.\")\n",
    "\n",
    "print(\"\\nAll analyses and visualizations are complete.\")\n",
    "\n",
    "# Summary of key metrics\n",
    "print(\"\\nSummary of key metrics:\")\n",
    "print(f\"Validation Accuracy: {val_accuracy:.4f}\")\n",
    "print(f\"Validation AUC: {val_auc:.4f}\")\n",
    "print(f\"Validation Precision: {val_precision:.4f}\")\n",
    "print(f\"Validation Recall: {val_recall:.4f}\")\n",
    "print(f\"ROC AUC: {roc_auc:.4f}\")\n",
    "print(f\"Precision-Recall AUC: {pr_auc:.4f}\")\n",
    "print(f\"Best F1-Score: {best_f1:.4f} at threshold {best_threshold:.2f}\")\n",
    "\n",
    "# Save summary to a text file\n",
    "with open('model_summary.txt', 'w') as f:\n",
    "    f.write(\"Summary of key metrics:\\n\")\n",
    "    f.write(f\"Validation Accuracy: {val_accuracy:.4f}\\n\")\n",
    "    f.write(f\"Validation AUC: {val_auc:.4f}\\n\")\n",
    "    f.write(f\"Validation Precision: {val_precision:.4f}\\n\")\n",
    "    f.write(f\"Validation Recall: {val_recall:.4f}\\n\")\n",
    "    f.write(f\"ROC AUC: {roc_auc:.4f}\\n\")\n",
    "    f.write(f\"Precision-Recall AUC: {pr_auc:.4f}\\n\")\n",
    "    f.write(f\"Best F1-Score: {best_f1:.4f} at threshold {best_threshold:.2f}\\n\")\n",
    "\n",
    "print(\"\\nA summary of key metrics has been saved in 'model_summary.txt'.\")\n",
    "print(\"\\nThe script has been executed successfully. All results and visualizations have been saved.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1f2c8c1-6656-492a-bfee-d3e40423540f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ede66ba9-0d18-4a74-9571-44454310aa28",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original CSV data loaded. Number of rows: 19274\n",
      "Original distribution of labels:\n",
      "label_majority\n",
      "2    10850\n",
      "1     3485\n",
      "4     2745\n",
      "3     2194\n",
      "Name: count, dtype: int64\n",
      "1 Physical GPUs, 1 Logical GPUs\n",
      "WARNING:tensorflow:Mixed precision compatibility check (mixed_float16): WARNING\n",
      "Your GPU may run slowly with dtype policy mixed_float16 because it does not have compute capability of at least 7.0. Your GPU:\n",
      "  Quadro P5000, compute capability 6.1\n",
      "See https://developer.nvidia.com/cuda-gpus for a list of GPUs and their compute capabilities.\n",
      "If you will use compatible GPU(s) not attached to this host, e.g. by running a multi-worker model, you can ignore this warning. This message will only be logged once\n",
      "Mixed precision policy set to mixed_float16\n",
      "Found images: Filtered: 5664, Not filtered: 27994\n",
      "Sample of extracted filenames:\n",
      "0       00042065\n",
      "1    00042065Alt\n",
      "2       00043749\n",
      "3    00043749Alt\n",
      "4       00080967\n",
      "Name: filename, dtype: object\n",
      "After undersampling: Number of rows: 4499\n",
      "Distribution of labels after undersampling:\n",
      "label\n",
      "1    2499\n",
      "0    2000\n",
      "Name: count, dtype: int64\n",
      "Training data: 3599, Validation data: 900\n",
      "INFO:tensorflow:Using MirroredStrategy with devices ('/job:localhost/replica:0/task:0/device:GPU:0',)\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "Epoch 1/100\n",
      "112/112 [==============================] - 33s 130ms/step - loss: 0.6487 - accuracy: 0.7246 - auc: 0.7877 - precision: 0.7556 - recall: 0.7461 - val_loss: 0.3985 - val_accuracy: 0.8270 - val_auc: 0.9012 - val_precision: 0.8101 - val_recall: 0.8996 - lr: 0.0010\n",
      "Epoch 2/100\n",
      "112/112 [==============================] - 12s 110ms/step - loss: 0.4695 - accuracy: 0.7905 - auc: 0.8731 - precision: 0.8082 - recall: 0.8159 - val_loss: 0.3641 - val_accuracy: 0.8371 - val_auc: 0.9199 - val_precision: 0.8200 - val_recall: 0.9056 - lr: 0.0010\n",
      "Epoch 3/100\n",
      "112/112 [==============================] - 11s 102ms/step - loss: 0.4147 - accuracy: 0.8181 - auc: 0.8962 - precision: 0.8225 - recall: 0.8587 - val_loss: 0.3730 - val_accuracy: 0.8415 - val_auc: 0.9045 - val_precision: 0.8309 - val_recall: 0.8976 - lr: 0.0010\n",
      "Epoch 4/100\n",
      "112/112 [==============================] - 12s 110ms/step - loss: 0.3894 - accuracy: 0.8287 - auc: 0.9061 - precision: 0.8330 - recall: 0.8633 - val_loss: 0.3496 - val_accuracy: 0.8560 - val_auc: 0.9210 - val_precision: 0.8596 - val_recall: 0.8855 - lr: 0.0010\n",
      "Epoch 5/100\n",
      "112/112 [==============================] - 11s 102ms/step - loss: 0.3483 - accuracy: 0.8502 - auc: 0.9226 - precision: 0.8524 - recall: 0.8837 - val_loss: 0.3503 - val_accuracy: 0.8549 - val_auc: 0.9196 - val_precision: 0.8695 - val_recall: 0.8695 - lr: 0.0010\n",
      "Epoch 6/100\n",
      "112/112 [==============================] - 12s 110ms/step - loss: 0.3379 - accuracy: 0.8552 - auc: 0.9267 - precision: 0.8534 - recall: 0.8917 - val_loss: 0.3259 - val_accuracy: 0.8683 - val_auc: 0.9307 - val_precision: 0.8571 - val_recall: 0.9157 - lr: 0.0010\n",
      "Epoch 7/100\n",
      "112/112 [==============================] - 12s 109ms/step - loss: 0.3217 - accuracy: 0.8697 - auc: 0.9333 - precision: 0.8700 - recall: 0.9001 - val_loss: 0.3178 - val_accuracy: 0.8661 - val_auc: 0.9360 - val_precision: 0.8663 - val_recall: 0.8976 - lr: 0.0010\n",
      "Epoch 8/100\n",
      "112/112 [==============================] - 11s 101ms/step - loss: 0.3059 - accuracy: 0.8677 - auc: 0.9412 - precision: 0.8717 - recall: 0.8928 - val_loss: 0.3452 - val_accuracy: 0.8460 - val_auc: 0.9256 - val_precision: 0.8719 - val_recall: 0.8474 - lr: 0.0010\n",
      "Epoch 9/100\n",
      "112/112 [==============================] - 11s 101ms/step - loss: 0.2986 - accuracy: 0.8756 - auc: 0.9425 - precision: 0.8683 - recall: 0.9160 - val_loss: 0.3438 - val_accuracy: 0.8248 - val_auc: 0.9310 - val_precision: 0.8797 - val_recall: 0.7932 - lr: 9.0000e-04\n",
      "Epoch 10/100\n",
      "112/112 [==============================] - 11s 101ms/step - loss: 0.2945 - accuracy: 0.8736 - auc: 0.9450 - precision: 0.8763 - recall: 0.8980 - val_loss: 0.3203 - val_accuracy: 0.8638 - val_auc: 0.9343 - val_precision: 0.8715 - val_recall: 0.8855 - lr: 9.0000e-04\n",
      "Epoch 11/100\n",
      "112/112 [==============================] - 12s 109ms/step - loss: 0.2883 - accuracy: 0.8783 - auc: 0.9463 - precision: 0.8782 - recall: 0.9088 - val_loss: 0.3149 - val_accuracy: 0.8627 - val_auc: 0.9387 - val_precision: 0.8758 - val_recall: 0.8775 - lr: 9.0000e-04\n",
      "Epoch 12/100\n",
      "112/112 [==============================] - 12s 109ms/step - loss: 0.2721 - accuracy: 0.8912 - auc: 0.9519 - precision: 0.8889 - recall: 0.9185 - val_loss: 0.3087 - val_accuracy: 0.8650 - val_auc: 0.9425 - val_precision: 0.8793 - val_recall: 0.8775 - lr: 9.0000e-04\n",
      "Epoch 13/100\n",
      "112/112 [==============================] - 11s 101ms/step - loss: 0.2616 - accuracy: 0.8929 - auc: 0.9558 - precision: 0.8865 - recall: 0.9249 - val_loss: 0.3098 - val_accuracy: 0.8627 - val_auc: 0.9406 - val_precision: 0.8378 - val_recall: 0.9337 - lr: 9.0000e-04\n",
      "Epoch 14/100\n",
      "112/112 [==============================] - 11s 101ms/step - loss: 0.2622 - accuracy: 0.8898 - auc: 0.9558 - precision: 0.8874 - recall: 0.9173 - val_loss: 0.3074 - val_accuracy: 0.8650 - val_auc: 0.9409 - val_precision: 0.8733 - val_recall: 0.8855 - lr: 9.0000e-04\n",
      "Epoch 15/100\n",
      "112/112 [==============================] - 12s 109ms/step - loss: 0.2611 - accuracy: 0.8937 - auc: 0.9550 - precision: 0.8904 - recall: 0.9237 - val_loss: 0.2995 - val_accuracy: 0.8728 - val_auc: 0.9462 - val_precision: 0.8466 - val_recall: 0.9418 - lr: 9.0000e-04\n",
      "Epoch 16/100\n",
      "112/112 [==============================] - 11s 101ms/step - loss: 0.2467 - accuracy: 0.8990 - auc: 0.9607 - precision: 0.8949 - recall: 0.9249 - val_loss: 0.2941 - val_accuracy: 0.8839 - val_auc: 0.9445 - val_precision: 0.8833 - val_recall: 0.9116 - lr: 9.0000e-04\n",
      "Epoch 17/100\n",
      "112/112 [==============================] - 12s 109ms/step - loss: 0.2623 - accuracy: 0.8895 - auc: 0.9558 - precision: 0.8908 - recall: 0.9148 - val_loss: 0.2872 - val_accuracy: 0.8806 - val_auc: 0.9473 - val_precision: 0.8826 - val_recall: 0.9056 - lr: 9.0000e-04\n",
      "Epoch 18/100\n",
      "112/112 [==============================] - 12s 108ms/step - loss: 0.2412 - accuracy: 0.9004 - auc: 0.9623 - precision: 0.8968 - recall: 0.9270 - val_loss: 0.2784 - val_accuracy: 0.8873 - val_auc: 0.9512 - val_precision: 0.8738 - val_recall: 0.9317 - lr: 8.1000e-04\n",
      "Epoch 19/100\n",
      "112/112 [==============================] - 11s 100ms/step - loss: 0.2385 - accuracy: 0.9065 - auc: 0.9634 - precision: 0.9090 - recall: 0.9240 - val_loss: 0.2834 - val_accuracy: 0.8828 - val_auc: 0.9510 - val_precision: 0.8816 - val_recall: 0.9116 - lr: 8.1000e-04\n",
      "Epoch 20/100\n",
      "112/112 [==============================] - 11s 100ms/step - loss: 0.2368 - accuracy: 0.9065 - auc: 0.9632 - precision: 0.9034 - recall: 0.9311 - val_loss: 0.2812 - val_accuracy: 0.8839 - val_auc: 0.9507 - val_precision: 0.8788 - val_recall: 0.9177 - lr: 8.1000e-04\n",
      "Epoch 21/100\n",
      "112/112 [==============================] - 11s 100ms/step - loss: 0.2405 - accuracy: 0.9035 - auc: 0.9625 - precision: 0.9023 - recall: 0.9268 - val_loss: 0.2973 - val_accuracy: 0.8750 - val_auc: 0.9460 - val_precision: 0.8814 - val_recall: 0.8956 - lr: 8.1000e-04\n",
      "Epoch 22/100\n",
      "112/112 [==============================] - 11s 100ms/step - loss: 0.2266 - accuracy: 0.9085 - auc: 0.9670 - precision: 0.9069 - recall: 0.9330 - val_loss: 0.3040 - val_accuracy: 0.8616 - val_auc: 0.9438 - val_precision: 0.8740 - val_recall: 0.8775 - lr: 8.1000e-04\n",
      "Epoch 23/100\n",
      "112/112 [==============================] - ETA: 0s - loss: 0.2243 - accuracy: 0.9110 - auc: 0.9672 - precision: 0.9125 - recall: 0.9277\n",
      "Epoch 23: ReduceLROnPlateau reducing learning rate to 0.0001619999995455146.\n",
      "112/112 [==============================] - 11s 102ms/step - loss: 0.2243 - accuracy: 0.9110 - auc: 0.9672 - precision: 0.9125 - recall: 0.9277 - val_loss: 0.3127 - val_accuracy: 0.8672 - val_auc: 0.9440 - val_precision: 0.8366 - val_recall: 0.9458 - lr: 1.3122e-04\n",
      "Epoch 24/100\n",
      "112/112 [==============================] - 11s 100ms/step - loss: 0.2306 - accuracy: 0.9074 - auc: 0.9658 - precision: 0.9049 - recall: 0.9296 - val_loss: 0.2879 - val_accuracy: 0.8828 - val_auc: 0.9483 - val_precision: 0.8772 - val_recall: 0.9177 - lr: 1.3122e-04\n",
      "Epoch 25/100\n",
      "112/112 [==============================] - 11s 99ms/step - loss: 0.2340 - accuracy: 0.9035 - auc: 0.9639 - precision: 0.9027 - recall: 0.9278 - val_loss: 0.2855 - val_accuracy: 0.8806 - val_auc: 0.9500 - val_precision: 0.8668 - val_recall: 0.9277 - lr: 1.3122e-04\n",
      "Epoch 26/100\n",
      "112/112 [==============================] - 12s 107ms/step - loss: 0.2444 - accuracy: 0.9029 - auc: 0.9608 - precision: 0.8992 - recall: 0.9275 - val_loss: 0.2799 - val_accuracy: 0.8839 - val_auc: 0.9517 - val_precision: 0.8848 - val_recall: 0.9096 - lr: 1.3122e-04\n",
      "Epoch 27/100\n",
      "112/112 [==============================] - 11s 99ms/step - loss: 0.2334 - accuracy: 0.9093 - auc: 0.9650 - precision: 0.9095 - recall: 0.9291 - val_loss: 0.2839 - val_accuracy: 0.8862 - val_auc: 0.9509 - val_precision: 0.8708 - val_recall: 0.9337 - lr: 1.1810e-04\n",
      "Epoch 28/100\n",
      "112/112 [==============================] - 12s 108ms/step - loss: 0.2181 - accuracy: 0.9124 - auc: 0.9679 - precision: 0.9113 - recall: 0.9349 - val_loss: 0.2755 - val_accuracy: 0.8895 - val_auc: 0.9527 - val_precision: 0.8844 - val_recall: 0.9217 - lr: 1.1810e-04\n",
      "Epoch 29/100\n",
      "112/112 [==============================] - 11s 100ms/step - loss: 0.2393 - accuracy: 0.9021 - auc: 0.9628 - precision: 0.8977 - recall: 0.9281 - val_loss: 0.2913 - val_accuracy: 0.8817 - val_auc: 0.9494 - val_precision: 0.8828 - val_recall: 0.9076 - lr: 1.1810e-04\n",
      "Epoch 30/100\n",
      "112/112 [==============================] - 12s 108ms/step - loss: 0.2068 - accuracy: 0.9146 - auc: 0.9717 - precision: 0.9061 - recall: 0.9443 - val_loss: 0.2676 - val_accuracy: 0.8862 - val_auc: 0.9562 - val_precision: 0.8944 - val_recall: 0.9016 - lr: 1.1810e-04\n",
      "Epoch 31/100\n",
      "112/112 [==============================] - 11s 100ms/step - loss: 0.2110 - accuracy: 0.9093 - auc: 0.9719 - precision: 0.9165 - recall: 0.9215 - val_loss: 0.2844 - val_accuracy: 0.8850 - val_auc: 0.9498 - val_precision: 0.8776 - val_recall: 0.9217 - lr: 1.1810e-04\n",
      "Epoch 32/100\n",
      "112/112 [==============================] - 12s 108ms/step - loss: 0.2125 - accuracy: 0.9141 - auc: 0.9700 - precision: 0.9067 - recall: 0.9407 - val_loss: 0.2709 - val_accuracy: 0.8917 - val_auc: 0.9568 - val_precision: 0.8748 - val_recall: 0.9398 - lr: 1.1810e-04\n",
      "Epoch 33/100\n",
      "112/112 [==============================] - 12s 108ms/step - loss: 0.2060 - accuracy: 0.9230 - auc: 0.9716 - precision: 0.9232 - recall: 0.9415 - val_loss: 0.2657 - val_accuracy: 0.8895 - val_auc: 0.9573 - val_precision: 0.8919 - val_recall: 0.9116 - lr: 1.1810e-04\n",
      "Epoch 34/100\n",
      "112/112 [==============================] - 11s 100ms/step - loss: 0.2080 - accuracy: 0.9104 - auc: 0.9728 - precision: 0.9079 - recall: 0.9314 - val_loss: 0.2869 - val_accuracy: 0.8761 - val_auc: 0.9530 - val_precision: 0.8564 - val_recall: 0.9337 - lr: 1.1810e-04\n",
      "Epoch 35/100\n",
      "112/112 [==============================] - 11s 100ms/step - loss: 0.2073 - accuracy: 0.9202 - auc: 0.9710 - precision: 0.9202 - recall: 0.9394 - val_loss: 0.2765 - val_accuracy: 0.8795 - val_auc: 0.9544 - val_precision: 0.8963 - val_recall: 0.8855 - lr: 1.1810e-04\n",
      "Epoch 36/100\n",
      "112/112 [==============================] - 11s 100ms/step - loss: 0.1948 - accuracy: 0.9222 - auc: 0.9754 - precision: 0.9241 - recall: 0.9367 - val_loss: 0.2728 - val_accuracy: 0.8906 - val_auc: 0.9538 - val_precision: 0.8774 - val_recall: 0.9337 - lr: 1.0629e-04\n",
      "Epoch 37/100\n",
      "112/112 [==============================] - 11s 100ms/step - loss: 0.1936 - accuracy: 0.9241 - auc: 0.9757 - precision: 0.9221 - recall: 0.9425 - val_loss: 0.2949 - val_accuracy: 0.8817 - val_auc: 0.9491 - val_precision: 0.8784 - val_recall: 0.9137 - lr: 1.0629e-04\n",
      "Epoch 38/100\n",
      "112/112 [==============================] - ETA: 0s - loss: 0.1807 - accuracy: 0.9294 - auc: 0.9775 - precision: 0.9263 - recall: 0.9482\n",
      "Epoch 38: ReduceLROnPlateau reducing learning rate to 2.125763421645388e-05.\n",
      "112/112 [==============================] - 11s 102ms/step - loss: 0.1807 - accuracy: 0.9294 - auc: 0.9775 - precision: 0.9263 - recall: 0.9482 - val_loss: 0.2829 - val_accuracy: 0.8839 - val_auc: 0.9542 - val_precision: 0.8788 - val_recall: 0.9177 - lr: 1.3947e-05\n",
      "Epoch 39/100\n",
      "112/112 [==============================] - 11s 100ms/step - loss: 0.2127 - accuracy: 0.9149 - auc: 0.9707 - precision: 0.9148 - recall: 0.9320 - val_loss: 0.2831 - val_accuracy: 0.8795 - val_auc: 0.9534 - val_precision: 0.8585 - val_recall: 0.9378 - lr: 1.3947e-05\n",
      "Epoch 40/100\n",
      "112/112 [==============================] - 11s 99ms/step - loss: 0.1939 - accuracy: 0.9252 - auc: 0.9749 - precision: 0.9204 - recall: 0.9480 - val_loss: 0.2791 - val_accuracy: 0.8839 - val_auc: 0.9549 - val_precision: 0.8689 - val_recall: 0.9317 - lr: 1.3947e-05\n",
      "Epoch 41/100\n",
      "112/112 [==============================] - 11s 100ms/step - loss: 0.1901 - accuracy: 0.9244 - auc: 0.9761 - precision: 0.9209 - recall: 0.9454 - val_loss: 0.3081 - val_accuracy: 0.8739 - val_auc: 0.9491 - val_precision: 0.9053 - val_recall: 0.8635 - lr: 1.3947e-05\n",
      "Epoch 42/100\n",
      "112/112 [==============================] - 11s 100ms/step - loss: 0.1981 - accuracy: 0.9227 - auc: 0.9739 - precision: 0.9241 - recall: 0.9376 - val_loss: 0.2800 - val_accuracy: 0.8817 - val_auc: 0.9542 - val_precision: 0.8657 - val_recall: 0.9317 - lr: 1.3947e-05\n",
      "Epoch 43/100\n",
      "112/112 [==============================] - ETA: 0s - loss: 0.1953 - accuracy: 0.9205 - auc: 0.9755 - precision: 0.9168 - recall: 0.9414\n",
      "Epoch 43: ReduceLROnPlateau reducing learning rate to 2.7894260711036626e-06.\n",
      "112/112 [==============================] - 11s 102ms/step - loss: 0.1953 - accuracy: 0.9205 - auc: 0.9755 - precision: 0.9168 - recall: 0.9414 - val_loss: 0.2784 - val_accuracy: 0.8783 - val_auc: 0.9551 - val_precision: 0.8691 - val_recall: 0.9197 - lr: 1.8301e-06\n",
      "Epoch 44/100\n",
      "112/112 [==============================] - 11s 99ms/step - loss: 0.1963 - accuracy: 0.9227 - auc: 0.9746 - precision: 0.9203 - recall: 0.9445 - val_loss: 0.2780 - val_accuracy: 0.8817 - val_auc: 0.9528 - val_precision: 0.8798 - val_recall: 0.9116 - lr: 1.8301e-06\n",
      "Epoch 45/100\n",
      "112/112 [==============================] - 11s 100ms/step - loss: 0.1931 - accuracy: 0.9272 - auc: 0.9748 - precision: 0.9196 - recall: 0.9501 - val_loss: 0.2722 - val_accuracy: 0.8973 - val_auc: 0.9557 - val_precision: 0.9044 - val_recall: 0.9116 - lr: 1.6471e-06\n",
      "Epoch 46/100\n",
      "112/112 [==============================] - 11s 100ms/step - loss: 0.1829 - accuracy: 0.9302 - auc: 0.9782 - precision: 0.9315 - recall: 0.9454 - val_loss: 0.2965 - val_accuracy: 0.8873 - val_auc: 0.9496 - val_precision: 0.8825 - val_recall: 0.9197 - lr: 1.6471e-06\n",
      "Epoch 47/100\n",
      "112/112 [==============================] - 11s 100ms/step - loss: 0.1882 - accuracy: 0.9300 - auc: 0.9770 - precision: 0.9305 - recall: 0.9441 - val_loss: 0.2966 - val_accuracy: 0.8906 - val_auc: 0.9492 - val_precision: 0.8984 - val_recall: 0.9056 - lr: 1.6471e-06\n",
      "Epoch 48/100\n",
      "112/112 [==============================] - ETA: 0s - loss: 0.1891 - accuracy: 0.9255 - auc: 0.9769 - precision: 0.9256 - recall: 0.9410\n",
      "Epoch 48: ReduceLROnPlateau reducing learning rate to 1e-06.\n",
      "112/112 [==============================] - 11s 102ms/step - loss: 0.1891 - accuracy: 0.9255 - auc: 0.9769 - precision: 0.9256 - recall: 0.9410 - val_loss: 0.2879 - val_accuracy: 0.8817 - val_auc: 0.9512 - val_precision: 0.8769 - val_recall: 0.9157 - lr: 5.9049e-07\n",
      "Epoch 49/100\n",
      "112/112 [==============================] - 11s 100ms/step - loss: 0.1971 - accuracy: 0.9205 - auc: 0.9745 - precision: 0.9171 - recall: 0.9424 - val_loss: 0.2910 - val_accuracy: 0.8795 - val_auc: 0.9490 - val_precision: 0.8869 - val_recall: 0.8976 - lr: 5.9049e-07\n",
      "Epoch 50/100\n",
      "112/112 [==============================] - 11s 100ms/step - loss: 0.1982 - accuracy: 0.9258 - auc: 0.9741 - precision: 0.9269 - recall: 0.9416 - val_loss: 0.2764 - val_accuracy: 0.8839 - val_auc: 0.9533 - val_precision: 0.8803 - val_recall: 0.9157 - lr: 5.9049e-07\n",
      "Epoch 51/100\n",
      "112/112 [==============================] - 11s 100ms/step - loss: 0.2032 - accuracy: 0.9219 - auc: 0.9724 - precision: 0.9153 - recall: 0.9466 - val_loss: 0.2860 - val_accuracy: 0.8828 - val_auc: 0.9522 - val_precision: 0.8786 - val_recall: 0.9157 - lr: 5.9049e-07\n",
      "Epoch 52/100\n",
      "112/112 [==============================] - 11s 100ms/step - loss: 0.1837 - accuracy: 0.9277 - auc: 0.9778 - precision: 0.9233 - recall: 0.9464 - val_loss: 0.3009 - val_accuracy: 0.8795 - val_auc: 0.9471 - val_precision: 0.8611 - val_recall: 0.9337 - lr: 5.9049e-07\n",
      "Epoch 53/100\n",
      "112/112 [==============================] - 11s 100ms/step - loss: 0.1809 - accuracy: 0.9266 - auc: 0.9783 - precision: 0.9238 - recall: 0.9464 - val_loss: 0.2942 - val_accuracy: 0.8806 - val_auc: 0.9532 - val_precision: 0.8587 - val_recall: 0.9398 - lr: 5.9049e-07\n",
      "Epoch 54/100\n",
      "112/112 [==============================] - 11s 100ms/step - loss: 0.1807 - accuracy: 0.9319 - auc: 0.9782 - precision: 0.9308 - recall: 0.9476 - val_loss: 0.2922 - val_accuracy: 0.8850 - val_auc: 0.9524 - val_precision: 0.8911 - val_recall: 0.9036 - lr: 5.3144e-07\n",
      "Epoch 55/100\n",
      "112/112 [==============================] - 11s 100ms/step - loss: 0.1794 - accuracy: 0.9297 - auc: 0.9786 - precision: 0.9319 - recall: 0.9447 - val_loss: 0.3108 - val_accuracy: 0.8817 - val_auc: 0.9476 - val_precision: 0.8643 - val_recall: 0.9337 - lr: 5.3144e-07\n",
      "Epoch 56/100\n",
      "112/112 [==============================] - 11s 100ms/step - loss: 0.1852 - accuracy: 0.9222 - auc: 0.9773 - precision: 0.9161 - recall: 0.9453 - val_loss: 0.2891 - val_accuracy: 0.8862 - val_auc: 0.9498 - val_precision: 0.8852 - val_recall: 0.9137 - lr: 5.3144e-07\n",
      "Epoch 57/100\n",
      "112/112 [==============================] - 11s 100ms/step - loss: 0.1838 - accuracy: 0.9272 - auc: 0.9781 - precision: 0.9297 - recall: 0.9413 - val_loss: 0.2971 - val_accuracy: 0.8783 - val_auc: 0.9516 - val_precision: 0.8636 - val_recall: 0.9277 - lr: 5.3144e-07\n",
      "Epoch 58/100\n",
      "112/112 [==============================] - 11s 100ms/step - loss: 0.1702 - accuracy: 0.9350 - auc: 0.9805 - precision: 0.9274 - recall: 0.9554 - val_loss: 0.2954 - val_accuracy: 0.8884 - val_auc: 0.9544 - val_precision: 0.8798 - val_recall: 0.9257 - lr: 5.3144e-07\n",
      "Epoch 59/100\n",
      "112/112 [==============================] - 11s 100ms/step - loss: 0.1643 - accuracy: 0.9375 - auc: 0.9823 - precision: 0.9349 - recall: 0.9553 - val_loss: 0.2877 - val_accuracy: 0.8828 - val_auc: 0.9539 - val_precision: 0.8743 - val_recall: 0.9217 - lr: 5.3144e-07\n",
      "Epoch 60/100\n",
      "112/112 [==============================] - 11s 100ms/step - loss: 0.1678 - accuracy: 0.9358 - auc: 0.9816 - precision: 0.9346 - recall: 0.9506 - val_loss: 0.3043 - val_accuracy: 0.8839 - val_auc: 0.9490 - val_precision: 0.8818 - val_recall: 0.9137 - lr: 5.3144e-07\n",
      "Epoch 61/100\n",
      "112/112 [==============================] - 11s 100ms/step - loss: 0.1725 - accuracy: 0.9314 - auc: 0.9808 - precision: 0.9304 - recall: 0.9473 - val_loss: 0.2988 - val_accuracy: 0.8862 - val_auc: 0.9508 - val_precision: 0.8822 - val_recall: 0.9177 - lr: 5.3144e-07\n",
      "Epoch 62/100\n",
      "112/112 [==============================] - 11s 100ms/step - loss: 0.1734 - accuracy: 0.9372 - auc: 0.9797 - precision: 0.9363 - recall: 0.9518 - val_loss: 0.2870 - val_accuracy: 0.8951 - val_auc: 0.9518 - val_precision: 0.9024 - val_recall: 0.9096 - lr: 5.3144e-07\n",
      "Epoch 63/100\n",
      "112/112 [==============================] - 11s 101ms/step - loss: 0.1665 - accuracy: 0.9369 - auc: 0.9801 - precision: 0.9300 - recall: 0.9582 - val_loss: 0.3050 - val_accuracy: 0.8828 - val_auc: 0.9485 - val_precision: 0.8876 - val_recall: 0.9036 - lr: 4.7830e-07\n",
      "Epoch 64/100\n",
      "112/112 [==============================] - 11s 101ms/step - loss: 0.1588 - accuracy: 0.9411 - auc: 0.9831 - precision: 0.9415 - recall: 0.9533 - val_loss: 0.2895 - val_accuracy: 0.8917 - val_auc: 0.9518 - val_precision: 0.8908 - val_recall: 0.9177 - lr: 4.7830e-07\n",
      "Epoch 65/100\n",
      "112/112 [==============================] - 11s 101ms/step - loss: 0.1716 - accuracy: 0.9347 - auc: 0.9801 - precision: 0.9332 - recall: 0.9501 - val_loss: 0.3010 - val_accuracy: 0.8850 - val_auc: 0.9484 - val_precision: 0.8958 - val_recall: 0.8976 - lr: 4.7830e-07\n",
      "Epoch 66/100\n",
      "112/112 [==============================] - 11s 101ms/step - loss: 0.1597 - accuracy: 0.9381 - auc: 0.9819 - precision: 0.9385 - recall: 0.9525 - val_loss: 0.2959 - val_accuracy: 0.8739 - val_auc: 0.9532 - val_precision: 0.8545 - val_recall: 0.9317 - lr: 4.7830e-07\n",
      "Epoch 67/100\n",
      "112/112 [==============================] - 11s 101ms/step - loss: 0.1461 - accuracy: 0.9408 - auc: 0.9867 - precision: 0.9355 - recall: 0.9575 - val_loss: 0.3063 - val_accuracy: 0.8828 - val_auc: 0.9508 - val_precision: 0.8816 - val_recall: 0.9116 - lr: 4.7830e-07\n",
      "Epoch 68/100\n",
      "112/112 [==============================] - 11s 101ms/step - loss: 0.1582 - accuracy: 0.9422 - auc: 0.9829 - precision: 0.9467 - recall: 0.9500 - val_loss: 0.3047 - val_accuracy: 0.8761 - val_auc: 0.9538 - val_precision: 0.8550 - val_recall: 0.9357 - lr: 4.7830e-07\n",
      "Epoch 69/100\n",
      "112/112 [==============================] - 11s 101ms/step - loss: 0.1489 - accuracy: 0.9425 - auc: 0.9852 - precision: 0.9392 - recall: 0.9590 - val_loss: 0.3172 - val_accuracy: 0.8772 - val_auc: 0.9501 - val_precision: 0.8619 - val_recall: 0.9277 - lr: 4.7830e-07\n",
      "Epoch 70/100\n",
      "112/112 [==============================] - 11s 101ms/step - loss: 0.1571 - accuracy: 0.9381 - auc: 0.9836 - precision: 0.9342 - recall: 0.9558 - val_loss: 0.2895 - val_accuracy: 0.8884 - val_auc: 0.9550 - val_precision: 0.8827 - val_recall: 0.9217 - lr: 4.7830e-07\n",
      "Epoch 71/100\n",
      "112/112 [==============================] - 11s 100ms/step - loss: 0.1436 - accuracy: 0.9434 - auc: 0.9863 - precision: 0.9473 - recall: 0.9516 - val_loss: 0.3101 - val_accuracy: 0.8873 - val_auc: 0.9510 - val_precision: 0.8738 - val_recall: 0.9317 - lr: 4.7830e-07\n",
      "Epoch 72/100\n",
      "112/112 [==============================] - 11s 101ms/step - loss: 0.1465 - accuracy: 0.9442 - auc: 0.9849 - precision: 0.9348 - recall: 0.9661 - val_loss: 0.3076 - val_accuracy: 0.8873 - val_auc: 0.9513 - val_precision: 0.8869 - val_recall: 0.9137 - lr: 4.3047e-07\n",
      "Epoch 73/100\n",
      "112/112 [==============================] - 11s 101ms/step - loss: 0.1463 - accuracy: 0.9467 - auc: 0.9851 - precision: 0.9478 - recall: 0.9568 - val_loss: 0.3182 - val_accuracy: 0.8828 - val_auc: 0.9511 - val_precision: 0.8786 - val_recall: 0.9157 - lr: 4.3047e-07\n",
      "Epoch 74/100\n",
      "112/112 [==============================] - 11s 101ms/step - loss: 0.1504 - accuracy: 0.9445 - auc: 0.9848 - precision: 0.9463 - recall: 0.9552 - val_loss: 0.3327 - val_accuracy: 0.8806 - val_auc: 0.9453 - val_precision: 0.8738 - val_recall: 0.9177 - lr: 4.3047e-07\n",
      "Epoch 75/100\n",
      "112/112 [==============================] - 11s 100ms/step - loss: 0.1398 - accuracy: 0.9467 - auc: 0.9861 - precision: 0.9364 - recall: 0.9684 - val_loss: 0.3432 - val_accuracy: 0.8728 - val_auc: 0.9459 - val_precision: 0.8794 - val_recall: 0.8936 - lr: 4.3047e-07\n",
      "Epoch 76/100\n",
      "112/112 [==============================] - 11s 100ms/step - loss: 0.1579 - accuracy: 0.9434 - auc: 0.9825 - precision: 0.9360 - recall: 0.9646 - val_loss: 0.3271 - val_accuracy: 0.8694 - val_auc: 0.9488 - val_precision: 0.8629 - val_recall: 0.9096 - lr: 4.3047e-07\n",
      "Epoch 77/100\n",
      "112/112 [==============================] - 11s 100ms/step - loss: 0.1491 - accuracy: 0.9473 - auc: 0.9843 - precision: 0.9455 - recall: 0.9603 - val_loss: 0.3364 - val_accuracy: 0.8683 - val_auc: 0.9454 - val_precision: 0.8831 - val_recall: 0.8795 - lr: 4.3047e-07\n",
      "Epoch 78/100\n",
      "112/112 [==============================] - 11s 100ms/step - loss: 0.1474 - accuracy: 0.9461 - auc: 0.9850 - precision: 0.9445 - recall: 0.9601 - val_loss: 0.3202 - val_accuracy: 0.8728 - val_auc: 0.9497 - val_precision: 0.8664 - val_recall: 0.9116 - lr: 4.3047e-07\n",
      "Epoch 79/100\n",
      "112/112 [==============================] - 11s 100ms/step - loss: 0.1385 - accuracy: 0.9481 - auc: 0.9872 - precision: 0.9502 - recall: 0.9568 - val_loss: 0.3379 - val_accuracy: 0.8761 - val_auc: 0.9434 - val_precision: 0.8817 - val_recall: 0.8976 - lr: 4.3047e-07\n",
      "Epoch 80/100\n",
      "112/112 [==============================] - 11s 100ms/step - loss: 0.1596 - accuracy: 0.9386 - auc: 0.9830 - precision: 0.9346 - recall: 0.9554 - val_loss: 0.3138 - val_accuracy: 0.8850 - val_auc: 0.9508 - val_precision: 0.8776 - val_recall: 0.9217 - lr: 4.3047e-07\n",
      "Epoch 81/100\n",
      "112/112 [==============================] - 11s 100ms/step - loss: 0.1214 - accuracy: 0.9540 - auc: 0.9900 - precision: 0.9496 - recall: 0.9683 - val_loss: 0.3216 - val_accuracy: 0.8717 - val_auc: 0.9504 - val_precision: 0.8593 - val_recall: 0.9197 - lr: 3.8742e-07\n",
      "Epoch 82/100\n",
      "112/112 [==============================] - 11s 101ms/step - loss: 0.1369 - accuracy: 0.9484 - auc: 0.9871 - precision: 0.9461 - recall: 0.9626 - val_loss: 0.3347 - val_accuracy: 0.8783 - val_auc: 0.9459 - val_precision: 0.8929 - val_recall: 0.8876 - lr: 3.8742e-07\n",
      "Epoch 83/100\n",
      "112/112 [==============================] - 11s 101ms/step - loss: 0.1324 - accuracy: 0.9520 - auc: 0.9876 - precision: 0.9505 - recall: 0.9639 - val_loss: 0.3348 - val_accuracy: 0.8672 - val_auc: 0.9461 - val_precision: 0.8694 - val_recall: 0.8956 - lr: 3.8742e-07\n",
      "Epoch 84/100\n",
      "112/112 [==============================] - 11s 100ms/step - loss: 0.1363 - accuracy: 0.9492 - auc: 0.9871 - precision: 0.9463 - recall: 0.9617 - val_loss: 0.3250 - val_accuracy: 0.8862 - val_auc: 0.9495 - val_precision: 0.8867 - val_recall: 0.9116 - lr: 3.8742e-07\n",
      "Epoch 85/100\n",
      "112/112 [==============================] - 11s 100ms/step - loss: 0.1412 - accuracy: 0.9467 - auc: 0.9865 - precision: 0.9458 - recall: 0.9603 - val_loss: 0.3302 - val_accuracy: 0.8750 - val_auc: 0.9472 - val_precision: 0.8876 - val_recall: 0.8876 - lr: 3.8742e-07\n",
      "Epoch 86/100\n",
      "112/112 [==============================] - 11s 100ms/step - loss: 0.1368 - accuracy: 0.9506 - auc: 0.9873 - precision: 0.9483 - recall: 0.9631 - val_loss: 0.3348 - val_accuracy: 0.8806 - val_auc: 0.9458 - val_precision: 0.8811 - val_recall: 0.9076 - lr: 3.8742e-07\n",
      "Epoch 87/100\n",
      "112/112 [==============================] - 11s 100ms/step - loss: 0.1445 - accuracy: 0.9445 - auc: 0.9855 - precision: 0.9430 - recall: 0.9585 - val_loss: 0.3345 - val_accuracy: 0.8839 - val_auc: 0.9479 - val_precision: 0.8878 - val_recall: 0.9056 - lr: 3.8742e-07\n",
      "Epoch 88/100\n",
      "112/112 [==============================] - 11s 100ms/step - loss: 0.1196 - accuracy: 0.9545 - auc: 0.9905 - precision: 0.9475 - recall: 0.9711 - val_loss: 0.3270 - val_accuracy: 0.8873 - val_auc: 0.9472 - val_precision: 0.8946 - val_recall: 0.9036 - lr: 3.8742e-07\n",
      "Epoch 89/100\n",
      "112/112 [==============================] - 11s 100ms/step - loss: 0.1211 - accuracy: 0.9542 - auc: 0.9904 - precision: 0.9528 - recall: 0.9653 - val_loss: 0.3500 - val_accuracy: 0.8873 - val_auc: 0.9460 - val_precision: 0.8946 - val_recall: 0.9036 - lr: 3.8742e-07\n",
      "Epoch 90/100\n",
      "112/112 [==============================] - 11s 100ms/step - loss: 0.1289 - accuracy: 0.9517 - auc: 0.9883 - precision: 0.9529 - recall: 0.9627 - val_loss: 0.3281 - val_accuracy: 0.8862 - val_auc: 0.9491 - val_precision: 0.8852 - val_recall: 0.9137 - lr: 3.4868e-07\n",
      "Epoch 91/100\n",
      "112/112 [==============================] - 11s 100ms/step - loss: 0.1095 - accuracy: 0.9568 - auc: 0.9917 - precision: 0.9525 - recall: 0.9695 - val_loss: 0.3510 - val_accuracy: 0.8850 - val_auc: 0.9443 - val_precision: 0.8748 - val_recall: 0.9257 - lr: 3.4868e-07\n",
      "Epoch 92/100\n",
      "112/112 [==============================] - 11s 101ms/step - loss: 0.1313 - accuracy: 0.9487 - auc: 0.9886 - precision: 0.9521 - recall: 0.9560 - val_loss: 0.3317 - val_accuracy: 0.8806 - val_auc: 0.9478 - val_precision: 0.8752 - val_recall: 0.9157 - lr: 3.4868e-07\n",
      "Epoch 93/100\n",
      "112/112 [==============================] - 11s 100ms/step - loss: 0.1165 - accuracy: 0.9556 - auc: 0.9909 - precision: 0.9535 - recall: 0.9674 - val_loss: 0.3341 - val_accuracy: 0.8862 - val_auc: 0.9507 - val_precision: 0.8867 - val_recall: 0.9116 - lr: 3.4868e-07\n",
      "Epoch 94/100\n",
      "112/112 [==============================] - 11s 100ms/step - loss: 0.1183 - accuracy: 0.9598 - auc: 0.9893 - precision: 0.9577 - recall: 0.9703 - val_loss: 0.3397 - val_accuracy: 0.8806 - val_auc: 0.9473 - val_precision: 0.8752 - val_recall: 0.9157 - lr: 3.4868e-07\n",
      "Epoch 95/100\n",
      "112/112 [==============================] - 11s 100ms/step - loss: 0.1228 - accuracy: 0.9551 - auc: 0.9897 - precision: 0.9562 - recall: 0.9643 - val_loss: 0.3335 - val_accuracy: 0.8839 - val_auc: 0.9494 - val_precision: 0.8833 - val_recall: 0.9116 - lr: 3.4868e-07\n",
      "Epoch 96/100\n",
      "112/112 [==============================] - 11s 100ms/step - loss: 0.1293 - accuracy: 0.9495 - auc: 0.9892 - precision: 0.9437 - recall: 0.9653 - val_loss: 0.3366 - val_accuracy: 0.8839 - val_auc: 0.9475 - val_precision: 0.8924 - val_recall: 0.8996 - lr: 3.4868e-07\n",
      "Epoch 97/100\n",
      "112/112 [==============================] - 11s 100ms/step - loss: 0.1249 - accuracy: 0.9551 - auc: 0.9890 - precision: 0.9502 - recall: 0.9682 - val_loss: 0.3474 - val_accuracy: 0.8817 - val_auc: 0.9471 - val_precision: 0.8858 - val_recall: 0.9036 - lr: 3.4868e-07\n",
      "Epoch 98/100\n",
      "112/112 [==============================] - 11s 100ms/step - loss: 0.1190 - accuracy: 0.9531 - auc: 0.9909 - precision: 0.9604 - recall: 0.9576 - val_loss: 0.3353 - val_accuracy: 0.8873 - val_auc: 0.9483 - val_precision: 0.8795 - val_recall: 0.9237 - lr: 3.4868e-07\n",
      "Epoch 99/100\n",
      "112/112 [==============================] - 11s 100ms/step - loss: 0.1091 - accuracy: 0.9579 - auc: 0.9918 - precision: 0.9492 - recall: 0.9749 - val_loss: 0.3379 - val_accuracy: 0.8929 - val_auc: 0.9498 - val_precision: 0.8957 - val_recall: 0.9137 - lr: 3.1381e-07\n",
      "Epoch 100/100\n",
      "112/112 [==============================] - 11s 100ms/step - loss: 0.1095 - accuracy: 0.9568 - auc: 0.9918 - precision: 0.9574 - recall: 0.9656 - val_loss: 0.3589 - val_accuracy: 0.8884 - val_auc: 0.9417 - val_precision: 0.8902 - val_recall: 0.9116 - lr: 3.1381e-07\n",
      "28/28 [==============================] - 2s 72ms/step - loss: 0.3589 - accuracy: 0.8884 - auc: 0.9417 - precision: 0.8902 - recall: 0.9116\n",
      "Validation Loss: 0.3589\n",
      "Validation Accuracy: 0.8884\n",
      "Validation AUC: 0.9417\n",
      "Validation Precision: 0.8902\n",
      "Validation Recall: 0.9116\n",
      "28/28 [==============================] - 4s 64ms/step\n",
      "Confusion Matrix:\n",
      "[[342  56]\n",
      " [ 44 454]]\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.89      0.86      0.87       398\n",
      "           1       0.89      0.91      0.90       498\n",
      "\n",
      "    accuracy                           0.89       896\n",
      "   macro avg       0.89      0.89      0.89       896\n",
      "weighted avg       0.89      0.89      0.89       896\n",
      "\n",
      "Visualizations have been saved as PNG files.\n",
      "Predictions have been saved in 'predictions.csv'.\n",
      
     
     
      "==================================================================================================\n",
      "Total params: 24,295,553\n",
      "Trainable params: 728,449\n",
      "Non-trainable params: 23,567,104\n",
      "__________________________________________________________________________________________________\n"
     ]
    },
  {
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models, Input, mixed_precision\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import confusion_matrix, classification_report, f1_score, roc_auc_score, precision_recall_curve, average_precision_score, accuracy_score, roc_curve\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Load the DataFrame\n",
    "file_path = r\"C:\\Users\\meh91075\\Downloads\\AutoSplit_all_in_one_dataset_05112024.csv\"\n",
    "#\"C:\\Users\\meh91075\\Downloads\\merged_features_filtered.csv\"\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# Entferne label_standard\n",
    "df = df.drop('label_standard', axis=1)\n",
    "\n",
    "\n",
    "print(f\"Original CSV data loaded. Number of rows: {len(df)}\")\n",
    "print(f\"Original distribution of labels:\")\n",
    "print(df['label_majority'].value_counts())\n",
    "\n",
    "# Optimize GPU configuration\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    try:\n",
    "        for gpu in gpus:\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)\n",
    "        logical_gpus = tf.config.experimental.list_logical_devices('GPU')\n",
    "        print(f\"{len(gpus)} Physical GPUs, {len(logical_gpus)} Logical GPUs\")\n",
    "    except RuntimeError as e:\n",
    "        print(e)\n",
    "\n",
    "# Activate Mixed Precision\n",
    "if tf.test.is_built_with_cuda():\n",
    "    policy = mixed_precision.Policy('mixed_float16')\n",
    "    mixed_precision.set_global_policy(policy)\n",
    "    print(\"Mixed precision policy set to mixed_float16\")\n",
    "else:\n",
    "    print(\"CUDA not available, using default precision\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Pfade und Konfiguration\n",
    "base_dir =  r\"C:\\Users\\meh91075\\Desktop\\DataSet(S1)\"\n",
    "filtered_dir = os.path.join(base_dir, 'New_filtered_data_label0')\n",
    "non_filtered_dir = os.path.join(base_dir, 'New_non_filtered_data')\n",
    "\n",
    "\n",
    "\n",
    "img_height, img_width = 224, 224\n",
    "batch_size = 32\n",
    "\n",
    "def get_image_paths(directory):\n",
    "    return [os.path.join(root, file) for root, _, files in os.walk(directory) \n",
    "            for file in files if file.lower().endswith(('.png', '.jpg', '.jpeg'))]\n",
    "\n",
    "filtered_images = get_image_paths(filtered_dir)\n",
    "non_filtered_images = get_image_paths(non_filtered_dir)\n",
    "print(f\"Found images: Filtered: {len(filtered_images)}, Not filtered: {len(non_filtered_images)}\")\n",
    "\n",
    "def extract_filename(path):\n",
    "    basename = os.path.basename(path)\n",
    "    return basename.replace('_ISO', '').replace('_ISOAlt', '').split('.')[0]\n",
    "\n",
    "image_df = pd.DataFrame({\n",
    "    'image_path': filtered_images + non_filtered_images,\n",
    "    'label': [1] * len(filtered_images) + [0] * len(non_filtered_images),\n",
    "    'filename': [extract_filename(path) for path in filtered_images + non_filtered_images]\n",
    "})\n",
    "\n",
    "print(\"Sample of extracted filenames:\")\n",
    "print(image_df['filename'].head())\n",
    "\n",
    "# Adjusted merge process\n",
    "merged_df = pd.merge(df, image_df[['filename', 'image_path']], on='filename', how='left')\n",
    "merged_df['label'] = merged_df['label_majority'].map({1: 0, 2: 0, 3: 0, 4: 1})\n",
    "merged_df['has_image'] = merged_df['image_path'].notna()\n",
    "\n",
    "# Filter only entries with associated images\n",
    "merged_df_with_images = merged_df[merged_df['has_image']]\n",
    "\n",
    "# Undersampling of the majority class\n",
    "majority_class = merged_df_with_images[merged_df_with_images['label'] == 0]\n",
    "minority_class = merged_df_with_images[merged_df_with_images['label'] == 1]\n",
    "\n",
    "majority_downsampled = majority_class.sample(n=2000, random_state=42)\n",
    "balanced_df = pd.concat([majority_downsampled, minority_class])\n",
    "\n",
    "print(f\"After undersampling: Number of rows: {len(balanced_df)}\")\n",
    "print(\"Distribution of labels after undersampling:\")\n",
    "print(balanced_df['label'].value_counts())\n",
    "\n",
    "# Train-Test-Split\n",
    "train_df, val_df = train_test_split(balanced_df, test_size=0.2, stratify=balanced_df['label'], random_state=42)\n",
    "print(f\"Training data: {len(train_df)}, Validation data: {len(val_df)}\")\n",
    "\n",
    "# Determine the numeric columns\n",
    "numeric_columns = df.select_dtypes(include=[np.number]).columns.tolist()\n",
    "numeric_columns = [col for col in numeric_columns if col not in ['label_majority', 'label']]\n",
    "\n",
    "# Normalization of numeric data\n",
    "scaler = StandardScaler()\n",
    "train_df[numeric_columns] = scaler.fit_transform(train_df[numeric_columns])\n",
    "val_df[numeric_columns] = scaler.transform(val_df[numeric_columns])\n",
    "\n",
    "# Optimized data generator function with data augmentation\n",
    "def create_dataset(dataframe, is_training=True):\n",
    "    def parse_function(filename, label, *numeric_data):\n",
    "        img = tf.io.read_file(filename)\n",
    "        img = tf.image.decode_jpeg(img, channels=3)\n",
    "        img = tf.image.resize(img, [img_height, img_width])\n",
    "        img = tf.keras.applications.resnet_v2.preprocess_input(img)\n",
    "        \n",
    "        if is_training:\n",
    "            img = tf.image.random_flip_left_right(img)\n",
    "            img = tf.image.random_brightness(img, max_delta=0.2)\n",
    "            img = tf.image.random_contrast(img, lower=0.8, upper=1.2)\n",
    "            img = tf.image.random_saturation(img, lower=0.8, upper=1.2)\n",
    "            img = tf.image.random_hue(img, max_delta=0.1)\n",
    "        \n",
    "        numeric_data = tf.convert_to_tensor(numeric_data, dtype=tf.float32)\n",
    "        numeric_data = tf.squeeze(numeric_data)\n",
    "        return (img, numeric_data), label\n",
    "\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((\n",
    "        dataframe['image_path'].values,\n",
    "        dataframe['label'].values,\n",
    "        dataframe[numeric_columns].values\n",
    "    ))\n",
    "    \n",
    "    dataset = dataset.map(parse_function, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "    \n",
    "    if is_training:\n",
    "        dataset = dataset.shuffle(buffer_size=len(dataframe)).repeat()\n",
    "    else:\n",
    "        dataset = dataset.repeat()\n",
    "    \n",
    "    dataset = dataset.batch(batch_size)\n",
    "    dataset = dataset.prefetch(tf.data.AUTOTUNE)\n",
    "    return dataset\n",
    "\n",
    "# Create the datasets\n",
    "train_dataset = create_dataset(train_df)\n",
    "val_dataset = create_dataset(val_df, is_training=False)\n",
    "\n",
    "# Calculate steps per epoch\n",
    "steps_per_epoch = len(train_df) // batch_size\n",
    "validation_steps = len(val_df) // batch_size\n",
    "\n",
    "# Optimized model definition\n",
    "def create_optimized_hybrid_model(img_height, img_width, num_numeric_features):\n",
    "    # Image processing branch (CNN)\n",
    "    img_input = Input(shape=(img_height, img_width, 3))\n",
    "    \n",
    "    # Use a pre-trained model for feature extraction\n",
    "    base_model = tf.keras.applications.ResNet50V2(include_top=False, weights='imagenet', input_tensor=img_input)\n",
    "    \n",
    "    # Freeze the pre-trained layers\n",
    "    base_model.trainable = False\n",
    "    \n",
    "    x = base_model.output\n",
    "    x = layers.GlobalAveragePooling2D()(x)\n",
    "    x = layers.Dense(256, activation='relu')(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.Dropout(0.5)(x)\n",
    "    \n",
    "    # Numerical data branch (MLP)\n",
    "    num_input = Input(shape=(num_numeric_features,))\n",
    "    y = layers.Dense(128, activation='relu')(num_input)\n",
    "    y = layers.BatchNormalization()(y)\n",
    "    y = layers.Dropout(0.3)(y)\n",
    "    y = layers.Dense(256, activation='relu')(y)\n",
    "    y = layers.BatchNormalization()(y)\n",
    "    y = layers.Dropout(0.4)(y)\n",
    "    y = layers.Dense(128, activation='relu')(y)\n",
    "    y = layers.BatchNormalization()(y)\n",
    "    y = layers.Dropout(0.3)(y)\n",
    "    \n",
    "    # Combining the branches\n",
    "    combined = layers.concatenate([x, y])\n",
    "    \n",
    "    # Final layers\n",
    "    z = layers.Dense(256, activation='relu')(combined)\n",
    "    z = layers.BatchNormalization()(z)\n",
    "    z = layers.Dropout(0.5)(z)\n",
    "    z = layers.Dense(128, activation='relu')(z)\n",
    "    z = layers.BatchNormalization()(z)\n",
    "    z = layers.Dropout(0.5)(z)\n",
    "    \n",
    "    # Output layer\n",
    "    output = layers.Dense(1, activation='sigmoid')(z)\n",
    "    \n",
    "    model = models.Model(inputs=[img_input, num_input], outputs=output)\n",
    "    return model\n",
    "\n",
    "# Learning Rate Logger Callback\n",
    "class LearningRateLogger(tf.keras.callbacks.Callback):\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        if logs is not None:\n",
    "            lr = self.model.optimizer.learning_rate\n",
    "            if isinstance(lr, tf.keras.optimizers.schedules.LearningRateSchedule):\n",
    "                lr = lr(self.model.optimizer.iterations)\n",
    "            logs['lr'] = tf.keras.backend.get_value(lr)\n",
    "\n",
    "# Custom ReduceLROnPlateau Callback\n",
    "class CustomReduceLROnPlateau(tf.keras.callbacks.Callback):\n",
    "    def __init__(self, factor=0.1, patience=10, min_lr=0, monitor='val_loss', mode='min'):\n",
    "        super(CustomReduceLROnPlateau, self).__init__()\n",
    "        self.factor = factor\n",
    "        self.patience = patience\n",
    "        self.min_lr = min_lr\n",
    "        self.monitor = monitor\n",
    "        self.mode = mode\n",
    "        self.best = np.Inf if mode == 'min' else -np.Inf\n",
    "        self.wait = 0\n",
    "        self.best_weights = None\n",
    "\n",
    "    def on_train_begin(self, logs=None):\n",
    "        self.best = np.Inf if self.mode == 'min' else -np.Inf\n",
    "        self.wait = 0\n",
    "        self.best_weights = None\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        logs = logs or {}\n",
    "        current = logs.get(self.monitor)\n",
    "        if current is None:\n",
    "            return\n",
    "        \n",
    "        if self.mode == 'min' and current < self.best:\n",
    "            self.best = current\n",
    "            self.wait = 0\n",
    "            self.best_weights = self.model.get_weights()\n",
    "        elif self.mode == 'max' and current > self.best:\n",
    "            self.best = current\n",
    "            self.wait = 0\n",
    "            self.best_weights = self.model.get_weights()\n",
    "        else:\n",
    "            self.wait += 1\n",
    "        \n",
    "        if self.wait >= self.patience:\n",
    "            current_lr = self.model.optimizer.learning_rate\n",
    "            if isinstance(current_lr, tf.keras.optimizers.schedules.LearningRateSchedule):\n",
    "                current_lr = current_lr(self.model.optimizer.iterations)\n",
    "            current_lr = float(current_lr.numpy())\n",
    "            \n",
    "            if current_lr > self.min_lr:\n",
    "                new_lr = max(current_lr * self.factor, self.min_lr)\n",
    "                new_lr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(\n",
    "                    initial_learning_rate=new_lr,\n",
    "                    decay_steps=1000,\n",
    "                    decay_rate=0.9,\n",
    "                    staircase=True\n",
    "                )\n",
    "                self.model.optimizer.learning_rate = new_lr_schedule\n",
    "                print(f'\\nEpoch {epoch+1}: ReduceLROnPlateau reducing learning rate to {new_lr}.')\n",
    "                self.wait = 0\n",
    "                self.model.set_weights(self.best_weights)\n",
    "\n",
    "# Compile and train\n",
    "initial_learning_rate = 1e-3\n",
    "lr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(\n",
    "    initial_learning_rate, decay_steps=1000, decay_rate=0.9, staircase=True\n",
    ")\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=lr_schedule)\n",
    "\n",
    "# Multi-GPU Strategy\n",
    "strategy = tf.distribute.MirroredStrategy()\n",
    "with strategy.scope():\n",
    "    model = create_optimized_hybrid_model(img_height, img_width, len(numeric_columns))\n",
    "    model.compile(\n",
    "        optimizer=optimizer,\n",
    "        loss='binary_crossentropy',\n",
    "        metrics=['accuracy', tf.keras.metrics.AUC(name='auc'), tf.keras.metrics.Precision(), tf.keras.metrics.Recall()]\n",
    "    )\n",
    "\n",
    "# Callbacks\n",
    "reduce_lr = CustomReduceLROnPlateau(monitor='val_auc', factor=0.2, patience=5, min_lr=1e-6, mode='max')\n",
    "model_checkpoint = tf.keras.callbacks.ModelCheckpoint('best_model.h5', save_best_only=True, monitor='val_auc', mode='max')\n",
    "lr_logger = LearningRateLogger()\n",
    "\n",
    "# Training\n",
    "history = model.fit(\n",
    "    train_dataset,\n",
    "    epochs=100,\n",
    "    steps_per_epoch=steps_per_epoch,\n",
    "    validation_data=val_dataset,\n",
    "    validation_steps=validation_steps,\n",
    "    callbacks=[reduce_lr, model_checkpoint, lr_logger]\n",
    ")\n",
    "\n",
    "# Save model\n",
    "model.save('hybrid_model_optimized_final_new.h5')\n",
    "\n",
    "# Evaluation\n",
    "val_loss, val_accuracy, val_auc, val_precision, val_recall = model.evaluate(val_dataset, steps=validation_steps)\n",
    "print(f\"Validation Loss: {val_loss:.4f}\")\n",
    "print(f\"Validation Accuracy: {val_accuracy:.4f}\")\n",
    "print(f\"Validation AUC: {val_auc:.4f}\")\n",
    "print(f\"Validation Precision: {val_precision:.4f}\")\n",
    "print(f\"Validation Recall: {val_recall:.4f}\")\n",
    "\n",
    "# Predictions for the validation dataset\n",
    "y_pred = model.predict(val_dataset, steps=validation_steps)\n",
    "y_pred_classes = (y_pred > 0.5).astype(\"int32\")\n",
    "y_true = np.concatenate([y for _, y in val_dataset.take(validation_steps)], axis=0)\n",
    "\n",
    "# Confusion matrix and classification report\n",
    "conf_matrix = confusion_matrix(y_true, y_pred_classes)\n",
    "class_report = classification_report(y_true, y_pred_classes)\n",
    "\n",
    "print(\"Confusion Matrix:\")\n",
    "print(conf_matrix)\n",
    "print(\"\\nClassification Report:\")\n",
    "print(class_report)\n",
    "\n",
    "# Visualization of training history\n",
    "plt.figure(figsize=(15, 5))\n",
    "plt.subplot(1, 3, 1)\n",
    "plt.plot(history.history['accuracy'], label='Training Accuracy')\n",
    "plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
    "plt.title('Model Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(1, 3, 2)\n",
    "plt.plot(history.history['loss'], label='Training Loss')\n",
    "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "plt.title('Model Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(1, 3, 3)\n",
    "plt.plot(history.history['auc'], label='Training AUC')\n",
    "plt.plot(history.history['val_auc'], label='Validation AUC')\n",
    "plt.title('Model AUC')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('AUC')\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('training_history.png')\n",
    "plt.close()\n",
    "\n",
    "# Visualization of learning rate\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(history.history['lr'])\n",
    "plt.title('Learning Rate over Epochs')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Learning Rate')\n",
    "plt.yscale('log')\n",
    "plt.savefig('learning_rate.png')\n",
    "plt.close()\n",
    "\n",
    "# Visualization of confusion matrix\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.xlabel('Predicted Label')\n",
    "plt.ylabel('True Label')\n",
    "plt.savefig('confusion_matrix.png')\n",
    "plt.close()\n",
    "\n",
    "print(\"Visualizations have been saved as PNG files.\")\n",
    "\n",
    "# Save predictions and true labels\n",
    "predictions_df = pd.DataFrame({\n",
    "    'True_Label': y_true,\n",
    "    'Predicted_Probability': y_pred.flatten(),\n",
    "    'Predicted_Class': y_pred_classes.flatten()\n",
    "})\n",
    "predictions_df.to_csv('predictions.csv', index=False)\n",
    "print(\"Predictions have been saved in 'predictions.csv'.\")\n",
    "\n",
    "# Model summary\n",
    "model.summary()\n",
    "\n",
    "# Save model architecture as image\n",
    "tf.keras.utils.plot_model(model, to_file='model_architecture.png', show_shapes=True, show_layer_names=True)\n",
    "print(\"Model architecture has been saved as 'model_architecture.png'.\")\n",
    "\n",
    "# Calculate and output additional metrics\n",
    "# ROC AUC\n",
    "roc_auc = roc_auc_score(y_true, y_pred)\n",
    "print(f\"ROC AUC: {roc_auc:.4f}\")\n",
    "\n",
    "# Precision-Recall AUC\n",
    "pr_auc = average_precision_score(y_true, y_pred)\n",
    "print(f\"Precision-Recall AUC: {pr_auc:.4f}\")\n",
    "\n",
    "# F1-Score at various thresholds\n",
    "thresholds = np.arange(0, 1.01, 0.01)\n",
    "f1_scores = [f1_score(y_true, y_pred > threshold) for threshold in thresholds]\n",
    "best_threshold = thresholds[np.argmax(f1_scores)]\n",
    "best_f1 = max(f1_scores)\n",
    "print(f\"Best F1-Score: {best_f1:.4f} at threshold {best_threshold:.2f}\")\n",
    "\n",
    "# Visualization of Precision-Recall curve\n",
    "precision, recall, _ = precision_recall_curve(y_true, y_pred)\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(recall, precision, marker='.')\n",
    "plt.title('Precision-Recall Curve')\n",
    "plt.xlabel('Recall')\n",
    "plt.ylabel('Precision')\n",
    "plt.savefig('precision_recall_curve.png')\n",
    "plt.close()\n",
    "print(\"Precision-Recall curve has been saved as 'precision_recall_curve.png'.\")\n",
    "\n",
    "# Visualization of ROC curve\n",
    "fpr, tpr, _ = roc_curve(y_true, y_pred)\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(fpr, tpr, label=f'ROC Curve (AUC = {roc_auc:.2f})')\n",
    "plt.plot([0, 1], [0, 1], linestyle='--', label='Random Classifier')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver Operating Characteristic (ROC) Curve')\n",
    "plt.legend()\n",
    "plt.savefig('roc_curve.png')\n",
    "plt.close()\n",
    "print(\"ROC curve has been saved as 'roc_curve.png'.\")\n",
    "\n",
    "# Save training history\n",
    "with open('training_history.txt', 'w') as f:\n",
    "    for key, values in history.history.items():\n",
    "        f.write(f\"{key}: {values}\\n\")\n",
    "print(\"Training history has been saved in 'training_history.txt'.\")\n",
    "\n",
    "# Analyze misclassified examples\n",
    "misclassified = np.where(y_pred_classes.flatten() != y_true)[0]\n",
    "print(f\"\\nNumber of misclassified examples: {len(misclassified)}\")\n",
    "\n",
    "if len(misclassified) > 0:\n",
    "    print(\"\\nDetails of some misclassified examples:\")\n",
    "    for i in misclassified[:5]:  # Show details for the first 5 misclassified examples\n",
    "        true_label = y_true[i]\n",
    "        pred_label = y_pred_classes[i][0]\n",
    "        pred_prob = y_pred[i][0]\n",
    "        print(f\"Example {i}: True Label: {true_label}, Predicted Label: {pred_label}, Predicted Probability: {pred_prob:.4f}\")\n",
    "\n",
    "# Model performance across different thresholds\n",
    "accuracies = [accuracy_score(y_true, y_pred > threshold) for threshold in thresholds]\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(thresholds, accuracies, label='Accuracy')\n",
    "plt.plot(thresholds, f1_scores, label='F1-Score')\n",
    "plt.axvline(x=0.5, color='r', linestyle='--', label='Default Threshold (0.5)')\n",
    "plt.xlabel('Threshold')\n",
    "plt.ylabel('Score')\n",
    "plt.title('Model Performance vs. Classification Threshold')\n",
    "plt.legend()\n",
    "plt.savefig('performance_vs_threshold.png')\n",
    "plt.close()\n",
    "print(\"Performance vs. Threshold plot has been saved as 'performance_vs_threshold.png'.\")\n",
    "\n",
    "# Optional: SHAP Analysis\n",
    "try:\n",
    "    import shap\n",
    "    \n",
    "    # Select a subset of validation data for SHAP analysis\n",
    "    num_explain = 100\n",
    "    explain_dataset = val_dataset.take(num_explain).unbatch()\n",
    "    background_dataset = train_dataset.take(num_explain).unbatch()\n",
    "    \n",
    "    # Create an Explainer\n",
    "    explainer = shap.DeepExplainer(model, background_dataset)\n",
    "    \n",
    "    # Calculate SHAP values\n",
    "    shap_values = explainer.shap_values(explain_dataset)\n",
    "    \n",
    "    # Visualize SHAP values\n",
    "    shap.summary_plot(shap_values[0], plot_type=\"bar\", feature_names=numeric_columns, show=False)\n",
    "    plt.savefig('shap_summary.png')\n",
    "    plt.close()\n",
    "    print(\"SHAP summary has been saved as 'shap_summary.png'.\")\n",
    "except ImportError:\n",
    "    print(\"SHAP is not installed. Skipping SHAP analysis.\")\n",
    "\n",
    "print(\"\\nAll analyses and visualizations are complete.\")\n",
    "\n",
    "# Summary of key metrics\n",
    "print(\"\\nSummary of key metrics:\")\n",
    "print(f\"Validation Accuracy: {val_accuracy:.4f}\")\n",
    "print(f\"Validation AUC: {val_auc:.4f}\")\n",
    "print(f\"Validation Precision: {val_precision:.4f}\")\n",
    "print(f\"Validation Recall: {val_recall:.4f}\")\n",
    "print(f\"ROC AUC: {roc_auc:.4f}\")\n",
    "print(f\"Precision-Recall AUC: {pr_auc:.4f}\")\n",
    "print(f\"Best F1-Score: {best_f1:.4f} at threshold {best_threshold:.2f}\")\n",
    "\n",
    "# Save summary to a text file\n",
    "with open('model_summary.txt', 'w') as f:\n",
    "    f.write(\"Summary of key metrics:\\n\")\n",
    "    f.write(f\"Validation Accuracy: {val_accuracy:.4f}\\n\")\n",
    "    f.write(f\"Validation AUC: {val_auc:.4f}\\n\")\n",
    "    f.write(f\"Validation Precision: {val_precision:.4f}\\n\")\n",
    "    f.write(f\"Validation Recall: {val_recall:.4f}\\n\")\n",
    "    f.write(f\"ROC AUC: {roc_auc:.4f}\\n\")\n",
    "    f.write(f\"Precision-Recall AUC: {pr_auc:.4f}\\n\")\n",
    "    f.write(f\"Best F1-Score: {best_f1:.4f} at threshold {best_threshold:.2f}\\n\")\n",
    "\n",
    "print(\"\\nA summary of key metrics has been saved in 'model_summary.txt'.\")\n",
    "print(\"\\nThe script has been executed successfully. All results and visualizations have been saved.\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
